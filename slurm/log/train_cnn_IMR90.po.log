************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 09:14:21 2021

begin time >>> Sun Oct  3 09:14:21 2021

begin time >>> Sun Oct  3 09:14:21 2021

begin time >>> Sun Oct  3 09:14:21 2021

begin time >>> Sun Oct  3 09:14:21 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_cnn_one_branch
args.type = train
args.name = IMR90.po
args.length = 10001
===========================


-> make new folder: h5_weights/IMR90.po
-> make new folder: result/IMR90.po/onehot_cnn_one_branch
-> make new folder: result/IMR90.po/onehot_cnn_two_branch
-> make new folder: result/IMR90.po/onehot_embedding_dense
-> make new folder: result/IMR90.po/onehot_dense
-> make new folder: result/IMR90.po/onehot_resnet18
-> make new folder: result/IMR90.po/onehot_resnet34
-> make new folder: result/IMR90.po/embedding_cnn_one_branch
-> make new folder: result/IMR90.po/embedding_cnn_two_branch
-> make new folder: result/IMR90.po/embedding_dense
-> make new folder: result/IMR90.po/onehot_embedding_cnn_one_branch
-> make new folder: result/IMR90.po/onehot_embedding_cnn_two_branch
########################################
gen_name
IMR90.po
########################################

########################################
model_name
onehot_cnn_one_branch
########################################

Found 13814 images belonging to 2 classes.
Found 1706 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 5001, 5, 64)       1600      
_________________________________________________________________
batch_normalization (BatchNo (None, 5001, 5, 64)       256       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1251, 5, 64)       98368     
_________________________________________________________________
batch_normalization_1 (Batch (None, 1251, 5, 64)       256       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 313, 5, 64)        98368     
_________________________________________________________________
batch_normalization_2 (Batch (None, 313, 5, 64)        256       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 156, 5, 64)        0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 156, 5, 64)        256       
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 39, 5, 128)        196736    
_________________________________________________________________
batch_normalization_4 (Batch (None, 39, 5, 128)        512       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 10, 5, 128)        393344    
_________________________________________________________________
batch_normalization_5 (Batch (None, 10, 5, 128)        512       
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 3, 5, 128)         393344    
_________________________________________________________________
batch_normalization_6 (Batch (None, 3, 5, 128)         512       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 1, 5, 128)         0         
_________________________________________________________________
flatten (Flatten)            (None, 640)               0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 640)               2560      
_________________________________________________________________
dense (Dense)                (None, 512)               328192    
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
dense_3 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 2,041,410
Trainable params: 2,038,850
Non-trainable params: 2,560
_________________________________________________________________
Epoch 1/500
431/431 - 415s - loss: 0.7655 - accuracy: 0.5095 - val_loss: 0.7072 - val_accuracy: 0.5236
Epoch 2/500
431/431 - 54s - loss: 0.6998 - accuracy: 0.5557 - val_loss: 0.7778 - val_accuracy: 0.5100
Epoch 3/500
431/431 - 54s - loss: 0.6589 - accuracy: 0.6084 - val_loss: 0.6977 - val_accuracy: 0.5708
Epoch 4/500
431/431 - 55s - loss: 0.6053 - accuracy: 0.6730 - val_loss: 1.7053 - val_accuracy: 0.5024
Epoch 5/500
431/431 - 53s - loss: 0.4947 - accuracy: 0.7632 - val_loss: 0.9027 - val_accuracy: 0.6032
Epoch 6/500
431/431 - 53s - loss: 0.3365 - accuracy: 0.8553 - val_loss: 0.8037 - val_accuracy: 0.6309
Epoch 7/500
431/431 - 54s - loss: 0.1833 - accuracy: 0.9287 - val_loss: 2.8786 - val_accuracy: 0.5454
Epoch 8/500
431/431 - 54s - loss: 0.0993 - accuracy: 0.9647 - val_loss: 1.5680 - val_accuracy: 0.6315
Epoch 9/500
431/431 - 54s - loss: 0.0612 - accuracy: 0.9769 - val_loss: 2.2224 - val_accuracy: 0.5967
Epoch 10/500
431/431 - 54s - loss: 0.0526 - accuracy: 0.9821 - val_loss: 5.5531 - val_accuracy: 0.5324
Epoch 11/500
431/431 - 54s - loss: 0.0518 - accuracy: 0.9808 - val_loss: 1.9006 - val_accuracy: 0.6445
Epoch 12/500
431/431 - 54s - loss: 0.0404 - accuracy: 0.9845 - val_loss: 5.3512 - val_accuracy: 0.5395
Epoch 13/500
431/431 - 54s - loss: 0.0454 - accuracy: 0.9840 - val_loss: 3.2308 - val_accuracy: 0.5932
Epoch 14/500
431/431 - 54s - loss: 0.0388 - accuracy: 0.9866 - val_loss: 2.5467 - val_accuracy: 0.6191
Epoch 15/500
431/431 - 54s - loss: 0.0345 - accuracy: 0.9887 - val_loss: 1.8939 - val_accuracy: 0.6633
Epoch 16/500
431/431 - 54s - loss: 0.0367 - accuracy: 0.9866 - val_loss: 2.9226 - val_accuracy: 0.6279
Epoch 17/500
431/431 - 54s - loss: 0.0333 - accuracy: 0.9885 - val_loss: 2.1595 - val_accuracy: 0.6604
Epoch 18/500
431/431 - 55s - loss: 0.0295 - accuracy: 0.9900 - val_loss: 2.5297 - val_accuracy: 0.6663
Epoch 19/500
431/431 - 54s - loss: 0.0281 - accuracy: 0.9905 - val_loss: 2.9488 - val_accuracy: 0.6132
Epoch 20/500
431/431 - 54s - loss: 0.0237 - accuracy: 0.9917 - val_loss: 4.4706 - val_accuracy: 0.5696
Epoch 21/500
431/431 - 55s - loss: 0.0276 - accuracy: 0.9909 - val_loss: 2.3282 - val_accuracy: 0.6521
Epoch 22/500
431/431 - 54s - loss: 0.0248 - accuracy: 0.9914 - val_loss: 3.8310 - val_accuracy: 0.5967
Epoch 23/500
431/431 - 54s - loss: 0.0255 - accuracy: 0.9913 - val_loss: 2.6633 - val_accuracy: 0.6297
Epoch 24/500
431/431 - 54s - loss: 0.0248 - accuracy: 0.9912 - val_loss: 2.1319 - val_accuracy: 0.6574
Epoch 25/500
431/431 - 55s - loss: 0.0215 - accuracy: 0.9930 - val_loss: 2.9226 - val_accuracy: 0.6356
Epoch 26/500
431/431 - 55s - loss: 0.0203 - accuracy: 0.9933 - val_loss: 2.2872 - val_accuracy: 0.6686
Epoch 27/500
431/431 - 54s - loss: 0.0202 - accuracy: 0.9930 - val_loss: 2.6049 - val_accuracy: 0.6374
Epoch 28/500
431/431 - 56s - loss: 0.0209 - accuracy: 0.9933 - val_loss: 5.5263 - val_accuracy: 0.5631
Epoch 29/500
431/431 - 53s - loss: 0.0252 - accuracy: 0.9913 - val_loss: 2.3826 - val_accuracy: 0.6509
Epoch 30/500
431/431 - 56s - loss: 0.0204 - accuracy: 0.9941 - val_loss: 3.1930 - val_accuracy: 0.6285
Epoch 31/500
431/431 - 55s - loss: 0.0209 - accuracy: 0.9931 - val_loss: 4.2592 - val_accuracy: 0.5837
Epoch 32/500
431/431 - 55s - loss: 0.0208 - accuracy: 0.9933 - val_loss: 1.9234 - val_accuracy: 0.6616
Epoch 33/500
431/431 - 59s - loss: 0.0177 - accuracy: 0.9942 - val_loss: 2.1006 - val_accuracy: 0.6610
Epoch 34/500
431/431 - 59s - loss: 0.0193 - accuracy: 0.9936 - val_loss: 2.1575 - val_accuracy: 0.6521
Epoch 35/500
431/431 - 56s - loss: 0.0204 - accuracy: 0.9930 - val_loss: 3.0615 - val_accuracy: 0.6633
Epoch 36/500
431/431 - 55s - loss: 0.0182 - accuracy: 0.9930 - val_loss: 2.9464 - val_accuracy: 0.6303
========================================
save_weights
h5_weights/IMR90.po/onehot_cnn_one_branch.h5
========================================

end time >>> Sun Oct  3 09:53:29 2021

end time >>> Sun Oct  3 09:53:29 2021

end time >>> Sun Oct  3 09:53:29 2021

end time >>> Sun Oct  3 09:53:29 2021

end time >>> Sun Oct  3 09:53:29 2021












args.model = onehot_cnn_one_branch
time used = 2348.2427418231964


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 09:53:30 2021

begin time >>> Sun Oct  3 09:53:30 2021

begin time >>> Sun Oct  3 09:53:30 2021

begin time >>> Sun Oct  3 09:53:30 2021

begin time >>> Sun Oct  3 09:53:30 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_cnn_two_branch
args.type = train
args.name = IMR90.po
args.length = 10001
===========================


-> h5_weights/IMR90.po folder already exist. pass.
-> result/IMR90.po/onehot_cnn_one_branch folder already exist. pass.
-> result/IMR90.po/onehot_cnn_two_branch folder already exist. pass.
-> result/IMR90.po/onehot_embedding_dense folder already exist. pass.
-> result/IMR90.po/onehot_dense folder already exist. pass.
-> result/IMR90.po/onehot_resnet18 folder already exist. pass.
-> result/IMR90.po/onehot_resnet34 folder already exist. pass.
-> result/IMR90.po/embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.po/embedding_cnn_two_branch folder already exist. pass.
-> result/IMR90.po/embedding_dense folder already exist. pass.
-> result/IMR90.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.po/onehot_embedding_cnn_two_branch folder already exist. pass.
Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001, 5, 1) 0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001, 5, 1) 0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 2501, 5, 64)  1600        input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 2501, 5, 64)  1600        input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 2501, 5, 64)  256         conv2d[0][0]                     
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 2501, 5, 64)  256         conv2d_6[0][0]                   
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 626, 5, 64)   98368       batch_normalization[0][0]        
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 626, 5, 64)   98368       batch_normalization_8[0][0]      
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 626, 5, 64)   256         conv2d_1[0][0]                   
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 626, 5, 64)   256         conv2d_7[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 157, 5, 64)   98368       batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 157, 5, 64)   98368       batch_normalization_9[0][0]      
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 157, 5, 64)   256         conv2d_2[0][0]                   
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 157, 5, 64)   256         conv2d_8[0][0]                   
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 78, 5, 64)    0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 78, 5, 64)    0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 78, 5, 64)    256         max_pooling2d[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 78, 5, 64)    256         max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 20, 5, 128)   196736      batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 20, 5, 128)   196736      batch_normalization_11[0][0]     
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 20, 5, 128)   512         conv2d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 20, 5, 128)   512         conv2d_9[0][0]                   
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 5, 5, 128)    393344      batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 5, 5, 128)    393344      batch_normalization_12[0][0]     
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 5, 5, 128)    512         conv2d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 5, 5, 128)    512         conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 2, 5, 128)    393344      batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 2, 5, 128)    393344      batch_normalization_13[0][0]     
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 2, 5, 128)    512         conv2d_5[0][0]                   
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 2, 5, 128)    512         conv2d_11[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 1, 5, 128)    0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 1, 5, 128)    0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
flatten (Flatten)               (None, 640)          0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 640)          0           max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 640)          2560        flatten[0][0]                    
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 640)          2560        flatten_1[0][0]                  
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          328192      batch_normalization_7[0][0]      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          328192      batch_normalization_15[0][0]     
__________________________________________________________________________________________________
dropout (Dropout)               (None, 512)          0           dense[0][0]                      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 1024)         0           dropout[0][0]                    
                                                                 dropout_1[0][0]                  
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          524800      concatenate[0][0]                
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 512)          262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 2)            1026        dense_3[0][0]                    
==================================================================================================
Total params: 3,818,626
Trainable params: 3,813,506
Non-trainable params: 5,120
__________________________________________________________________________________________________
Found 13814 images belonging to 2 classes.
Found 13814 images belonging to 2 classes.
Epoch 1/500
Found 1706 images belonging to 2 classes.
Found 1706 images belonging to 2 classes.
1535/1535 - 887s - loss: 0.6510 - accuracy: 0.6198 - val_loss: 0.8218 - val_accuracy: 0.5531
Epoch 2/500
1535/1535 - 255s - loss: 0.3154 - accuracy: 0.8632 - val_loss: 1.0454 - val_accuracy: 0.6738
Epoch 3/500
1535/1535 - 244s - loss: 0.0900 - accuracy: 0.9680 - val_loss: 1.5527 - val_accuracy: 0.6451
Epoch 4/500
1535/1535 - 235s - loss: 0.0555 - accuracy: 0.9824 - val_loss: 2.6342 - val_accuracy: 0.6247
Epoch 5/500
1535/1535 - 239s - loss: 0.0370 - accuracy: 0.9888 - val_loss: 2.9135 - val_accuracy: 0.6259
Epoch 6/500
1535/1535 - 250s - loss: 0.0319 - accuracy: 0.9904 - val_loss: 2.1934 - val_accuracy: 0.6458
Epoch 7/500
1535/1535 - 253s - loss: 0.0287 - accuracy: 0.9915 - val_loss: 2.2857 - val_accuracy: 0.6408
Epoch 8/500
1535/1535 - 235s - loss: 0.0256 - accuracy: 0.9923 - val_loss: 2.0096 - val_accuracy: 0.6633
Epoch 9/500
1535/1535 - 238s - loss: 0.0221 - accuracy: 0.9932 - val_loss: 2.5203 - val_accuracy: 0.6214
Epoch 10/500
1535/1535 - 241s - loss: 0.0197 - accuracy: 0.9942 - val_loss: 2.6143 - val_accuracy: 0.6303
Epoch 11/500
1535/1535 - 241s - loss: 0.0209 - accuracy: 0.9941 - val_loss: 2.0169 - val_accuracy: 0.6697
Epoch 12/500
1535/1535 - 259s - loss: 0.0163 - accuracy: 0.9951 - val_loss: 2.2433 - val_accuracy: 0.6619
========================================
save_weights
h5_weights/IMR90.po/onehot_cnn_two_branch.h5
========================================

end time >>> Sun Oct  3 10:53:24 2021

end time >>> Sun Oct  3 10:53:24 2021

end time >>> Sun Oct  3 10:53:24 2021

end time >>> Sun Oct  3 10:53:24 2021

end time >>> Sun Oct  3 10:53:24 2021












args.model = onehot_cnn_two_branch
time used = 3594.178589105606


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 10:53:26 2021

begin time >>> Sun Oct  3 10:53:26 2021

begin time >>> Sun Oct  3 10:53:26 2021

begin time >>> Sun Oct  3 10:53:26 2021

begin time >>> Sun Oct  3 10:53:26 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_dense
args.type = train
args.name = IMR90.po
args.length = 10001
===========================


-> h5_weights/IMR90.po folder already exist. pass.
-> result/IMR90.po/onehot_cnn_one_branch folder already exist. pass.
-> result/IMR90.po/onehot_cnn_two_branch folder already exist. pass.
-> result/IMR90.po/onehot_embedding_dense folder already exist. pass.
-> result/IMR90.po/onehot_dense folder already exist. pass.
-> result/IMR90.po/onehot_resnet18 folder already exist. pass.
-> result/IMR90.po/onehot_resnet34 folder already exist. pass.
-> result/IMR90.po/embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.po/embedding_cnn_two_branch folder already exist. pass.
-> result/IMR90.po/embedding_dense folder already exist. pass.
-> result/IMR90.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
IMR90.po
########################################

########################################
model_name
onehot_dense
########################################

Found 13814 images belonging to 2 classes.
Found 1706 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 100010)            0         
_________________________________________________________________
batch_normalization (BatchNo (None, 100010)            400040    
_________________________________________________________________
dense (Dense)                (None, 512)               51205632  
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_3 (Batch (None, 512)               2048      
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_3 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 52,402,858
Trainable params: 52,198,742
Non-trainable params: 204,116
_________________________________________________________________
Epoch 1/500
431/431 - 45s - loss: 0.7559 - accuracy: 0.5622 - val_loss: 0.6773 - val_accuracy: 0.5949
Epoch 2/500
431/431 - 42s - loss: 0.6209 - accuracy: 0.6704 - val_loss: 0.8401 - val_accuracy: 0.5702
Epoch 3/500
431/431 - 41s - loss: 0.4767 - accuracy: 0.7772 - val_loss: 1.2018 - val_accuracy: 0.5649
Epoch 4/500
431/431 - 45s - loss: 0.3469 - accuracy: 0.8536 - val_loss: 1.5362 - val_accuracy: 0.5678
Epoch 5/500
431/431 - 41s - loss: 0.2472 - accuracy: 0.9026 - val_loss: 1.8128 - val_accuracy: 0.5772
Epoch 6/500
431/431 - 40s - loss: 0.1926 - accuracy: 0.9250 - val_loss: 1.9275 - val_accuracy: 0.5796
Epoch 7/500
431/431 - 41s - loss: 0.1490 - accuracy: 0.9395 - val_loss: 2.0327 - val_accuracy: 0.5849
Epoch 8/500
431/431 - 40s - loss: 0.1241 - accuracy: 0.9511 - val_loss: 2.1609 - val_accuracy: 0.5938
Epoch 9/500
431/431 - 40s - loss: 0.1059 - accuracy: 0.9610 - val_loss: 2.1452 - val_accuracy: 0.6038
Epoch 10/500
431/431 - 41s - loss: 0.0944 - accuracy: 0.9642 - val_loss: 2.1939 - val_accuracy: 0.6091
Epoch 11/500
431/431 - 41s - loss: 0.0905 - accuracy: 0.9665 - val_loss: 2.2193 - val_accuracy: 0.5920
Epoch 12/500
431/431 - 41s - loss: 0.0696 - accuracy: 0.9734 - val_loss: 2.2111 - val_accuracy: 0.6108
Epoch 13/500
431/431 - 42s - loss: 0.0749 - accuracy: 0.9726 - val_loss: 2.2191 - val_accuracy: 0.6209
Epoch 14/500
431/431 - 42s - loss: 0.0658 - accuracy: 0.9753 - val_loss: 2.1636 - val_accuracy: 0.6173
Epoch 15/500
431/431 - 40s - loss: 0.0612 - accuracy: 0.9780 - val_loss: 2.1754 - val_accuracy: 0.6197
Epoch 16/500
431/431 - 41s - loss: 0.0589 - accuracy: 0.9789 - val_loss: 2.1831 - val_accuracy: 0.6333
Epoch 17/500
431/431 - 42s - loss: 0.0506 - accuracy: 0.9819 - val_loss: 2.1850 - val_accuracy: 0.6209
Epoch 18/500
431/431 - 44s - loss: 0.0463 - accuracy: 0.9838 - val_loss: 2.2050 - val_accuracy: 0.6285
Epoch 19/500
431/431 - 41s - loss: 0.0436 - accuracy: 0.9857 - val_loss: 2.1900 - val_accuracy: 0.6321
Epoch 20/500
431/431 - 46s - loss: 0.0410 - accuracy: 0.9850 - val_loss: 2.2922 - val_accuracy: 0.6356
Epoch 21/500
431/431 - 42s - loss: 0.0449 - accuracy: 0.9835 - val_loss: 2.2204 - val_accuracy: 0.6427
Epoch 22/500
431/431 - 42s - loss: 0.0433 - accuracy: 0.9840 - val_loss: 2.1584 - val_accuracy: 0.6521
Epoch 23/500
431/431 - 46s - loss: 0.0375 - accuracy: 0.9873 - val_loss: 2.1411 - val_accuracy: 0.6616
Epoch 24/500
431/431 - 45s - loss: 0.0399 - accuracy: 0.9864 - val_loss: 2.1169 - val_accuracy: 0.6509
Epoch 25/500
431/431 - 45s - loss: 0.0411 - accuracy: 0.9858 - val_loss: 2.0731 - val_accuracy: 0.6621
Epoch 26/500
431/431 - 46s - loss: 0.0338 - accuracy: 0.9872 - val_loss: 2.1679 - val_accuracy: 0.6380
Epoch 27/500
431/431 - 44s - loss: 0.0347 - accuracy: 0.9882 - val_loss: 2.1545 - val_accuracy: 0.6356
Epoch 28/500
431/431 - 47s - loss: 0.0296 - accuracy: 0.9893 - val_loss: 2.1573 - val_accuracy: 0.6456
Epoch 29/500
431/431 - 44s - loss: 0.0289 - accuracy: 0.9893 - val_loss: 2.1499 - val_accuracy: 0.6498
Epoch 30/500
431/431 - 46s - loss: 0.0337 - accuracy: 0.9886 - val_loss: 2.1806 - val_accuracy: 0.6492
Epoch 31/500
431/431 - 47s - loss: 0.0291 - accuracy: 0.9911 - val_loss: 2.1578 - val_accuracy: 0.6545
Epoch 32/500
431/431 - 41s - loss: 0.0261 - accuracy: 0.9909 - val_loss: 2.2063 - val_accuracy: 0.6474
Epoch 33/500
431/431 - 41s - loss: 0.0221 - accuracy: 0.9925 - val_loss: 2.1681 - val_accuracy: 0.6580
Epoch 34/500
431/431 - 45s - loss: 0.0263 - accuracy: 0.9917 - val_loss: 2.2350 - val_accuracy: 0.6568
Epoch 35/500
431/431 - 42s - loss: 0.0287 - accuracy: 0.9909 - val_loss: 2.1544 - val_accuracy: 0.6621
========================================
save_weights
h5_weights/IMR90.po/onehot_dense.h5
========================================

end time >>> Sun Oct  3 11:18:37 2021

end time >>> Sun Oct  3 11:18:37 2021

end time >>> Sun Oct  3 11:18:37 2021

end time >>> Sun Oct  3 11:18:37 2021

end time >>> Sun Oct  3 11:18:37 2021












args.model = onehot_dense
time used = 1511.7464845180511


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 11:18:38 2021

begin time >>> Sun Oct  3 11:18:38 2021

begin time >>> Sun Oct  3 11:18:38 2021

begin time >>> Sun Oct  3 11:18:38 2021

begin time >>> Sun Oct  3 11:18:38 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_resnet18
args.type = train
args.name = IMR90.po
args.length = 10001
===========================


-> h5_weights/IMR90.po folder already exist. pass.
-> result/IMR90.po/onehot_cnn_one_branch folder already exist. pass.
-> result/IMR90.po/onehot_cnn_two_branch folder already exist. pass.
-> result/IMR90.po/onehot_embedding_dense folder already exist. pass.
-> result/IMR90.po/onehot_dense folder already exist. pass.
-> result/IMR90.po/onehot_resnet18 folder already exist. pass.
-> result/IMR90.po/onehot_resnet34 folder already exist. pass.
-> result/IMR90.po/embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.po/embedding_cnn_two_branch folder already exist. pass.
-> result/IMR90.po/embedding_dense folder already exist. pass.
-> result/IMR90.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
IMR90.po
########################################

########################################
model_name
onehot_resnet18
########################################

Found 13814 images belonging to 2 classes.
Found 1706 images belonging to 2 classes.
Model: "res_net_type_i"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              multiple                  1600      
_________________________________________________________________
batch_normalization (BatchNo multiple                  256       
_________________________________________________________________
max_pooling2d (MaxPooling2D) multiple                  0         
_________________________________________________________________
sequential (Sequential)      (None, 313, 1, 64)        398912    
_________________________________________________________________
sequential_2 (Sequential)    (None, 20, 1, 64)         398912    
_________________________________________________________________
sequential_4 (Sequential)    (None, 2, 1, 64)          398912    
_________________________________________________________________
sequential_6 (Sequential)    (None, 1, 1, 64)          398912    
_________________________________________________________________
global_average_pooling2d (Gl multiple                  0         
_________________________________________________________________
dense (Dense)                multiple                  130       
=================================================================
Total params: 1,597,634
Trainable params: 1,594,946
Non-trainable params: 2,688
_________________________________________________________________
Epoch 1/500
431/431 - 52s - loss: 0.7521 - accuracy: 0.5025 - val_loss: 0.7014 - val_accuracy: 0.4971
Epoch 2/500
431/431 - 53s - loss: 0.6813 - accuracy: 0.5681 - val_loss: 0.7325 - val_accuracy: 0.4971
Epoch 3/500
431/431 - 53s - loss: 0.6553 - accuracy: 0.6093 - val_loss: 0.7289 - val_accuracy: 0.5029
Epoch 4/500
431/431 - 52s - loss: 0.6262 - accuracy: 0.6506 - val_loss: 0.7284 - val_accuracy: 0.5419
Epoch 5/500
431/431 - 52s - loss: 0.5719 - accuracy: 0.7021 - val_loss: 0.7509 - val_accuracy: 0.5525
Epoch 6/500
431/431 - 52s - loss: 0.4913 - accuracy: 0.7666 - val_loss: 0.7571 - val_accuracy: 0.5855
Epoch 7/500
431/431 - 51s - loss: 0.3984 - accuracy: 0.8256 - val_loss: 0.8390 - val_accuracy: 0.5890
Epoch 8/500
431/431 - 51s - loss: 0.3013 - accuracy: 0.8749 - val_loss: 0.8993 - val_accuracy: 0.5991
Epoch 9/500
431/431 - 51s - loss: 0.2167 - accuracy: 0.9183 - val_loss: 1.0049 - val_accuracy: 0.5932
Epoch 10/500
431/431 - 52s - loss: 0.1718 - accuracy: 0.9370 - val_loss: 1.0805 - val_accuracy: 0.5932
Epoch 11/500
431/431 - 52s - loss: 0.1463 - accuracy: 0.9449 - val_loss: 1.1022 - val_accuracy: 0.6044
Epoch 12/500
431/431 - 53s - loss: 0.1278 - accuracy: 0.9517 - val_loss: 1.1423 - val_accuracy: 0.6108
Epoch 13/500
431/431 - 53s - loss: 0.1265 - accuracy: 0.9537 - val_loss: 1.2045 - val_accuracy: 0.6008
Epoch 14/500
431/431 - 52s - loss: 0.1109 - accuracy: 0.9589 - val_loss: 1.2128 - val_accuracy: 0.6050
Epoch 15/500
431/431 - 52s - loss: 0.0977 - accuracy: 0.9646 - val_loss: 1.2470 - val_accuracy: 0.6002
Epoch 16/500
431/431 - 51s - loss: 0.0898 - accuracy: 0.9681 - val_loss: 1.1928 - val_accuracy: 0.6162
Epoch 17/500
431/431 - 52s - loss: 0.0829 - accuracy: 0.9710 - val_loss: 1.2430 - val_accuracy: 0.6209
Epoch 18/500
431/431 - 52s - loss: 0.0830 - accuracy: 0.9693 - val_loss: 1.2996 - val_accuracy: 0.6050
Epoch 19/500
431/431 - 52s - loss: 0.0816 - accuracy: 0.9701 - val_loss: 1.2910 - val_accuracy: 0.6067
Epoch 20/500
431/431 - 52s - loss: 0.0765 - accuracy: 0.9732 - val_loss: 1.3150 - val_accuracy: 0.6203
Epoch 21/500
431/431 - 53s - loss: 0.0662 - accuracy: 0.9782 - val_loss: 1.3280 - val_accuracy: 0.6138
Epoch 22/500
431/431 - 53s - loss: 0.0623 - accuracy: 0.9789 - val_loss: 1.3485 - val_accuracy: 0.6221
Epoch 23/500
431/431 - 53s - loss: 0.0597 - accuracy: 0.9784 - val_loss: 1.2772 - val_accuracy: 0.6374
Epoch 24/500
431/431 - 53s - loss: 0.0575 - accuracy: 0.9804 - val_loss: 1.2800 - val_accuracy: 0.6285
Epoch 25/500
431/431 - 52s - loss: 0.0564 - accuracy: 0.9809 - val_loss: 1.3807 - val_accuracy: 0.6262
Epoch 26/500
431/431 - 52s - loss: 0.0555 - accuracy: 0.9811 - val_loss: 1.4293 - val_accuracy: 0.6191
Epoch 27/500
431/431 - 52s - loss: 0.0634 - accuracy: 0.9779 - val_loss: 1.4078 - val_accuracy: 0.6350
Epoch 28/500
431/431 - 53s - loss: 0.0537 - accuracy: 0.9822 - val_loss: 1.3256 - val_accuracy: 0.6338
Epoch 29/500
431/431 - 53s - loss: 0.0545 - accuracy: 0.9798 - val_loss: 1.3753 - val_accuracy: 0.6279
Epoch 30/500
431/431 - 52s - loss: 0.0512 - accuracy: 0.9825 - val_loss: 1.4276 - val_accuracy: 0.6167
Epoch 31/500
431/431 - 52s - loss: 0.0501 - accuracy: 0.9823 - val_loss: 1.3979 - val_accuracy: 0.6338
Epoch 32/500
431/431 - 52s - loss: 0.0470 - accuracy: 0.9835 - val_loss: 1.4383 - val_accuracy: 0.6279
Epoch 33/500
431/431 - 52s - loss: 0.0420 - accuracy: 0.9857 - val_loss: 1.5673 - val_accuracy: 0.6226
========================================
save_weights
h5_weights/IMR90.po/onehot_resnet18.h5
========================================

end time >>> Sun Oct  3 11:47:35 2021

end time >>> Sun Oct  3 11:47:35 2021

end time >>> Sun Oct  3 11:47:35 2021

end time >>> Sun Oct  3 11:47:35 2021

end time >>> Sun Oct  3 11:47:35 2021












args.model = onehot_resnet18
time used = 1737.0473093986511


