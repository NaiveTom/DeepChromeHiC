************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 19:14:27 2021

begin time >>> Sun Oct  3 19:14:27 2021

begin time >>> Sun Oct  3 19:14:27 2021

begin time >>> Sun Oct  3 19:14:27 2021

begin time >>> Sun Oct  3 19:14:27 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_dense
args.type = train
args.name = ME.pp
args.length = 10001
===========================


-> h5_weights/ME.pp folder already exist. pass.
-> result/ME.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/ME.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/ME.pp/onehot_embedding_dense folder already exist. pass.
-> result/ME.pp/onehot_dense folder already exist. pass.
-> result/ME.pp/onehot_resnet18 folder already exist. pass.
-> result/ME.pp/onehot_resnet34 folder already exist. pass.
-> result/ME.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/ME.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/ME.pp/embedding_dense folder already exist. pass.
-> result/ME.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/ME.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
ME.pp
########################################

########################################
model_name
embedding_dense
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 626, 64)      409664      concatenate[0][0]                
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, 38, 64)       0           conv1d[0][0]                     
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 38, 64)       256         max_pooling1d[0][0]              
__________________________________________________________________________________________________
flatten (Flatten)               (None, 2432)         0           batch_normalization[0][0]        
__________________________________________________________________________________________________
dropout (Dropout)               (None, 2432)         0           flatten[0][0]                    
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          1245696     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation (Activation)         (None, 512)          0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation[0][0]                 
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 512)          0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_1[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 512)          2048        dense_2[0][0]                    
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 512)          0           activation_2[0][0]               
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            513         dropout_3[0][0]                  
==================================================================================================
Total params: 3,006,985
Trainable params: 3,003,785
Non-trainable params: 3,200
__________________________________________________________________________________________________
Epoch 1/500
108/108 - 14s - loss: 0.8767 - accuracy: 0.4974 - val_loss: 0.6959 - val_accuracy: 0.5035
Epoch 2/500
108/108 - 14s - loss: 0.8783 - accuracy: 0.4956 - val_loss: 0.6950 - val_accuracy: 0.5035
Epoch 3/500
108/108 - 14s - loss: 0.8528 - accuracy: 0.5081 - val_loss: 0.6996 - val_accuracy: 0.5012
Epoch 4/500
108/108 - 14s - loss: 0.8613 - accuracy: 0.5081 - val_loss: 0.7026 - val_accuracy: 0.5106
Epoch 5/500
108/108 - 14s - loss: 0.8681 - accuracy: 0.4962 - val_loss: 0.7037 - val_accuracy: 0.5106
Epoch 6/500
108/108 - 14s - loss: 0.8588 - accuracy: 0.5044 - val_loss: 0.7030 - val_accuracy: 0.4988
Epoch 7/500
108/108 - 14s - loss: 0.8588 - accuracy: 0.5035 - val_loss: 0.7033 - val_accuracy: 0.4941
Epoch 8/500
108/108 - 14s - loss: 0.8674 - accuracy: 0.4916 - val_loss: 0.7027 - val_accuracy: 0.4965
Epoch 9/500
108/108 - 14s - loss: 0.8449 - accuracy: 0.5212 - val_loss: 0.7023 - val_accuracy: 0.4988
Epoch 10/500
108/108 - 14s - loss: 0.8642 - accuracy: 0.5067 - val_loss: 0.7018 - val_accuracy: 0.4988
Epoch 11/500
108/108 - 14s - loss: 0.8584 - accuracy: 0.5067 - val_loss: 0.7017 - val_accuracy: 0.5082
Epoch 12/500
108/108 - 14s - loss: 0.8613 - accuracy: 0.5041 - val_loss: 0.7019 - val_accuracy: 0.5012
Epoch 13/500
108/108 - 14s - loss: 0.8571 - accuracy: 0.5090 - val_loss: 0.7014 - val_accuracy: 0.5082
Epoch 14/500
108/108 - 14s - loss: 0.8323 - accuracy: 0.5148 - val_loss: 0.7014 - val_accuracy: 0.5106
Epoch 15/500
108/108 - 14s - loss: 0.8585 - accuracy: 0.4988 - val_loss: 0.7013 - val_accuracy: 0.5035
Epoch 16/500
108/108 - 14s - loss: 0.8411 - accuracy: 0.5105 - val_loss: 0.7013 - val_accuracy: 0.5082
Epoch 17/500
108/108 - 14s - loss: 0.8453 - accuracy: 0.5047 - val_loss: 0.7011 - val_accuracy: 0.4988
Epoch 18/500
108/108 - 14s - loss: 0.8401 - accuracy: 0.5146 - val_loss: 0.7003 - val_accuracy: 0.5082
Epoch 19/500
108/108 - 14s - loss: 0.8289 - accuracy: 0.5189 - val_loss: 0.7006 - val_accuracy: 0.4941
Epoch 20/500
108/108 - 14s - loss: 0.8378 - accuracy: 0.5114 - val_loss: 0.7006 - val_accuracy: 0.4941
Epoch 21/500
108/108 - 14s - loss: 0.8176 - accuracy: 0.5183 - val_loss: 0.6999 - val_accuracy: 0.4941
Epoch 22/500
108/108 - 14s - loss: 0.8290 - accuracy: 0.5218 - val_loss: 0.7001 - val_accuracy: 0.4988
Epoch 23/500
108/108 - 14s - loss: 0.8355 - accuracy: 0.5070 - val_loss: 0.7004 - val_accuracy: 0.4871
Epoch 24/500
108/108 - 14s - loss: 0.8270 - accuracy: 0.5154 - val_loss: 0.7002 - val_accuracy: 0.4894
========================================
save_weights
h5_weights/ME.pp/embedding_dense.h5
========================================

end time >>> Sun Oct  3 19:20:18 2021

end time >>> Sun Oct  3 19:20:18 2021

end time >>> Sun Oct  3 19:20:18 2021

end time >>> Sun Oct  3 19:20:18 2021

end time >>> Sun Oct  3 19:20:18 2021












args.model = embedding_dense
time used = 351.13911485671997


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 19:20:19 2021

begin time >>> Sun Oct  3 19:20:19 2021

begin time >>> Sun Oct  3 19:20:19 2021

begin time >>> Sun Oct  3 19:20:19 2021

begin time >>> Sun Oct  3 19:20:19 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_cnn_one_branch
args.type = train
args.name = ME.pp
args.length = 10001
===========================


-> h5_weights/ME.pp folder already exist. pass.
-> result/ME.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/ME.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/ME.pp/onehot_embedding_dense folder already exist. pass.
-> result/ME.pp/onehot_dense folder already exist. pass.
-> result/ME.pp/onehot_resnet18 folder already exist. pass.
-> result/ME.pp/onehot_resnet34 folder already exist. pass.
-> result/ME.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/ME.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/ME.pp/embedding_dense folder already exist. pass.
-> result/ME.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/ME.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
ME.pp
########################################

########################################
model_name
embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
108/108 - 15s - loss: 0.9280 - accuracy: 0.4997 - val_loss: 0.6996 - val_accuracy: 0.5035
Epoch 2/500
108/108 - 15s - loss: 0.9057 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.5412
Epoch 3/500
108/108 - 15s - loss: 0.8802 - accuracy: 0.5119 - val_loss: 0.6956 - val_accuracy: 0.5082
Epoch 4/500
108/108 - 15s - loss: 0.8723 - accuracy: 0.5189 - val_loss: 0.6996 - val_accuracy: 0.5224
Epoch 5/500
108/108 - 15s - loss: 0.8779 - accuracy: 0.5148 - val_loss: 0.6992 - val_accuracy: 0.5341
Epoch 6/500
108/108 - 15s - loss: 0.8615 - accuracy: 0.5163 - val_loss: 0.6980 - val_accuracy: 0.5435
Epoch 7/500
108/108 - 15s - loss: 0.8380 - accuracy: 0.5288 - val_loss: 0.6972 - val_accuracy: 0.5529
Epoch 8/500
108/108 - 15s - loss: 0.8434 - accuracy: 0.5250 - val_loss: 0.6955 - val_accuracy: 0.5435
Epoch 9/500
108/108 - 15s - loss: 0.8460 - accuracy: 0.5218 - val_loss: 0.6940 - val_accuracy: 0.5482
Epoch 10/500
108/108 - 15s - loss: 0.8402 - accuracy: 0.5212 - val_loss: 0.6927 - val_accuracy: 0.5482
Epoch 11/500
108/108 - 15s - loss: 0.8361 - accuracy: 0.5285 - val_loss: 0.6913 - val_accuracy: 0.5365
Epoch 12/500
108/108 - 15s - loss: 0.8224 - accuracy: 0.5346 - val_loss: 0.6889 - val_accuracy: 0.5318
Epoch 13/500
108/108 - 15s - loss: 0.7975 - accuracy: 0.5445 - val_loss: 0.6877 - val_accuracy: 0.5506
Epoch 14/500
108/108 - 15s - loss: 0.8004 - accuracy: 0.5445 - val_loss: 0.6867 - val_accuracy: 0.5576
Epoch 15/500
108/108 - 15s - loss: 0.7960 - accuracy: 0.5538 - val_loss: 0.6858 - val_accuracy: 0.5600
Epoch 16/500
108/108 - 15s - loss: 0.7866 - accuracy: 0.5442 - val_loss: 0.6852 - val_accuracy: 0.5600
Epoch 17/500
108/108 - 15s - loss: 0.7935 - accuracy: 0.5489 - val_loss: 0.6834 - val_accuracy: 0.5459
Epoch 18/500
108/108 - 15s - loss: 0.7882 - accuracy: 0.5457 - val_loss: 0.6827 - val_accuracy: 0.5506
Epoch 19/500
108/108 - 15s - loss: 0.7770 - accuracy: 0.5568 - val_loss: 0.6819 - val_accuracy: 0.5529
Epoch 20/500
108/108 - 15s - loss: 0.7735 - accuracy: 0.5608 - val_loss: 0.6812 - val_accuracy: 0.5647
Epoch 21/500
108/108 - 15s - loss: 0.7477 - accuracy: 0.5696 - val_loss: 0.6807 - val_accuracy: 0.5506
Epoch 22/500
108/108 - 15s - loss: 0.7672 - accuracy: 0.5553 - val_loss: 0.6803 - val_accuracy: 0.5576
Epoch 23/500
108/108 - 15s - loss: 0.7645 - accuracy: 0.5719 - val_loss: 0.6803 - val_accuracy: 0.5600
Epoch 24/500
108/108 - 15s - loss: 0.7455 - accuracy: 0.5797 - val_loss: 0.6791 - val_accuracy: 0.5529
Epoch 25/500
108/108 - 15s - loss: 0.7650 - accuracy: 0.5632 - val_loss: 0.6787 - val_accuracy: 0.5506
Epoch 26/500
108/108 - 15s - loss: 0.7494 - accuracy: 0.5795 - val_loss: 0.6787 - val_accuracy: 0.5553
Epoch 27/500
108/108 - 15s - loss: 0.7510 - accuracy: 0.5733 - val_loss: 0.6773 - val_accuracy: 0.5600
Epoch 28/500
108/108 - 15s - loss: 0.7410 - accuracy: 0.5829 - val_loss: 0.6771 - val_accuracy: 0.5576
Epoch 29/500
108/108 - 15s - loss: 0.7372 - accuracy: 0.5806 - val_loss: 0.6768 - val_accuracy: 0.5576
Epoch 30/500
108/108 - 15s - loss: 0.7377 - accuracy: 0.5829 - val_loss: 0.6764 - val_accuracy: 0.5600
Epoch 31/500
108/108 - 15s - loss: 0.7335 - accuracy: 0.5850 - val_loss: 0.6761 - val_accuracy: 0.5553
Epoch 32/500
108/108 - 15s - loss: 0.7278 - accuracy: 0.5923 - val_loss: 0.6755 - val_accuracy: 0.5600
Epoch 33/500
108/108 - 15s - loss: 0.7150 - accuracy: 0.5987 - val_loss: 0.6754 - val_accuracy: 0.5600
Epoch 34/500
108/108 - 15s - loss: 0.7146 - accuracy: 0.5934 - val_loss: 0.6755 - val_accuracy: 0.5529
Epoch 35/500
108/108 - 15s - loss: 0.7260 - accuracy: 0.5958 - val_loss: 0.6751 - val_accuracy: 0.5553
Epoch 36/500
108/108 - 15s - loss: 0.7222 - accuracy: 0.5920 - val_loss: 0.6743 - val_accuracy: 0.5459
Epoch 37/500
108/108 - 15s - loss: 0.7021 - accuracy: 0.6065 - val_loss: 0.6738 - val_accuracy: 0.5459
Epoch 38/500
108/108 - 15s - loss: 0.7188 - accuracy: 0.6022 - val_loss: 0.6736 - val_accuracy: 0.5506
Epoch 39/500
108/108 - 15s - loss: 0.6878 - accuracy: 0.6150 - val_loss: 0.6724 - val_accuracy: 0.5576
Epoch 40/500
108/108 - 15s - loss: 0.6953 - accuracy: 0.6024 - val_loss: 0.6723 - val_accuracy: 0.5600
========================================
save_weights
h5_weights/ME.pp/embedding_cnn_one_branch.h5
========================================

end time >>> Sun Oct  3 19:30:24 2021

end time >>> Sun Oct  3 19:30:24 2021

end time >>> Sun Oct  3 19:30:24 2021

end time >>> Sun Oct  3 19:30:24 2021

end time >>> Sun Oct  3 19:30:24 2021












args.model = embedding_cnn_one_branch
time used = 604.8918609619141


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 19:30:26 2021

begin time >>> Sun Oct  3 19:30:26 2021

begin time >>> Sun Oct  3 19:30:26 2021

begin time >>> Sun Oct  3 19:30:26 2021

begin time >>> Sun Oct  3 19:30:26 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_cnn_two_branch
args.type = train
args.name = ME.pp
args.length = 10001
===========================


-> h5_weights/ME.pp folder already exist. pass.
-> result/ME.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/ME.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/ME.pp/onehot_embedding_dense folder already exist. pass.
-> result/ME.pp/onehot_dense folder already exist. pass.
-> result/ME.pp/onehot_resnet18 folder already exist. pass.
-> result/ME.pp/onehot_resnet34 folder already exist. pass.
-> result/ME.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/ME.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/ME.pp/embedding_dense folder already exist. pass.
-> result/ME.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/ME.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
ME.pp
########################################

########################################
model_name
embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
108/108 - 15s - loss: 0.8980 - accuracy: 0.5058 - val_loss: 0.6988 - val_accuracy: 0.5012
Epoch 2/500
108/108 - 15s - loss: 0.9079 - accuracy: 0.4878 - val_loss: 0.6976 - val_accuracy: 0.5176
Epoch 3/500
108/108 - 15s - loss: 0.8937 - accuracy: 0.4863 - val_loss: 0.7006 - val_accuracy: 0.5247
Epoch 4/500
108/108 - 15s - loss: 0.8663 - accuracy: 0.5044 - val_loss: 0.7039 - val_accuracy: 0.4988
Epoch 5/500
108/108 - 15s - loss: 0.8849 - accuracy: 0.4997 - val_loss: 0.7045 - val_accuracy: 0.4941
Epoch 6/500
108/108 - 15s - loss: 0.8694 - accuracy: 0.5108 - val_loss: 0.7039 - val_accuracy: 0.4941
Epoch 7/500
108/108 - 15s - loss: 0.8569 - accuracy: 0.5143 - val_loss: 0.7027 - val_accuracy: 0.4988
Epoch 8/500
108/108 - 15s - loss: 0.8300 - accuracy: 0.5285 - val_loss: 0.7019 - val_accuracy: 0.4941
Epoch 9/500
108/108 - 15s - loss: 0.8356 - accuracy: 0.5207 - val_loss: 0.7006 - val_accuracy: 0.5012
Epoch 10/500
108/108 - 15s - loss: 0.8221 - accuracy: 0.5265 - val_loss: 0.6998 - val_accuracy: 0.5106
Epoch 11/500
108/108 - 15s - loss: 0.8205 - accuracy: 0.5311 - val_loss: 0.6996 - val_accuracy: 0.5106
Epoch 12/500
108/108 - 15s - loss: 0.8348 - accuracy: 0.5221 - val_loss: 0.6988 - val_accuracy: 0.5106
Epoch 13/500
108/108 - 15s - loss: 0.7926 - accuracy: 0.5463 - val_loss: 0.6975 - val_accuracy: 0.5106
Epoch 14/500
108/108 - 15s - loss: 0.8063 - accuracy: 0.5323 - val_loss: 0.6968 - val_accuracy: 0.5129
Epoch 15/500
108/108 - 15s - loss: 0.7935 - accuracy: 0.5451 - val_loss: 0.6961 - val_accuracy: 0.5224
Epoch 16/500
108/108 - 15s - loss: 0.7674 - accuracy: 0.5597 - val_loss: 0.6957 - val_accuracy: 0.5224
Epoch 17/500
108/108 - 15s - loss: 0.7882 - accuracy: 0.5425 - val_loss: 0.6954 - val_accuracy: 0.5153
Epoch 18/500
108/108 - 15s - loss: 0.7843 - accuracy: 0.5512 - val_loss: 0.6947 - val_accuracy: 0.5082
Epoch 19/500
108/108 - 15s - loss: 0.7692 - accuracy: 0.5722 - val_loss: 0.6935 - val_accuracy: 0.5176
Epoch 20/500
108/108 - 15s - loss: 0.7890 - accuracy: 0.5480 - val_loss: 0.6932 - val_accuracy: 0.5294
Epoch 21/500
108/108 - 15s - loss: 0.7761 - accuracy: 0.5602 - val_loss: 0.6931 - val_accuracy: 0.5271
Epoch 22/500
108/108 - 15s - loss: 0.7633 - accuracy: 0.5728 - val_loss: 0.6923 - val_accuracy: 0.5294
Epoch 23/500
108/108 - 15s - loss: 0.7470 - accuracy: 0.5850 - val_loss: 0.6916 - val_accuracy: 0.5318
Epoch 24/500
108/108 - 15s - loss: 0.7421 - accuracy: 0.5806 - val_loss: 0.6912 - val_accuracy: 0.5318
Epoch 25/500
108/108 - 15s - loss: 0.7599 - accuracy: 0.5588 - val_loss: 0.6912 - val_accuracy: 0.5271
Epoch 26/500
108/108 - 15s - loss: 0.7489 - accuracy: 0.5795 - val_loss: 0.6906 - val_accuracy: 0.5388
Epoch 27/500
108/108 - 15s - loss: 0.7480 - accuracy: 0.5728 - val_loss: 0.6899 - val_accuracy: 0.5388
Epoch 28/500
108/108 - 15s - loss: 0.7271 - accuracy: 0.5928 - val_loss: 0.6894 - val_accuracy: 0.5341
Epoch 29/500
108/108 - 15s - loss: 0.7285 - accuracy: 0.5896 - val_loss: 0.6899 - val_accuracy: 0.5365
Epoch 30/500
108/108 - 15s - loss: 0.7229 - accuracy: 0.5963 - val_loss: 0.6893 - val_accuracy: 0.5435
Epoch 31/500
108/108 - 15s - loss: 0.7061 - accuracy: 0.5992 - val_loss: 0.6884 - val_accuracy: 0.5412
Epoch 32/500
108/108 - 15s - loss: 0.7194 - accuracy: 0.5963 - val_loss: 0.6882 - val_accuracy: 0.5459
Epoch 33/500
108/108 - 15s - loss: 0.7076 - accuracy: 0.5987 - val_loss: 0.6881 - val_accuracy: 0.5529
Epoch 34/500
108/108 - 15s - loss: 0.6960 - accuracy: 0.6112 - val_loss: 0.6871 - val_accuracy: 0.5482
Epoch 35/500
108/108 - 15s - loss: 0.6900 - accuracy: 0.6103 - val_loss: 0.6871 - val_accuracy: 0.5529
Epoch 36/500
108/108 - 15s - loss: 0.6954 - accuracy: 0.6118 - val_loss: 0.6863 - val_accuracy: 0.5482
Epoch 37/500
108/108 - 15s - loss: 0.6922 - accuracy: 0.6193 - val_loss: 0.6861 - val_accuracy: 0.5506
Epoch 38/500
108/108 - 15s - loss: 0.6699 - accuracy: 0.6292 - val_loss: 0.6858 - val_accuracy: 0.5553
Epoch 39/500
108/108 - 15s - loss: 0.6804 - accuracy: 0.6286 - val_loss: 0.6848 - val_accuracy: 0.5600
Epoch 40/500
108/108 - 15s - loss: 0.6714 - accuracy: 0.6350 - val_loss: 0.6845 - val_accuracy: 0.5624
Epoch 41/500
108/108 - 15s - loss: 0.6574 - accuracy: 0.6391 - val_loss: 0.6843 - val_accuracy: 0.5694
Epoch 42/500
108/108 - 15s - loss: 0.6569 - accuracy: 0.6365 - val_loss: 0.6851 - val_accuracy: 0.5671
Epoch 43/500
108/108 - 15s - loss: 0.6757 - accuracy: 0.6336 - val_loss: 0.6846 - val_accuracy: 0.5647
Epoch 44/500
108/108 - 15s - loss: 0.6626 - accuracy: 0.6286 - val_loss: 0.6844 - val_accuracy: 0.5718
Epoch 45/500
108/108 - 15s - loss: 0.6504 - accuracy: 0.6508 - val_loss: 0.6845 - val_accuracy: 0.5741
Epoch 46/500
108/108 - 15s - loss: 0.6533 - accuracy: 0.6391 - val_loss: 0.6839 - val_accuracy: 0.5694
Epoch 47/500
108/108 - 15s - loss: 0.6506 - accuracy: 0.6359 - val_loss: 0.6844 - val_accuracy: 0.5694
Epoch 48/500
108/108 - 15s - loss: 0.6508 - accuracy: 0.6461 - val_loss: 0.6837 - val_accuracy: 0.5741
Epoch 49/500
108/108 - 15s - loss: 0.6273 - accuracy: 0.6639 - val_loss: 0.6836 - val_accuracy: 0.5788
Epoch 50/500
108/108 - 15s - loss: 0.6238 - accuracy: 0.6717 - val_loss: 0.6836 - val_accuracy: 0.5835
Epoch 51/500
108/108 - 15s - loss: 0.6241 - accuracy: 0.6665 - val_loss: 0.6838 - val_accuracy: 0.5859
Epoch 52/500
108/108 - 15s - loss: 0.6166 - accuracy: 0.6700 - val_loss: 0.6837 - val_accuracy: 0.5859
Epoch 53/500
108/108 - 15s - loss: 0.6142 - accuracy: 0.6761 - val_loss: 0.6839 - val_accuracy: 0.5835
Epoch 54/500
108/108 - 15s - loss: 0.6234 - accuracy: 0.6700 - val_loss: 0.6840 - val_accuracy: 0.5859
Epoch 55/500
108/108 - 15s - loss: 0.6114 - accuracy: 0.6767 - val_loss: 0.6846 - val_accuracy: 0.6000
Epoch 56/500
108/108 - 15s - loss: 0.5915 - accuracy: 0.6927 - val_loss: 0.6840 - val_accuracy: 0.5882
Epoch 57/500
108/108 - 15s - loss: 0.5870 - accuracy: 0.6845 - val_loss: 0.6845 - val_accuracy: 0.5906
Epoch 58/500
108/108 - 15s - loss: 0.5757 - accuracy: 0.6991 - val_loss: 0.6846 - val_accuracy: 0.5976
Epoch 59/500
108/108 - 15s - loss: 0.5746 - accuracy: 0.7005 - val_loss: 0.6844 - val_accuracy: 0.5882
Epoch 60/500
108/108 - 15s - loss: 0.5897 - accuracy: 0.6883 - val_loss: 0.6849 - val_accuracy: 0.5882
Epoch 61/500
108/108 - 15s - loss: 0.5745 - accuracy: 0.6997 - val_loss: 0.6848 - val_accuracy: 0.6000
Epoch 62/500
108/108 - 15s - loss: 0.5779 - accuracy: 0.6941 - val_loss: 0.6844 - val_accuracy: 0.5953
Epoch 63/500
108/108 - 15s - loss: 0.5488 - accuracy: 0.7258 - val_loss: 0.6850 - val_accuracy: 0.5906
Epoch 64/500
108/108 - 15s - loss: 0.5529 - accuracy: 0.7125 - val_loss: 0.6845 - val_accuracy: 0.5929
Epoch 65/500
108/108 - 15s - loss: 0.5510 - accuracy: 0.7328 - val_loss: 0.6849 - val_accuracy: 0.5929
Epoch 66/500
108/108 - 15s - loss: 0.5508 - accuracy: 0.7107 - val_loss: 0.6845 - val_accuracy: 0.5953
Epoch 67/500
108/108 - 15s - loss: 0.5322 - accuracy: 0.7334 - val_loss: 0.6859 - val_accuracy: 0.5953
Epoch 68/500
108/108 - 15s - loss: 0.5475 - accuracy: 0.7253 - val_loss: 0.6855 - val_accuracy: 0.6024
Epoch 69/500
108/108 - 15s - loss: 0.5426 - accuracy: 0.7253 - val_loss: 0.6856 - val_accuracy: 0.6000
Epoch 70/500
108/108 - 15s - loss: 0.5302 - accuracy: 0.7386 - val_loss: 0.6861 - val_accuracy: 0.6000
Epoch 71/500
108/108 - 15s - loss: 0.5253 - accuracy: 0.7430 - val_loss: 0.6862 - val_accuracy: 0.6000
Epoch 72/500
108/108 - 15s - loss: 0.5202 - accuracy: 0.7416 - val_loss: 0.6871 - val_accuracy: 0.6000
Epoch 73/500
108/108 - 15s - loss: 0.5073 - accuracy: 0.7532 - val_loss: 0.6874 - val_accuracy: 0.6024
Epoch 74/500
108/108 - 15s - loss: 0.5092 - accuracy: 0.7485 - val_loss: 0.6883 - val_accuracy: 0.6047
Epoch 75/500
108/108 - 15s - loss: 0.5038 - accuracy: 0.7471 - val_loss: 0.6898 - val_accuracy: 0.6071
Epoch 76/500
108/108 - 15s - loss: 0.4925 - accuracy: 0.7593 - val_loss: 0.6904 - val_accuracy: 0.6071
Epoch 77/500
108/108 - 15s - loss: 0.4805 - accuracy: 0.7611 - val_loss: 0.6900 - val_accuracy: 0.6141
Epoch 78/500
108/108 - 15s - loss: 0.4824 - accuracy: 0.7686 - val_loss: 0.6910 - val_accuracy: 0.6047
Epoch 79/500
108/108 - 15s - loss: 0.4859 - accuracy: 0.7660 - val_loss: 0.6927 - val_accuracy: 0.6071
Epoch 80/500
108/108 - 15s - loss: 0.4798 - accuracy: 0.7707 - val_loss: 0.6939 - val_accuracy: 0.6071
Epoch 81/500
108/108 - 15s - loss: 0.4702 - accuracy: 0.7803 - val_loss: 0.6950 - val_accuracy: 0.6071
Epoch 82/500
108/108 - 15s - loss: 0.4689 - accuracy: 0.7753 - val_loss: 0.6954 - val_accuracy: 0.6094
Epoch 83/500
108/108 - 15s - loss: 0.4678 - accuracy: 0.7820 - val_loss: 0.6960 - val_accuracy: 0.6165
Epoch 84/500
108/108 - 15s - loss: 0.4521 - accuracy: 0.7899 - val_loss: 0.6967 - val_accuracy: 0.6141
Epoch 85/500
108/108 - 15s - loss: 0.4418 - accuracy: 0.7937 - val_loss: 0.6996 - val_accuracy: 0.6071
Epoch 86/500
108/108 - 15s - loss: 0.4447 - accuracy: 0.7881 - val_loss: 0.6993 - val_accuracy: 0.6118
Epoch 87/500
108/108 - 15s - loss: 0.4320 - accuracy: 0.7942 - val_loss: 0.6999 - val_accuracy: 0.6118
Epoch 88/500
108/108 - 15s - loss: 0.4268 - accuracy: 0.8056 - val_loss: 0.7010 - val_accuracy: 0.6118
Epoch 89/500
108/108 - 15s - loss: 0.4114 - accuracy: 0.8062 - val_loss: 0.7024 - val_accuracy: 0.6094
Epoch 90/500
108/108 - 15s - loss: 0.4359 - accuracy: 0.7931 - val_loss: 0.7050 - val_accuracy: 0.6094
Epoch 91/500
108/108 - 15s - loss: 0.4180 - accuracy: 0.8091 - val_loss: 0.7053 - val_accuracy: 0.6047
Epoch 92/500
108/108 - 15s - loss: 0.4147 - accuracy: 0.8091 - val_loss: 0.7069 - val_accuracy: 0.6071
Epoch 93/500
108/108 - 15s - loss: 0.3858 - accuracy: 0.8193 - val_loss: 0.7084 - val_accuracy: 0.6047
Epoch 94/500
108/108 - 15s - loss: 0.3978 - accuracy: 0.8152 - val_loss: 0.7091 - val_accuracy: 0.6047
Epoch 95/500
108/108 - 15s - loss: 0.3755 - accuracy: 0.8300 - val_loss: 0.7114 - val_accuracy: 0.6000
Epoch 96/500
108/108 - 15s - loss: 0.3748 - accuracy: 0.8283 - val_loss: 0.7115 - val_accuracy: 0.6024
Epoch 97/500
108/108 - 15s - loss: 0.3790 - accuracy: 0.8300 - val_loss: 0.7138 - val_accuracy: 0.6047
Epoch 98/500
108/108 - 15s - loss: 0.3724 - accuracy: 0.8347 - val_loss: 0.7158 - val_accuracy: 0.6047
Epoch 99/500
108/108 - 15s - loss: 0.3646 - accuracy: 0.8405 - val_loss: 0.7173 - val_accuracy: 0.6071
Epoch 100/500
108/108 - 15s - loss: 0.3659 - accuracy: 0.8309 - val_loss: 0.7183 - val_accuracy: 0.6047
Epoch 101/500
108/108 - 15s - loss: 0.3471 - accuracy: 0.8487 - val_loss: 0.7199 - val_accuracy: 0.6071
Epoch 102/500
108/108 - 15s - loss: 0.3607 - accuracy: 0.8449 - val_loss: 0.7212 - val_accuracy: 0.6094
Epoch 103/500
108/108 - 15s - loss: 0.3618 - accuracy: 0.8402 - val_loss: 0.7226 - val_accuracy: 0.6047
========================================
save_weights
h5_weights/ME.pp/embedding_cnn_two_branch.h5
========================================

end time >>> Sun Oct  3 19:55:54 2021

end time >>> Sun Oct  3 19:55:54 2021

end time >>> Sun Oct  3 19:55:54 2021

end time >>> Sun Oct  3 19:55:54 2021

end time >>> Sun Oct  3 19:55:54 2021












args.model = embedding_cnn_two_branch
time used = 1528.5289311408997


