************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 00:55:04 2021

begin time >>> Sun Oct  3 00:55:04 2021

begin time >>> Sun Oct  3 00:55:04 2021

begin time >>> Sun Oct  3 00:55:04 2021

begin time >>> Sun Oct  3 00:55:04 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_cnn_one_branch
args.type = train
args.name = GM.po
args.length = 10001
===========================


-> make new folder: h5_weights/GM.po
-> make new folder: result/GM.po/onehot_cnn_one_branch
-> make new folder: result/GM.po/onehot_cnn_two_branch
-> make new folder: result/GM.po/onehot_embedding_dense
-> make new folder: result/GM.po/onehot_dense
-> make new folder: result/GM.po/onehot_resnet18
-> make new folder: result/GM.po/onehot_resnet34
-> make new folder: result/GM.po/embedding_cnn_one_branch
-> make new folder: result/GM.po/embedding_cnn_two_branch
-> make new folder: result/GM.po/embedding_dense
-> make new folder: result/GM.po/onehot_embedding_cnn_one_branch
-> make new folder: result/GM.po/onehot_embedding_cnn_two_branch
########################################
gen_name
GM.po
########################################

########################################
model_name
onehot_cnn_one_branch
########################################

Found 4232 images belonging to 2 classes.
Found 522 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 5001, 5, 64)       1600      
_________________________________________________________________
batch_normalization (BatchNo (None, 5001, 5, 64)       256       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1251, 5, 64)       98368     
_________________________________________________________________
batch_normalization_1 (Batch (None, 1251, 5, 64)       256       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 313, 5, 64)        98368     
_________________________________________________________________
batch_normalization_2 (Batch (None, 313, 5, 64)        256       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 156, 5, 64)        0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 156, 5, 64)        256       
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 39, 5, 128)        196736    
_________________________________________________________________
batch_normalization_4 (Batch (None, 39, 5, 128)        512       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 10, 5, 128)        393344    
_________________________________________________________________
batch_normalization_5 (Batch (None, 10, 5, 128)        512       
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 3, 5, 128)         393344    
_________________________________________________________________
batch_normalization_6 (Batch (None, 3, 5, 128)         512       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 1, 5, 128)         0         
_________________________________________________________________
flatten (Flatten)            (None, 640)               0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 640)               2560      
_________________________________________________________________
dense (Dense)                (None, 512)               328192    
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
dense_3 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 2,041,410
Trainable params: 2,038,850
Non-trainable params: 2,560
_________________________________________________________________
Epoch 1/500
132/132 - 112s - loss: 0.7996 - accuracy: 0.5012 - val_loss: 0.6961 - val_accuracy: 0.4785
Epoch 2/500
132/132 - 16s - loss: 0.7102 - accuracy: 0.5669 - val_loss: 0.6956 - val_accuracy: 0.5293
Epoch 3/500
132/132 - 16s - loss: 0.6774 - accuracy: 0.5912 - val_loss: 0.7642 - val_accuracy: 0.5078
Epoch 4/500
132/132 - 16s - loss: 0.6145 - accuracy: 0.6564 - val_loss: 0.8185 - val_accuracy: 0.5137
Epoch 5/500
132/132 - 16s - loss: 0.5536 - accuracy: 0.7131 - val_loss: 0.7689 - val_accuracy: 0.5273
Epoch 6/500
132/132 - 16s - loss: 0.4449 - accuracy: 0.7971 - val_loss: 1.0620 - val_accuracy: 0.5254
Epoch 7/500
132/132 - 16s - loss: 0.3148 - accuracy: 0.8743 - val_loss: 0.8473 - val_accuracy: 0.5977
Epoch 8/500
132/132 - 16s - loss: 0.2061 - accuracy: 0.9257 - val_loss: 0.8873 - val_accuracy: 0.6289
Epoch 9/500
132/132 - 16s - loss: 0.1197 - accuracy: 0.9629 - val_loss: 1.0706 - val_accuracy: 0.6406
Epoch 10/500
132/132 - 16s - loss: 0.0725 - accuracy: 0.9762 - val_loss: 2.0565 - val_accuracy: 0.5273
Epoch 11/500
132/132 - 16s - loss: 0.0468 - accuracy: 0.9857 - val_loss: 1.0559 - val_accuracy: 0.6621
Epoch 12/500
132/132 - 16s - loss: 0.0326 - accuracy: 0.9912 - val_loss: 1.6131 - val_accuracy: 0.6113
Epoch 13/500
132/132 - 16s - loss: 0.0297 - accuracy: 0.9900 - val_loss: 1.2650 - val_accuracy: 0.6465
Epoch 14/500
132/132 - 16s - loss: 0.0280 - accuracy: 0.9924 - val_loss: 2.8245 - val_accuracy: 0.5488
Epoch 15/500
132/132 - 16s - loss: 0.0195 - accuracy: 0.9936 - val_loss: 1.7046 - val_accuracy: 0.6641
Epoch 16/500
132/132 - 16s - loss: 0.0181 - accuracy: 0.9943 - val_loss: 2.8845 - val_accuracy: 0.5566
Epoch 17/500
132/132 - 16s - loss: 0.0149 - accuracy: 0.9948 - val_loss: 4.9011 - val_accuracy: 0.5020
Epoch 18/500
132/132 - 16s - loss: 0.0210 - accuracy: 0.9938 - val_loss: 1.6262 - val_accuracy: 0.6836
Epoch 19/500
132/132 - 16s - loss: 0.0213 - accuracy: 0.9936 - val_loss: 1.7662 - val_accuracy: 0.6309
Epoch 20/500
132/132 - 15s - loss: 0.0359 - accuracy: 0.9900 - val_loss: 9.3728 - val_accuracy: 0.5059
Epoch 21/500
132/132 - 16s - loss: 0.0269 - accuracy: 0.9905 - val_loss: 8.9292 - val_accuracy: 0.4961
Epoch 22/500
132/132 - 15s - loss: 0.0116 - accuracy: 0.9969 - val_loss: 3.9447 - val_accuracy: 0.5312
Epoch 23/500
132/132 - 16s - loss: 0.0141 - accuracy: 0.9957 - val_loss: 3.7696 - val_accuracy: 0.5801
Epoch 24/500
132/132 - 15s - loss: 0.0126 - accuracy: 0.9952 - val_loss: 2.7470 - val_accuracy: 0.6113
Epoch 25/500
132/132 - 16s - loss: 0.0144 - accuracy: 0.9960 - val_loss: 1.8939 - val_accuracy: 0.6543
Epoch 26/500
132/132 - 15s - loss: 0.0176 - accuracy: 0.9938 - val_loss: 7.5160 - val_accuracy: 0.5020
Epoch 27/500
132/132 - 15s - loss: 0.0143 - accuracy: 0.9952 - val_loss: 5.6200 - val_accuracy: 0.5488
Epoch 28/500
132/132 - 16s - loss: 0.0190 - accuracy: 0.9940 - val_loss: 2.1159 - val_accuracy: 0.6484
========================================
save_weights
h5_weights/GM.po/onehot_cnn_one_branch.h5
========================================

end time >>> Sun Oct  3 01:04:14 2021

end time >>> Sun Oct  3 01:04:14 2021

end time >>> Sun Oct  3 01:04:14 2021

end time >>> Sun Oct  3 01:04:14 2021

end time >>> Sun Oct  3 01:04:14 2021












args.model = onehot_cnn_one_branch
time used = 549.7104511260986


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 01:04:15 2021

begin time >>> Sun Oct  3 01:04:15 2021

begin time >>> Sun Oct  3 01:04:15 2021

begin time >>> Sun Oct  3 01:04:15 2021

begin time >>> Sun Oct  3 01:04:15 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_cnn_two_branch
args.type = train
args.name = GM.po
args.length = 10001
===========================


-> h5_weights/GM.po folder already exist. pass.
-> result/GM.po/onehot_cnn_one_branch folder already exist. pass.
-> result/GM.po/onehot_cnn_two_branch folder already exist. pass.
-> result/GM.po/onehot_embedding_dense folder already exist. pass.
-> result/GM.po/onehot_dense folder already exist. pass.
-> result/GM.po/onehot_resnet18 folder already exist. pass.
-> result/GM.po/onehot_resnet34 folder already exist. pass.
-> result/GM.po/embedding_cnn_one_branch folder already exist. pass.
-> result/GM.po/embedding_cnn_two_branch folder already exist. pass.
-> result/GM.po/embedding_dense folder already exist. pass.
-> result/GM.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/GM.po/onehot_embedding_cnn_two_branch folder already exist. pass.
Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001, 5, 1) 0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001, 5, 1) 0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 2501, 5, 64)  1600        input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 2501, 5, 64)  1600        input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 2501, 5, 64)  256         conv2d[0][0]                     
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 2501, 5, 64)  256         conv2d_6[0][0]                   
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 626, 5, 64)   98368       batch_normalization[0][0]        
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 626, 5, 64)   98368       batch_normalization_8[0][0]      
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 626, 5, 64)   256         conv2d_1[0][0]                   
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 626, 5, 64)   256         conv2d_7[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 157, 5, 64)   98368       batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 157, 5, 64)   98368       batch_normalization_9[0][0]      
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 157, 5, 64)   256         conv2d_2[0][0]                   
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 157, 5, 64)   256         conv2d_8[0][0]                   
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 78, 5, 64)    0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 78, 5, 64)    0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 78, 5, 64)    256         max_pooling2d[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 78, 5, 64)    256         max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 20, 5, 128)   196736      batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 20, 5, 128)   196736      batch_normalization_11[0][0]     
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 20, 5, 128)   512         conv2d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 20, 5, 128)   512         conv2d_9[0][0]                   
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 5, 5, 128)    393344      batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 5, 5, 128)    393344      batch_normalization_12[0][0]     
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 5, 5, 128)    512         conv2d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 5, 5, 128)    512         conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 2, 5, 128)    393344      batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 2, 5, 128)    393344      batch_normalization_13[0][0]     
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 2, 5, 128)    512         conv2d_5[0][0]                   
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 2, 5, 128)    512         conv2d_11[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 1, 5, 128)    0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 1, 5, 128)    0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
flatten (Flatten)               (None, 640)          0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 640)          0           max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 640)          2560        flatten[0][0]                    
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 640)          2560        flatten_1[0][0]                  
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          328192      batch_normalization_7[0][0]      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          328192      batch_normalization_15[0][0]     
__________________________________________________________________________________________________
dropout (Dropout)               (None, 512)          0           dense[0][0]                      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 1024)         0           dropout[0][0]                    
                                                                 dropout_1[0][0]                  
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          524800      concatenate[0][0]                
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 512)          262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 2)            1026        dense_3[0][0]                    
==================================================================================================
Total params: 3,818,626
Trainable params: 3,813,506
Non-trainable params: 5,120
__________________________________________________________________________________________________
Found 4232 images belonging to 2 classes.
Found 4232 images belonging to 2 classes.
Epoch 1/500
Found 522 images belonging to 2 classes.
Found 522 images belonging to 2 classes.
1535/1535 - 386s - loss: 0.3394 - accuracy: 0.8279 - val_loss: 0.9803 - val_accuracy: 0.7070
Epoch 2/500
1535/1535 - 221s - loss: 0.0219 - accuracy: 0.9928 - val_loss: 2.6628 - val_accuracy: 0.6223
Epoch 3/500
1535/1535 - 220s - loss: 0.0215 - accuracy: 0.9933 - val_loss: 1.9187 - val_accuracy: 0.6508
Epoch 4/500
1535/1535 - 219s - loss: 0.0151 - accuracy: 0.9951 - val_loss: 6.6720 - val_accuracy: 0.5559
Epoch 5/500
1535/1535 - 219s - loss: 0.0105 - accuracy: 0.9971 - val_loss: 6.8122 - val_accuracy: 0.5140
Epoch 6/500
1535/1535 - 221s - loss: 0.0147 - accuracy: 0.9953 - val_loss: 2.5334 - val_accuracy: 0.6518
Epoch 7/500
1535/1535 - 225s - loss: 0.0122 - accuracy: 0.9964 - val_loss: 2.6919 - val_accuracy: 0.7180
Epoch 8/500
1535/1535 - 224s - loss: 0.0101 - accuracy: 0.9967 - val_loss: 4.6027 - val_accuracy: 0.5496
Epoch 9/500
1535/1535 - 221s - loss: 0.0070 - accuracy: 0.9981 - val_loss: 1.7805 - val_accuracy: 0.7351
Epoch 10/500
1535/1535 - 223s - loss: 0.0073 - accuracy: 0.9979 - val_loss: 1.6706 - val_accuracy: 0.7533
Epoch 11/500
1535/1535 - 222s - loss: 0.0034 - accuracy: 0.9991 - val_loss: 4.2348 - val_accuracy: 0.6296
Epoch 12/500
1535/1535 - 224s - loss: 0.0085 - accuracy: 0.9981 - val_loss: 1.8231 - val_accuracy: 0.7261
Epoch 13/500
1535/1535 - 223s - loss: 0.0053 - accuracy: 0.9986 - val_loss: 3.8517 - val_accuracy: 0.6249
Epoch 14/500
1535/1535 - 222s - loss: 0.0040 - accuracy: 0.9989 - val_loss: 2.2952 - val_accuracy: 0.7110
Epoch 15/500
1535/1535 - 223s - loss: 0.0085 - accuracy: 0.9983 - val_loss: 1.6719 - val_accuracy: 0.7460
Epoch 16/500
1535/1535 - 223s - loss: 0.0048 - accuracy: 0.9990 - val_loss: 1.9022 - val_accuracy: 0.7111
Epoch 17/500
1535/1535 - 225s - loss: 0.0074 - accuracy: 0.9986 - val_loss: 1.6335 - val_accuracy: 0.7472
Epoch 18/500
1535/1535 - 223s - loss: 0.0053 - accuracy: 0.9986 - val_loss: 1.3965 - val_accuracy: 0.7359
Epoch 19/500
1535/1535 - 222s - loss: 0.0029 - accuracy: 0.9991 - val_loss: 1.6765 - val_accuracy: 0.7342
Epoch 20/500
1535/1535 - 224s - loss: 0.0066 - accuracy: 0.9988 - val_loss: 1.5965 - val_accuracy: 0.6930
========================================
save_weights
h5_weights/GM.po/onehot_cnn_two_branch.h5
========================================

end time >>> Sun Oct  3 02:21:22 2021

end time >>> Sun Oct  3 02:21:22 2021

end time >>> Sun Oct  3 02:21:22 2021

end time >>> Sun Oct  3 02:21:22 2021

end time >>> Sun Oct  3 02:21:22 2021












args.model = onehot_cnn_two_branch
time used = 4626.767618894577


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 02:21:23 2021

begin time >>> Sun Oct  3 02:21:23 2021

begin time >>> Sun Oct  3 02:21:23 2021

begin time >>> Sun Oct  3 02:21:23 2021

begin time >>> Sun Oct  3 02:21:23 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_dense
args.type = train
args.name = GM.po
args.length = 10001
===========================


-> h5_weights/GM.po folder already exist. pass.
-> result/GM.po/onehot_cnn_one_branch folder already exist. pass.
-> result/GM.po/onehot_cnn_two_branch folder already exist. pass.
-> result/GM.po/onehot_embedding_dense folder already exist. pass.
-> result/GM.po/onehot_dense folder already exist. pass.
-> result/GM.po/onehot_resnet18 folder already exist. pass.
-> result/GM.po/onehot_resnet34 folder already exist. pass.
-> result/GM.po/embedding_cnn_one_branch folder already exist. pass.
-> result/GM.po/embedding_cnn_two_branch folder already exist. pass.
-> result/GM.po/embedding_dense folder already exist. pass.
-> result/GM.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/GM.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
GM.po
########################################

########################################
model_name
onehot_dense
########################################

Found 4232 images belonging to 2 classes.
Found 522 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 100010)            0         
_________________________________________________________________
batch_normalization (BatchNo (None, 100010)            400040    
_________________________________________________________________
dense (Dense)                (None, 512)               51205632  
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_3 (Batch (None, 512)               2048      
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_3 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 52,402,858
Trainable params: 52,198,742
Non-trainable params: 204,116
_________________________________________________________________
Epoch 1/500
132/132 - 28s - loss: 0.8184 - accuracy: 0.5333 - val_loss: 0.6201 - val_accuracy: 0.7012
Epoch 2/500
132/132 - 12s - loss: 0.6597 - accuracy: 0.6302 - val_loss: 0.5817 - val_accuracy: 0.6836
Epoch 3/500
132/132 - 12s - loss: 0.5671 - accuracy: 0.7045 - val_loss: 0.6139 - val_accuracy: 0.6777
Epoch 4/500
132/132 - 12s - loss: 0.4492 - accuracy: 0.7881 - val_loss: 0.6999 - val_accuracy: 0.6680
Epoch 5/500
132/132 - 12s - loss: 0.3380 - accuracy: 0.8576 - val_loss: 0.8712 - val_accuracy: 0.6484
Epoch 6/500
132/132 - 12s - loss: 0.2282 - accuracy: 0.9114 - val_loss: 1.0460 - val_accuracy: 0.6465
Epoch 7/500
132/132 - 12s - loss: 0.1835 - accuracy: 0.9329 - val_loss: 1.2084 - val_accuracy: 0.6445
Epoch 8/500
132/132 - 12s - loss: 0.1294 - accuracy: 0.9562 - val_loss: 1.3067 - val_accuracy: 0.6621
Epoch 9/500
132/132 - 12s - loss: 0.1048 - accuracy: 0.9669 - val_loss: 1.4285 - val_accuracy: 0.6445
Epoch 10/500
132/132 - 12s - loss: 0.0952 - accuracy: 0.9624 - val_loss: 1.5594 - val_accuracy: 0.6406
Epoch 11/500
132/132 - 12s - loss: 0.0785 - accuracy: 0.9750 - val_loss: 1.6174 - val_accuracy: 0.6523
========================================
save_weights
h5_weights/GM.po/onehot_dense.h5
========================================

end time >>> Sun Oct  3 02:24:05 2021

end time >>> Sun Oct  3 02:24:05 2021

end time >>> Sun Oct  3 02:24:05 2021

end time >>> Sun Oct  3 02:24:05 2021

end time >>> Sun Oct  3 02:24:05 2021












args.model = onehot_dense
time used = 161.80831003189087


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 02:24:05 2021

begin time >>> Sun Oct  3 02:24:05 2021

begin time >>> Sun Oct  3 02:24:05 2021

begin time >>> Sun Oct  3 02:24:05 2021

begin time >>> Sun Oct  3 02:24:05 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_resnet18
args.type = train
args.name = GM.po
args.length = 10001
===========================


-> h5_weights/GM.po folder already exist. pass.
-> result/GM.po/onehot_cnn_one_branch folder already exist. pass.
-> result/GM.po/onehot_cnn_two_branch folder already exist. pass.
-> result/GM.po/onehot_embedding_dense folder already exist. pass.
-> result/GM.po/onehot_dense folder already exist. pass.
-> result/GM.po/onehot_resnet18 folder already exist. pass.
-> result/GM.po/onehot_resnet34 folder already exist. pass.
-> result/GM.po/embedding_cnn_one_branch folder already exist. pass.
-> result/GM.po/embedding_cnn_two_branch folder already exist. pass.
-> result/GM.po/embedding_dense folder already exist. pass.
-> result/GM.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/GM.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
GM.po
########################################

########################################
model_name
onehot_resnet18
########################################

Found 4232 images belonging to 2 classes.
Found 522 images belonging to 2 classes.
Model: "res_net_type_i"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              multiple                  1600      
_________________________________________________________________
batch_normalization (BatchNo multiple                  256       
_________________________________________________________________
max_pooling2d (MaxPooling2D) multiple                  0         
_________________________________________________________________
sequential (Sequential)      (None, 313, 1, 64)        398912    
_________________________________________________________________
sequential_2 (Sequential)    (None, 20, 1, 64)         398912    
_________________________________________________________________
sequential_4 (Sequential)    (None, 2, 1, 64)          398912    
_________________________________________________________________
sequential_6 (Sequential)    (None, 1, 1, 64)          398912    
_________________________________________________________________
global_average_pooling2d (Gl multiple                  0         
_________________________________________________________________
dense (Dense)                multiple                  130       
=================================================================
Total params: 1,597,634
Trainable params: 1,594,946
Non-trainable params: 2,688
_________________________________________________________________
Epoch 1/500
132/132 - 17s - loss: 0.8224 - accuracy: 0.4917 - val_loss: 0.6964 - val_accuracy: 0.4980
Epoch 2/500
132/132 - 16s - loss: 0.6069 - accuracy: 0.6640 - val_loss: 0.6969 - val_accuracy: 0.4980
Epoch 3/500
132/132 - 16s - loss: 0.5139 - accuracy: 0.7555 - val_loss: 0.7134 - val_accuracy: 0.5059
Epoch 4/500
132/132 - 16s - loss: 0.4079 - accuracy: 0.8393 - val_loss: 0.7895 - val_accuracy: 0.4844
Epoch 5/500
132/132 - 16s - loss: 0.3228 - accuracy: 0.8862 - val_loss: 0.8422 - val_accuracy: 0.5293
Epoch 6/500
132/132 - 16s - loss: 0.2712 - accuracy: 0.9043 - val_loss: 0.9235 - val_accuracy: 0.5156
Epoch 7/500
132/132 - 16s - loss: 0.1902 - accuracy: 0.9421 - val_loss: 0.8799 - val_accuracy: 0.5645
Epoch 8/500
132/132 - 16s - loss: 0.1502 - accuracy: 0.9569 - val_loss: 0.9847 - val_accuracy: 0.5703
Epoch 9/500
132/132 - 16s - loss: 0.1231 - accuracy: 0.9640 - val_loss: 1.0721 - val_accuracy: 0.5410
Epoch 10/500
132/132 - 16s - loss: 0.1271 - accuracy: 0.9586 - val_loss: 1.0395 - val_accuracy: 0.5605
Epoch 11/500
132/132 - 16s - loss: 0.1165 - accuracy: 0.9643 - val_loss: 1.1226 - val_accuracy: 0.5371
Epoch 12/500
132/132 - 16s - loss: 0.0990 - accuracy: 0.9686 - val_loss: 1.1136 - val_accuracy: 0.5996
Epoch 13/500
132/132 - 16s - loss: 0.0815 - accuracy: 0.9743 - val_loss: 1.0677 - val_accuracy: 0.6172
Epoch 14/500
132/132 - 16s - loss: 0.0872 - accuracy: 0.9717 - val_loss: 1.1823 - val_accuracy: 0.5918
Epoch 15/500
132/132 - 16s - loss: 0.0875 - accuracy: 0.9721 - val_loss: 1.1814 - val_accuracy: 0.6035
Epoch 16/500
132/132 - 16s - loss: 0.0752 - accuracy: 0.9745 - val_loss: 1.1669 - val_accuracy: 0.6113
Epoch 17/500
132/132 - 16s - loss: 0.0801 - accuracy: 0.9745 - val_loss: 1.2661 - val_accuracy: 0.5996
Epoch 18/500
132/132 - 16s - loss: 0.0832 - accuracy: 0.9707 - val_loss: 1.1441 - val_accuracy: 0.6230
Epoch 19/500
132/132 - 16s - loss: 0.1047 - accuracy: 0.9624 - val_loss: 1.2502 - val_accuracy: 0.5996
Epoch 20/500
132/132 - 16s - loss: 0.0933 - accuracy: 0.9676 - val_loss: 1.2361 - val_accuracy: 0.6309
Epoch 21/500
132/132 - 16s - loss: 0.0935 - accuracy: 0.9638 - val_loss: 1.2488 - val_accuracy: 0.5918
Epoch 22/500
132/132 - 16s - loss: 0.0601 - accuracy: 0.9793 - val_loss: 1.1920 - val_accuracy: 0.6367
Epoch 23/500
132/132 - 16s - loss: 0.0458 - accuracy: 0.9869 - val_loss: 1.1991 - val_accuracy: 0.6309
Epoch 24/500
132/132 - 16s - loss: 0.0409 - accuracy: 0.9864 - val_loss: 1.1658 - val_accuracy: 0.6309
Epoch 25/500
132/132 - 16s - loss: 0.0472 - accuracy: 0.9852 - val_loss: 1.2809 - val_accuracy: 0.5977
Epoch 26/500
132/132 - 16s - loss: 0.0574 - accuracy: 0.9819 - val_loss: 1.2418 - val_accuracy: 0.6270
Epoch 27/500
132/132 - 16s - loss: 0.0495 - accuracy: 0.9836 - val_loss: 1.3175 - val_accuracy: 0.6230
Epoch 28/500
132/132 - 16s - loss: 0.0397 - accuracy: 0.9862 - val_loss: 1.3477 - val_accuracy: 0.6270
Epoch 29/500
132/132 - 16s - loss: 0.0399 - accuracy: 0.9876 - val_loss: 1.2559 - val_accuracy: 0.6348
Epoch 30/500
132/132 - 16s - loss: 0.0298 - accuracy: 0.9907 - val_loss: 1.2301 - val_accuracy: 0.6328
Epoch 31/500
132/132 - 16s - loss: 0.0357 - accuracy: 0.9881 - val_loss: 1.2648 - val_accuracy: 0.6406
Epoch 32/500
132/132 - 16s - loss: 0.0396 - accuracy: 0.9869 - val_loss: 1.3380 - val_accuracy: 0.6133
Epoch 33/500
132/132 - 16s - loss: 0.0284 - accuracy: 0.9900 - val_loss: 1.3073 - val_accuracy: 0.6191
Epoch 34/500
132/132 - 16s - loss: 0.0295 - accuracy: 0.9917 - val_loss: 1.3167 - val_accuracy: 0.6211
Epoch 35/500
132/132 - 16s - loss: 0.0263 - accuracy: 0.9910 - val_loss: 1.2682 - val_accuracy: 0.6367
Epoch 36/500
132/132 - 16s - loss: 0.0238 - accuracy: 0.9921 - val_loss: 1.3309 - val_accuracy: 0.6484
Epoch 37/500
132/132 - 16s - loss: 0.0359 - accuracy: 0.9881 - val_loss: 1.4021 - val_accuracy: 0.6387
Epoch 38/500
132/132 - 16s - loss: 0.0565 - accuracy: 0.9788 - val_loss: 1.3064 - val_accuracy: 0.6660
Epoch 39/500
132/132 - 16s - loss: 0.0656 - accuracy: 0.9755 - val_loss: 1.3797 - val_accuracy: 0.6211
Epoch 40/500
132/132 - 16s - loss: 0.0544 - accuracy: 0.9824 - val_loss: 1.3336 - val_accuracy: 0.6367
Epoch 41/500
132/132 - 16s - loss: 0.0450 - accuracy: 0.9845 - val_loss: 1.3448 - val_accuracy: 0.6562
Epoch 42/500
132/132 - 16s - loss: 0.0406 - accuracy: 0.9883 - val_loss: 1.3308 - val_accuracy: 0.6426
Epoch 43/500
132/132 - 16s - loss: 0.0292 - accuracy: 0.9902 - val_loss: 1.2277 - val_accuracy: 0.6543
Epoch 44/500
132/132 - 16s - loss: 0.0213 - accuracy: 0.9929 - val_loss: 1.2482 - val_accuracy: 0.6562
Epoch 45/500
132/132 - 16s - loss: 0.0193 - accuracy: 0.9948 - val_loss: 1.2107 - val_accuracy: 0.6445
Epoch 46/500
132/132 - 17s - loss: 0.0250 - accuracy: 0.9933 - val_loss: 1.3139 - val_accuracy: 0.6680
Epoch 47/500
132/132 - 16s - loss: 0.0167 - accuracy: 0.9948 - val_loss: 1.3888 - val_accuracy: 0.6270
Epoch 48/500
132/132 - 16s - loss: 0.0101 - accuracy: 0.9976 - val_loss: 1.3342 - val_accuracy: 0.6582
Epoch 49/500
132/132 - 17s - loss: 0.0129 - accuracy: 0.9964 - val_loss: 1.3617 - val_accuracy: 0.6523
Epoch 50/500
132/132 - 16s - loss: 0.0193 - accuracy: 0.9943 - val_loss: 1.2748 - val_accuracy: 0.6680
Epoch 51/500
132/132 - 17s - loss: 0.0241 - accuracy: 0.9929 - val_loss: 1.3247 - val_accuracy: 0.6621
Epoch 52/500
132/132 - 16s - loss: 0.0190 - accuracy: 0.9933 - val_loss: 1.3801 - val_accuracy: 0.6641
Epoch 53/500
132/132 - 16s - loss: 0.0371 - accuracy: 0.9895 - val_loss: 1.5360 - val_accuracy: 0.6172
Epoch 54/500
132/132 - 16s - loss: 0.0433 - accuracy: 0.9838 - val_loss: 1.4187 - val_accuracy: 0.6504
Epoch 55/500
132/132 - 17s - loss: 0.0419 - accuracy: 0.9857 - val_loss: 1.2781 - val_accuracy: 0.6562
Epoch 56/500
132/132 - 16s - loss: 0.0330 - accuracy: 0.9888 - val_loss: 1.2852 - val_accuracy: 0.6445
========================================
save_weights
h5_weights/GM.po/onehot_resnet18.h5
========================================

end time >>> Sun Oct  3 02:39:33 2021

end time >>> Sun Oct  3 02:39:33 2021

end time >>> Sun Oct  3 02:39:33 2021

end time >>> Sun Oct  3 02:39:33 2021

end time >>> Sun Oct  3 02:39:33 2021












args.model = onehot_resnet18
time used = 927.6139581203461


