************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 10:05:50 2021

begin time >>> Mon Oct  4 10:05:50 2021

begin time >>> Mon Oct  4 10:05:50 2021

begin time >>> Mon Oct  4 10:05:50 2021

begin time >>> Mon Oct  4 10:05:50 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_dense
args.type = train
args.name = SX.po
args.length = 10001
===========================


-> h5_weights/SX.po folder already exist. pass.
-> result/SX.po/onehot_cnn_one_branch folder already exist. pass.
-> result/SX.po/onehot_cnn_two_branch folder already exist. pass.
-> result/SX.po/onehot_embedding_dense folder already exist. pass.
-> result/SX.po/onehot_dense folder already exist. pass.
-> result/SX.po/onehot_resnet18 folder already exist. pass.
-> result/SX.po/onehot_resnet34 folder already exist. pass.
-> result/SX.po/embedding_cnn_one_branch folder already exist. pass.
-> result/SX.po/embedding_cnn_two_branch folder already exist. pass.
-> result/SX.po/embedding_dense folder already exist. pass.
-> result/SX.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/SX.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
SX.po
########################################

########################################
model_name
onehot_embedding_dense
########################################

Found 25666 images belonging to 2 classes.
Found 3170 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 20002, 5, 1, 8)    32776     
_________________________________________________________________
flatten (Flatten)            (None, 800080)            0         
_________________________________________________________________
batch_normalization (BatchNo (None, 800080)            3200320   
_________________________________________________________________
dense (Dense)                (None, 512)               409641472 
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_3 (Batch (None, 512)               2048      
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_3 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 413,671,754
Trainable params: 412,067,498
Non-trainable params: 1,604,256
_________________________________________________________________
Epoch 1/500
802/802 - 201s - loss: 0.6887 - accuracy: 0.6236 - val_loss: 0.6987 - val_accuracy: 0.6174
Epoch 2/500
802/802 - 149s - loss: 0.5468 - accuracy: 0.7337 - val_loss: 0.9811 - val_accuracy: 0.5900
Epoch 3/500
802/802 - 148s - loss: 0.4367 - accuracy: 0.8025 - val_loss: 1.3635 - val_accuracy: 0.5843
Epoch 4/500
802/802 - 148s - loss: 0.3541 - accuracy: 0.8474 - val_loss: 1.5833 - val_accuracy: 0.5890
Epoch 5/500
802/802 - 148s - loss: 0.2965 - accuracy: 0.8760 - val_loss: 1.7194 - val_accuracy: 0.6054
Epoch 6/500
802/802 - 148s - loss: 0.2539 - accuracy: 0.8940 - val_loss: 1.8040 - val_accuracy: 0.6124
Epoch 7/500
802/802 - 148s - loss: 0.2201 - accuracy: 0.9095 - val_loss: 1.9025 - val_accuracy: 0.6143
Epoch 8/500
802/802 - 149s - loss: 0.1833 - accuracy: 0.9276 - val_loss: 1.9574 - val_accuracy: 0.6241
Epoch 9/500
802/802 - 148s - loss: 0.1654 - accuracy: 0.9322 - val_loss: 1.9796 - val_accuracy: 0.6237
Epoch 10/500
802/802 - 150s - loss: 0.1471 - accuracy: 0.9419 - val_loss: 2.0411 - val_accuracy: 0.6285
Epoch 11/500
802/802 - 149s - loss: 0.1261 - accuracy: 0.9511 - val_loss: 2.1181 - val_accuracy: 0.6272
Epoch 12/500
802/802 - 149s - loss: 0.1092 - accuracy: 0.9590 - val_loss: 2.1338 - val_accuracy: 0.6417
Epoch 13/500
802/802 - 149s - loss: 0.0985 - accuracy: 0.9627 - val_loss: 2.0935 - val_accuracy: 0.6433
Epoch 14/500
802/802 - 149s - loss: 0.0918 - accuracy: 0.9657 - val_loss: 2.0349 - val_accuracy: 0.6484
Epoch 15/500
802/802 - 149s - loss: 0.0808 - accuracy: 0.9708 - val_loss: 2.0982 - val_accuracy: 0.6487
Epoch 16/500
802/802 - 148s - loss: 0.0785 - accuracy: 0.9718 - val_loss: 2.0681 - val_accuracy: 0.6420
Epoch 17/500
802/802 - 148s - loss: 0.0711 - accuracy: 0.9743 - val_loss: 2.0662 - val_accuracy: 0.6471
Epoch 18/500
802/802 - 148s - loss: 0.0702 - accuracy: 0.9743 - val_loss: 2.0590 - val_accuracy: 0.6433
Epoch 19/500
802/802 - 149s - loss: 0.0661 - accuracy: 0.9757 - val_loss: 1.9792 - val_accuracy: 0.6512
Epoch 20/500
802/802 - 148s - loss: 0.0597 - accuracy: 0.9788 - val_loss: 2.0120 - val_accuracy: 0.6509
Epoch 21/500
802/802 - 149s - loss: 0.0536 - accuracy: 0.9812 - val_loss: 2.0008 - val_accuracy: 0.6553
Epoch 22/500
802/802 - 150s - loss: 0.0505 - accuracy: 0.9821 - val_loss: 2.0179 - val_accuracy: 0.6600
Epoch 23/500
802/802 - 150s - loss: 0.0533 - accuracy: 0.9805 - val_loss: 1.9560 - val_accuracy: 0.6607
Epoch 24/500
802/802 - 150s - loss: 0.0497 - accuracy: 0.9825 - val_loss: 1.9495 - val_accuracy: 0.6613
Epoch 25/500
802/802 - 149s - loss: 0.0441 - accuracy: 0.9845 - val_loss: 2.0431 - val_accuracy: 0.6537
Epoch 26/500
802/802 - 150s - loss: 0.0398 - accuracy: 0.9865 - val_loss: 1.9997 - val_accuracy: 0.6619
Epoch 27/500
802/802 - 148s - loss: 0.0401 - accuracy: 0.9869 - val_loss: 1.9732 - val_accuracy: 0.6547
Epoch 28/500
802/802 - 148s - loss: 0.0385 - accuracy: 0.9875 - val_loss: 1.9804 - val_accuracy: 0.6619
Epoch 29/500
802/802 - 148s - loss: 0.0404 - accuracy: 0.9865 - val_loss: 2.0001 - val_accuracy: 0.6588
Epoch 30/500
802/802 - 149s - loss: 0.0345 - accuracy: 0.9877 - val_loss: 2.0215 - val_accuracy: 0.6559
Epoch 31/500
802/802 - 148s - loss: 0.0349 - accuracy: 0.9886 - val_loss: 2.0038 - val_accuracy: 0.6566
Epoch 32/500
802/802 - 148s - loss: 0.0335 - accuracy: 0.9888 - val_loss: 2.0474 - val_accuracy: 0.6512
Epoch 33/500
802/802 - 149s - loss: 0.0291 - accuracy: 0.9900 - val_loss: 2.0496 - val_accuracy: 0.6641
Epoch 34/500
802/802 - 150s - loss: 0.0302 - accuracy: 0.9903 - val_loss: 2.0010 - val_accuracy: 0.6657
Epoch 35/500
802/802 - 151s - loss: 0.0264 - accuracy: 0.9915 - val_loss: 2.0557 - val_accuracy: 0.6695
Epoch 36/500
802/802 - 148s - loss: 0.0252 - accuracy: 0.9919 - val_loss: 2.0740 - val_accuracy: 0.6660
Epoch 37/500
802/802 - 149s - loss: 0.0271 - accuracy: 0.9909 - val_loss: 2.0251 - val_accuracy: 0.6695
Epoch 38/500
802/802 - 150s - loss: 0.0258 - accuracy: 0.9916 - val_loss: 1.9819 - val_accuracy: 0.6714
Epoch 39/500
802/802 - 150s - loss: 0.0242 - accuracy: 0.9933 - val_loss: 1.9431 - val_accuracy: 0.6736
Epoch 40/500
802/802 - 149s - loss: 0.0195 - accuracy: 0.9937 - val_loss: 1.9957 - val_accuracy: 0.6783
Epoch 41/500
802/802 - 149s - loss: 0.0227 - accuracy: 0.9925 - val_loss: 2.0199 - val_accuracy: 0.6717
Epoch 42/500
802/802 - 148s - loss: 0.0198 - accuracy: 0.9936 - val_loss: 2.0626 - val_accuracy: 0.6698
Epoch 43/500
802/802 - 148s - loss: 0.0216 - accuracy: 0.9926 - val_loss: 2.0632 - val_accuracy: 0.6679
Epoch 44/500
802/802 - 148s - loss: 0.0213 - accuracy: 0.9929 - val_loss: 2.1321 - val_accuracy: 0.6727
Epoch 45/500
802/802 - 147s - loss: 0.0209 - accuracy: 0.9930 - val_loss: 2.1169 - val_accuracy: 0.6701
Epoch 46/500
802/802 - 147s - loss: 0.0204 - accuracy: 0.9934 - val_loss: 2.0292 - val_accuracy: 0.6708
Epoch 47/500
802/802 - 148s - loss: 0.0168 - accuracy: 0.9945 - val_loss: 2.1011 - val_accuracy: 0.6679
Epoch 48/500
802/802 - 148s - loss: 0.0168 - accuracy: 0.9946 - val_loss: 2.1731 - val_accuracy: 0.6664
Epoch 49/500
802/802 - 149s - loss: 0.0160 - accuracy: 0.9953 - val_loss: 2.1111 - val_accuracy: 0.6695
Epoch 50/500
802/802 - 149s - loss: 0.0156 - accuracy: 0.9954 - val_loss: 2.0807 - val_accuracy: 0.6673
========================================
save_weights
h5_weights/SX.po/onehot_embedding_dense.h5
========================================

end time >>> Mon Oct  4 12:11:02 2021

end time >>> Mon Oct  4 12:11:02 2021

end time >>> Mon Oct  4 12:11:02 2021

end time >>> Mon Oct  4 12:11:02 2021

end time >>> Mon Oct  4 12:11:02 2021












args.model = onehot_embedding_dense
time used = 7512.3135459423065


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 12:11:03 2021

begin time >>> Mon Oct  4 12:11:03 2021

begin time >>> Mon Oct  4 12:11:03 2021

begin time >>> Mon Oct  4 12:11:03 2021

begin time >>> Mon Oct  4 12:11:03 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_one_branch
args.type = train
args.name = SX.po
args.length = 10001
===========================


-> h5_weights/SX.po folder already exist. pass.
-> result/SX.po/onehot_cnn_one_branch folder already exist. pass.
-> result/SX.po/onehot_cnn_two_branch folder already exist. pass.
-> result/SX.po/onehot_embedding_dense folder already exist. pass.
-> result/SX.po/onehot_dense folder already exist. pass.
-> result/SX.po/onehot_resnet18 folder already exist. pass.
-> result/SX.po/onehot_resnet34 folder already exist. pass.
-> result/SX.po/embedding_cnn_one_branch folder already exist. pass.
-> result/SX.po/embedding_cnn_two_branch folder already exist. pass.
-> result/SX.po/embedding_dense folder already exist. pass.
-> result/SX.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/SX.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
SX.po
########################################

########################################
model_name
onehot_embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
802/802 - 111s - loss: 0.8757 - accuracy: 0.4993 - val_loss: 0.6999 - val_accuracy: 0.5136
Epoch 2/500
802/802 - 110s - loss: 0.8571 - accuracy: 0.5044 - val_loss: 0.6971 - val_accuracy: 0.5227
Epoch 3/500
802/802 - 110s - loss: 0.8437 - accuracy: 0.5105 - val_loss: 0.6924 - val_accuracy: 0.5299
Epoch 4/500
802/802 - 110s - loss: 0.8361 - accuracy: 0.5138 - val_loss: 0.6885 - val_accuracy: 0.5445
Epoch 5/500
802/802 - 110s - loss: 0.8234 - accuracy: 0.5170 - val_loss: 0.6851 - val_accuracy: 0.5467
Epoch 6/500
802/802 - 110s - loss: 0.8150 - accuracy: 0.5258 - val_loss: 0.6819 - val_accuracy: 0.5558
Epoch 7/500
802/802 - 110s - loss: 0.8112 - accuracy: 0.5300 - val_loss: 0.6787 - val_accuracy: 0.5678
Epoch 8/500
802/802 - 110s - loss: 0.7964 - accuracy: 0.5385 - val_loss: 0.6758 - val_accuracy: 0.5738
Epoch 9/500
802/802 - 110s - loss: 0.7845 - accuracy: 0.5377 - val_loss: 0.6733 - val_accuracy: 0.5794
Epoch 10/500
802/802 - 110s - loss: 0.7883 - accuracy: 0.5381 - val_loss: 0.6710 - val_accuracy: 0.5810
Epoch 11/500
802/802 - 110s - loss: 0.7719 - accuracy: 0.5488 - val_loss: 0.6688 - val_accuracy: 0.5876
Epoch 12/500
802/802 - 110s - loss: 0.7761 - accuracy: 0.5500 - val_loss: 0.6658 - val_accuracy: 0.5946
Epoch 13/500
802/802 - 109s - loss: 0.7630 - accuracy: 0.5570 - val_loss: 0.6632 - val_accuracy: 0.5965
Epoch 14/500
802/802 - 110s - loss: 0.7548 - accuracy: 0.5577 - val_loss: 0.6607 - val_accuracy: 0.6009
Epoch 15/500
802/802 - 110s - loss: 0.7549 - accuracy: 0.5634 - val_loss: 0.6581 - val_accuracy: 0.6040
Epoch 16/500
802/802 - 110s - loss: 0.7404 - accuracy: 0.5741 - val_loss: 0.6556 - val_accuracy: 0.6100
Epoch 17/500
802/802 - 110s - loss: 0.7305 - accuracy: 0.5811 - val_loss: 0.6524 - val_accuracy: 0.6119
Epoch 18/500
802/802 - 110s - loss: 0.7282 - accuracy: 0.5821 - val_loss: 0.6496 - val_accuracy: 0.6148
Epoch 19/500
802/802 - 109s - loss: 0.7187 - accuracy: 0.5914 - val_loss: 0.6469 - val_accuracy: 0.6195
Epoch 20/500
802/802 - 110s - loss: 0.7101 - accuracy: 0.5987 - val_loss: 0.6442 - val_accuracy: 0.6179
Epoch 21/500
802/802 - 110s - loss: 0.7011 - accuracy: 0.6073 - val_loss: 0.6411 - val_accuracy: 0.6248
Epoch 22/500
802/802 - 109s - loss: 0.6993 - accuracy: 0.6101 - val_loss: 0.6386 - val_accuracy: 0.6242
Epoch 23/500
802/802 - 110s - loss: 0.6893 - accuracy: 0.6186 - val_loss: 0.6362 - val_accuracy: 0.6236
Epoch 24/500
802/802 - 110s - loss: 0.6717 - accuracy: 0.6314 - val_loss: 0.6336 - val_accuracy: 0.6315
Epoch 25/500
802/802 - 110s - loss: 0.6645 - accuracy: 0.6389 - val_loss: 0.6320 - val_accuracy: 0.6327
Epoch 26/500
802/802 - 110s - loss: 0.6519 - accuracy: 0.6461 - val_loss: 0.6295 - val_accuracy: 0.6375
Epoch 27/500
802/802 - 110s - loss: 0.6518 - accuracy: 0.6485 - val_loss: 0.6272 - val_accuracy: 0.6387
Epoch 28/500
802/802 - 109s - loss: 0.6435 - accuracy: 0.6571 - val_loss: 0.6255 - val_accuracy: 0.6409
Epoch 29/500
802/802 - 110s - loss: 0.6289 - accuracy: 0.6709 - val_loss: 0.6228 - val_accuracy: 0.6453
Epoch 30/500
802/802 - 110s - loss: 0.6181 - accuracy: 0.6705 - val_loss: 0.6218 - val_accuracy: 0.6494
Epoch 31/500
802/802 - 109s - loss: 0.6086 - accuracy: 0.6823 - val_loss: 0.6210 - val_accuracy: 0.6501
Epoch 32/500
802/802 - 110s - loss: 0.5998 - accuracy: 0.6893 - val_loss: 0.6197 - val_accuracy: 0.6532
Epoch 33/500
802/802 - 110s - loss: 0.5899 - accuracy: 0.6956 - val_loss: 0.6189 - val_accuracy: 0.6579
Epoch 34/500
802/802 - 109s - loss: 0.5827 - accuracy: 0.7029 - val_loss: 0.6184 - val_accuracy: 0.6579
Epoch 35/500
802/802 - 110s - loss: 0.5740 - accuracy: 0.7077 - val_loss: 0.6184 - val_accuracy: 0.6573
Epoch 36/500
802/802 - 110s - loss: 0.5671 - accuracy: 0.7118 - val_loss: 0.6191 - val_accuracy: 0.6567
Epoch 37/500
802/802 - 109s - loss: 0.5551 - accuracy: 0.7195 - val_loss: 0.6193 - val_accuracy: 0.6608
Epoch 38/500
802/802 - 109s - loss: 0.5484 - accuracy: 0.7268 - val_loss: 0.6195 - val_accuracy: 0.6617
Epoch 39/500
802/802 - 109s - loss: 0.5441 - accuracy: 0.7305 - val_loss: 0.6203 - val_accuracy: 0.6630
Epoch 40/500
802/802 - 109s - loss: 0.5290 - accuracy: 0.7399 - val_loss: 0.6223 - val_accuracy: 0.6611
Epoch 41/500
802/802 - 109s - loss: 0.5223 - accuracy: 0.7476 - val_loss: 0.6224 - val_accuracy: 0.6661
Epoch 42/500
802/802 - 109s - loss: 0.5130 - accuracy: 0.7512 - val_loss: 0.6239 - val_accuracy: 0.6690
Epoch 43/500
802/802 - 109s - loss: 0.5019 - accuracy: 0.7562 - val_loss: 0.6266 - val_accuracy: 0.6639
Epoch 44/500
802/802 - 109s - loss: 0.4985 - accuracy: 0.7632 - val_loss: 0.6276 - val_accuracy: 0.6677
Epoch 45/500
802/802 - 109s - loss: 0.4888 - accuracy: 0.7664 - val_loss: 0.6281 - val_accuracy: 0.6715
Epoch 46/500
802/802 - 109s - loss: 0.4888 - accuracy: 0.7690 - val_loss: 0.6307 - val_accuracy: 0.6731
Epoch 47/500
802/802 - 109s - loss: 0.4738 - accuracy: 0.7786 - val_loss: 0.6322 - val_accuracy: 0.6718
Epoch 48/500
802/802 - 109s - loss: 0.4608 - accuracy: 0.7853 - val_loss: 0.6340 - val_accuracy: 0.6706
Epoch 49/500
802/802 - 109s - loss: 0.4589 - accuracy: 0.7872 - val_loss: 0.6374 - val_accuracy: 0.6709
Epoch 50/500
802/802 - 109s - loss: 0.4447 - accuracy: 0.7910 - val_loss: 0.6388 - val_accuracy: 0.6731
Epoch 51/500
802/802 - 109s - loss: 0.4471 - accuracy: 0.7925 - val_loss: 0.6420 - val_accuracy: 0.6706
Epoch 52/500
802/802 - 109s - loss: 0.4335 - accuracy: 0.7993 - val_loss: 0.6443 - val_accuracy: 0.6743
Epoch 53/500
802/802 - 109s - loss: 0.4242 - accuracy: 0.8054 - val_loss: 0.6473 - val_accuracy: 0.6731
Epoch 54/500
802/802 - 110s - loss: 0.4180 - accuracy: 0.8117 - val_loss: 0.6502 - val_accuracy: 0.6762
Epoch 55/500
802/802 - 109s - loss: 0.4171 - accuracy: 0.8104 - val_loss: 0.6506 - val_accuracy: 0.6759
Epoch 56/500
802/802 - 109s - loss: 0.4074 - accuracy: 0.8174 - val_loss: 0.6552 - val_accuracy: 0.6740
Epoch 57/500
802/802 - 109s - loss: 0.3958 - accuracy: 0.8252 - val_loss: 0.6609 - val_accuracy: 0.6753
Epoch 58/500
802/802 - 109s - loss: 0.3857 - accuracy: 0.8267 - val_loss: 0.6633 - val_accuracy: 0.6765
Epoch 59/500
802/802 - 110s - loss: 0.3882 - accuracy: 0.8259 - val_loss: 0.6642 - val_accuracy: 0.6797
Epoch 60/500
802/802 - 109s - loss: 0.3799 - accuracy: 0.8301 - val_loss: 0.6703 - val_accuracy: 0.6772
Epoch 61/500
802/802 - 109s - loss: 0.3711 - accuracy: 0.8368 - val_loss: 0.6722 - val_accuracy: 0.6772
Epoch 62/500
802/802 - 109s - loss: 0.3633 - accuracy: 0.8386 - val_loss: 0.6787 - val_accuracy: 0.6791
Epoch 63/500
802/802 - 109s - loss: 0.3621 - accuracy: 0.8402 - val_loss: 0.6807 - val_accuracy: 0.6825
Epoch 64/500
802/802 - 109s - loss: 0.3554 - accuracy: 0.8448 - val_loss: 0.6876 - val_accuracy: 0.6806
Epoch 65/500
802/802 - 109s - loss: 0.3460 - accuracy: 0.8505 - val_loss: 0.6897 - val_accuracy: 0.6813
Epoch 66/500
802/802 - 110s - loss: 0.3398 - accuracy: 0.8529 - val_loss: 0.6935 - val_accuracy: 0.6797
Epoch 67/500
802/802 - 109s - loss: 0.3319 - accuracy: 0.8571 - val_loss: 0.6989 - val_accuracy: 0.6810
Epoch 68/500
802/802 - 109s - loss: 0.3299 - accuracy: 0.8576 - val_loss: 0.7029 - val_accuracy: 0.6819
Epoch 69/500
802/802 - 109s - loss: 0.3238 - accuracy: 0.8597 - val_loss: 0.7064 - val_accuracy: 0.6806
Epoch 70/500
802/802 - 109s - loss: 0.3196 - accuracy: 0.8605 - val_loss: 0.7099 - val_accuracy: 0.6835
Epoch 71/500
802/802 - 109s - loss: 0.3079 - accuracy: 0.8688 - val_loss: 0.7171 - val_accuracy: 0.6791
Epoch 72/500
802/802 - 110s - loss: 0.3114 - accuracy: 0.8658 - val_loss: 0.7202 - val_accuracy: 0.6810
Epoch 73/500
802/802 - 109s - loss: 0.2964 - accuracy: 0.8720 - val_loss: 0.7227 - val_accuracy: 0.6825
Epoch 74/500
802/802 - 109s - loss: 0.2967 - accuracy: 0.8722 - val_loss: 0.7296 - val_accuracy: 0.6803
Epoch 75/500
802/802 - 109s - loss: 0.2907 - accuracy: 0.8749 - val_loss: 0.7310 - val_accuracy: 0.6810
Epoch 76/500
802/802 - 109s - loss: 0.2866 - accuracy: 0.8791 - val_loss: 0.7346 - val_accuracy: 0.6803
Epoch 77/500
802/802 - 109s - loss: 0.2843 - accuracy: 0.8794 - val_loss: 0.7391 - val_accuracy: 0.6813
Epoch 78/500
802/802 - 109s - loss: 0.2727 - accuracy: 0.8849 - val_loss: 0.7458 - val_accuracy: 0.6819
Epoch 79/500
802/802 - 109s - loss: 0.2736 - accuracy: 0.8833 - val_loss: 0.7490 - val_accuracy: 0.6822
Epoch 80/500
802/802 - 109s - loss: 0.2665 - accuracy: 0.8868 - val_loss: 0.7563 - val_accuracy: 0.6816
Epoch 81/500
802/802 - 109s - loss: 0.2653 - accuracy: 0.8883 - val_loss: 0.7606 - val_accuracy: 0.6803
Epoch 82/500
802/802 - 109s - loss: 0.2547 - accuracy: 0.8930 - val_loss: 0.7660 - val_accuracy: 0.6806
Epoch 83/500
802/802 - 109s - loss: 0.2521 - accuracy: 0.8942 - val_loss: 0.7687 - val_accuracy: 0.6822
Epoch 84/500
802/802 - 109s - loss: 0.2462 - accuracy: 0.8957 - val_loss: 0.7761 - val_accuracy: 0.6825
Epoch 85/500
802/802 - 109s - loss: 0.2453 - accuracy: 0.8974 - val_loss: 0.7797 - val_accuracy: 0.6825
Epoch 86/500
802/802 - 109s - loss: 0.2346 - accuracy: 0.9011 - val_loss: 0.7870 - val_accuracy: 0.6844
Epoch 87/500
802/802 - 110s - loss: 0.2350 - accuracy: 0.9018 - val_loss: 0.7931 - val_accuracy: 0.6819
Epoch 88/500
802/802 - 109s - loss: 0.2297 - accuracy: 0.9043 - val_loss: 0.7958 - val_accuracy: 0.6854
Epoch 89/500
802/802 - 109s - loss: 0.2300 - accuracy: 0.9057 - val_loss: 0.8017 - val_accuracy: 0.6838
Epoch 90/500
802/802 - 109s - loss: 0.2258 - accuracy: 0.9055 - val_loss: 0.8057 - val_accuracy: 0.6847
Epoch 91/500
802/802 - 110s - loss: 0.2205 - accuracy: 0.9071 - val_loss: 0.8094 - val_accuracy: 0.6841
Epoch 92/500
802/802 - 111s - loss: 0.2217 - accuracy: 0.9089 - val_loss: 0.8161 - val_accuracy: 0.6838
Epoch 93/500
802/802 - 111s - loss: 0.2130 - accuracy: 0.9118 - val_loss: 0.8206 - val_accuracy: 0.6844
Epoch 94/500
802/802 - 111s - loss: 0.2103 - accuracy: 0.9135 - val_loss: 0.8262 - val_accuracy: 0.6854
Epoch 95/500
802/802 - 111s - loss: 0.2063 - accuracy: 0.9157 - val_loss: 0.8281 - val_accuracy: 0.6838
Epoch 96/500
802/802 - 111s - loss: 0.2032 - accuracy: 0.9168 - val_loss: 0.8388 - val_accuracy: 0.6847
Epoch 97/500
802/802 - 110s - loss: 0.2050 - accuracy: 0.9147 - val_loss: 0.8418 - val_accuracy: 0.6828
Epoch 98/500
802/802 - 110s - loss: 0.1917 - accuracy: 0.9231 - val_loss: 0.8467 - val_accuracy: 0.6819
Epoch 99/500
802/802 - 110s - loss: 0.1928 - accuracy: 0.9217 - val_loss: 0.8521 - val_accuracy: 0.6828
Epoch 100/500
802/802 - 110s - loss: 0.1927 - accuracy: 0.9221 - val_loss: 0.8566 - val_accuracy: 0.6819
Epoch 101/500
802/802 - 110s - loss: 0.1918 - accuracy: 0.9225 - val_loss: 0.8622 - val_accuracy: 0.6835
Epoch 102/500
802/802 - 110s - loss: 0.1826 - accuracy: 0.9262 - val_loss: 0.8658 - val_accuracy: 0.6838
Epoch 103/500
802/802 - 110s - loss: 0.1782 - accuracy: 0.9282 - val_loss: 0.8730 - val_accuracy: 0.6816
Epoch 104/500
802/802 - 110s - loss: 0.1775 - accuracy: 0.9273 - val_loss: 0.8806 - val_accuracy: 0.6813
Epoch 105/500
802/802 - 110s - loss: 0.1805 - accuracy: 0.9274 - val_loss: 0.8822 - val_accuracy: 0.6828
Epoch 106/500
802/802 - 110s - loss: 0.1742 - accuracy: 0.9312 - val_loss: 0.8836 - val_accuracy: 0.6819
Epoch 107/500
802/802 - 110s - loss: 0.1701 - accuracy: 0.9326 - val_loss: 0.8888 - val_accuracy: 0.6816
Epoch 108/500
802/802 - 110s - loss: 0.1720 - accuracy: 0.9310 - val_loss: 0.8953 - val_accuracy: 0.6828
========================================
save_weights
h5_weights/SX.po/onehot_embedding_cnn_one_branch.h5
========================================

end time >>> Mon Oct  4 15:29:13 2021

end time >>> Mon Oct  4 15:29:13 2021

end time >>> Mon Oct  4 15:29:13 2021

end time >>> Mon Oct  4 15:29:13 2021

end time >>> Mon Oct  4 15:29:13 2021












args.model = onehot_embedding_cnn_one_branch
time used = 11889.493489742279


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 15:29:15 2021

begin time >>> Mon Oct  4 15:29:15 2021

begin time >>> Mon Oct  4 15:29:15 2021

begin time >>> Mon Oct  4 15:29:15 2021

begin time >>> Mon Oct  4 15:29:15 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_two_branch
args.type = train
args.name = SX.po
args.length = 10001
===========================


-> h5_weights/SX.po folder already exist. pass.
-> result/SX.po/onehot_cnn_one_branch folder already exist. pass.
-> result/SX.po/onehot_cnn_two_branch folder already exist. pass.
-> result/SX.po/onehot_embedding_dense folder already exist. pass.
-> result/SX.po/onehot_dense folder already exist. pass.
-> result/SX.po/onehot_resnet18 folder already exist. pass.
-> result/SX.po/onehot_resnet34 folder already exist. pass.
-> result/SX.po/embedding_cnn_one_branch folder already exist. pass.
-> result/SX.po/embedding_cnn_two_branch folder already exist. pass.
-> result/SX.po/embedding_dense folder already exist. pass.
-> result/SX.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/SX.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
SX.po
########################################

########################################
model_name
onehot_embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
802/802 - 111s - loss: 0.8738 - accuracy: 0.4962 - val_loss: 0.6988 - val_accuracy: 0.5211
Epoch 2/500
802/802 - 109s - loss: 0.8503 - accuracy: 0.5092 - val_loss: 0.6950 - val_accuracy: 0.5325
Epoch 3/500
802/802 - 110s - loss: 0.8333 - accuracy: 0.5174 - val_loss: 0.6906 - val_accuracy: 0.5404
Epoch 4/500
802/802 - 110s - loss: 0.8268 - accuracy: 0.5168 - val_loss: 0.6870 - val_accuracy: 0.5432
Epoch 5/500
802/802 - 110s - loss: 0.8224 - accuracy: 0.5205 - val_loss: 0.6829 - val_accuracy: 0.5520
Epoch 6/500
802/802 - 110s - loss: 0.8014 - accuracy: 0.5291 - val_loss: 0.6801 - val_accuracy: 0.5571
Epoch 7/500
802/802 - 110s - loss: 0.8067 - accuracy: 0.5282 - val_loss: 0.6768 - val_accuracy: 0.5615
Epoch 8/500
802/802 - 110s - loss: 0.7873 - accuracy: 0.5437 - val_loss: 0.6738 - val_accuracy: 0.5690
Epoch 9/500
802/802 - 110s - loss: 0.7827 - accuracy: 0.5396 - val_loss: 0.6707 - val_accuracy: 0.5753
Epoch 10/500
802/802 - 110s - loss: 0.7720 - accuracy: 0.5456 - val_loss: 0.6673 - val_accuracy: 0.5817
Epoch 11/500
802/802 - 110s - loss: 0.7674 - accuracy: 0.5565 - val_loss: 0.6641 - val_accuracy: 0.5845
Epoch 12/500
802/802 - 110s - loss: 0.7576 - accuracy: 0.5607 - val_loss: 0.6626 - val_accuracy: 0.5876
Epoch 13/500
802/802 - 110s - loss: 0.7548 - accuracy: 0.5643 - val_loss: 0.6586 - val_accuracy: 0.5943
Epoch 14/500
802/802 - 110s - loss: 0.7483 - accuracy: 0.5656 - val_loss: 0.6563 - val_accuracy: 0.5965
Epoch 15/500
802/802 - 110s - loss: 0.7426 - accuracy: 0.5744 - val_loss: 0.6537 - val_accuracy: 0.6006
Epoch 16/500
802/802 - 110s - loss: 0.7292 - accuracy: 0.5839 - val_loss: 0.6504 - val_accuracy: 0.6006
Epoch 17/500
802/802 - 110s - loss: 0.7229 - accuracy: 0.5890 - val_loss: 0.6473 - val_accuracy: 0.6047
Epoch 18/500
802/802 - 110s - loss: 0.7081 - accuracy: 0.5987 - val_loss: 0.6441 - val_accuracy: 0.6107
Epoch 19/500
802/802 - 110s - loss: 0.7054 - accuracy: 0.6031 - val_loss: 0.6415 - val_accuracy: 0.6122
Epoch 20/500
802/802 - 110s - loss: 0.6907 - accuracy: 0.6110 - val_loss: 0.6389 - val_accuracy: 0.6204
Epoch 21/500
802/802 - 110s - loss: 0.6847 - accuracy: 0.6208 - val_loss: 0.6365 - val_accuracy: 0.6236
Epoch 22/500
802/802 - 110s - loss: 0.6743 - accuracy: 0.6275 - val_loss: 0.6327 - val_accuracy: 0.6299
Epoch 23/500
802/802 - 110s - loss: 0.6663 - accuracy: 0.6314 - val_loss: 0.6291 - val_accuracy: 0.6362
Epoch 24/500
802/802 - 110s - loss: 0.6585 - accuracy: 0.6384 - val_loss: 0.6273 - val_accuracy: 0.6397
Epoch 25/500
802/802 - 110s - loss: 0.6428 - accuracy: 0.6525 - val_loss: 0.6261 - val_accuracy: 0.6365
Epoch 26/500
802/802 - 110s - loss: 0.6394 - accuracy: 0.6596 - val_loss: 0.6231 - val_accuracy: 0.6438
Epoch 27/500
802/802 - 110s - loss: 0.6238 - accuracy: 0.6688 - val_loss: 0.6210 - val_accuracy: 0.6494
Epoch 28/500
802/802 - 110s - loss: 0.6168 - accuracy: 0.6716 - val_loss: 0.6190 - val_accuracy: 0.6494
Epoch 29/500
802/802 - 110s - loss: 0.6013 - accuracy: 0.6854 - val_loss: 0.6174 - val_accuracy: 0.6504
Epoch 30/500
802/802 - 110s - loss: 0.5918 - accuracy: 0.6925 - val_loss: 0.6156 - val_accuracy: 0.6570
Epoch 31/500
802/802 - 110s - loss: 0.5893 - accuracy: 0.6970 - val_loss: 0.6156 - val_accuracy: 0.6570
Epoch 32/500
802/802 - 110s - loss: 0.5803 - accuracy: 0.7063 - val_loss: 0.6135 - val_accuracy: 0.6630
Epoch 33/500
802/802 - 110s - loss: 0.5637 - accuracy: 0.7097 - val_loss: 0.6130 - val_accuracy: 0.6652
Epoch 34/500
802/802 - 110s - loss: 0.5514 - accuracy: 0.7229 - val_loss: 0.6110 - val_accuracy: 0.6718
Epoch 35/500
802/802 - 110s - loss: 0.5438 - accuracy: 0.7294 - val_loss: 0.6130 - val_accuracy: 0.6699
Epoch 36/500
802/802 - 110s - loss: 0.5438 - accuracy: 0.7311 - val_loss: 0.6122 - val_accuracy: 0.6728
Epoch 37/500
802/802 - 110s - loss: 0.5186 - accuracy: 0.7449 - val_loss: 0.6132 - val_accuracy: 0.6715
Epoch 38/500
802/802 - 110s - loss: 0.5145 - accuracy: 0.7482 - val_loss: 0.6143 - val_accuracy: 0.6756
Epoch 39/500
802/802 - 110s - loss: 0.5097 - accuracy: 0.7512 - val_loss: 0.6161 - val_accuracy: 0.6756
Epoch 40/500
802/802 - 110s - loss: 0.4979 - accuracy: 0.7618 - val_loss: 0.6177 - val_accuracy: 0.6740
Epoch 41/500
802/802 - 110s - loss: 0.4908 - accuracy: 0.7662 - val_loss: 0.6175 - val_accuracy: 0.6772
Epoch 42/500
802/802 - 110s - loss: 0.4786 - accuracy: 0.7731 - val_loss: 0.6214 - val_accuracy: 0.6750
Epoch 43/500
802/802 - 109s - loss: 0.4705 - accuracy: 0.7778 - val_loss: 0.6198 - val_accuracy: 0.6813
Epoch 44/500
802/802 - 110s - loss: 0.4677 - accuracy: 0.7807 - val_loss: 0.6219 - val_accuracy: 0.6803
Epoch 45/500
802/802 - 110s - loss: 0.4534 - accuracy: 0.7909 - val_loss: 0.6248 - val_accuracy: 0.6819
Epoch 46/500
802/802 - 109s - loss: 0.4333 - accuracy: 0.7969 - val_loss: 0.6264 - val_accuracy: 0.6832
Epoch 47/500
802/802 - 110s - loss: 0.4396 - accuracy: 0.7970 - val_loss: 0.6304 - val_accuracy: 0.6816
Epoch 48/500
802/802 - 110s - loss: 0.4266 - accuracy: 0.8050 - val_loss: 0.6303 - val_accuracy: 0.6838
Epoch 49/500
802/802 - 110s - loss: 0.4171 - accuracy: 0.8087 - val_loss: 0.6355 - val_accuracy: 0.6832
Epoch 50/500
802/802 - 110s - loss: 0.4105 - accuracy: 0.8115 - val_loss: 0.6412 - val_accuracy: 0.6819
Epoch 51/500
802/802 - 110s - loss: 0.4098 - accuracy: 0.8167 - val_loss: 0.6405 - val_accuracy: 0.6825
Epoch 52/500
802/802 - 110s - loss: 0.3948 - accuracy: 0.8216 - val_loss: 0.6436 - val_accuracy: 0.6841
Epoch 53/500
802/802 - 110s - loss: 0.3857 - accuracy: 0.8294 - val_loss: 0.6516 - val_accuracy: 0.6813
Epoch 54/500
802/802 - 110s - loss: 0.3795 - accuracy: 0.8298 - val_loss: 0.6480 - val_accuracy: 0.6838
Epoch 55/500
802/802 - 110s - loss: 0.3722 - accuracy: 0.8331 - val_loss: 0.6523 - val_accuracy: 0.6835
Epoch 56/500
802/802 - 110s - loss: 0.3671 - accuracy: 0.8383 - val_loss: 0.6596 - val_accuracy: 0.6847
Epoch 57/500
802/802 - 110s - loss: 0.3616 - accuracy: 0.8384 - val_loss: 0.6618 - val_accuracy: 0.6844
Epoch 58/500
802/802 - 110s - loss: 0.3514 - accuracy: 0.8464 - val_loss: 0.6659 - val_accuracy: 0.6844
Epoch 59/500
802/802 - 110s - loss: 0.3485 - accuracy: 0.8481 - val_loss: 0.6686 - val_accuracy: 0.6863
Epoch 60/500
802/802 - 110s - loss: 0.3364 - accuracy: 0.8525 - val_loss: 0.6737 - val_accuracy: 0.6844
Epoch 61/500
802/802 - 110s - loss: 0.3300 - accuracy: 0.8573 - val_loss: 0.6785 - val_accuracy: 0.6857
Epoch 62/500
802/802 - 110s - loss: 0.3278 - accuracy: 0.8572 - val_loss: 0.6811 - val_accuracy: 0.6873
Epoch 63/500
802/802 - 110s - loss: 0.3153 - accuracy: 0.8632 - val_loss: 0.6870 - val_accuracy: 0.6847
Epoch 64/500
802/802 - 110s - loss: 0.3164 - accuracy: 0.8625 - val_loss: 0.6893 - val_accuracy: 0.6844
Epoch 65/500
802/802 - 110s - loss: 0.3053 - accuracy: 0.8662 - val_loss: 0.6974 - val_accuracy: 0.6844
Epoch 66/500
802/802 - 110s - loss: 0.2980 - accuracy: 0.8708 - val_loss: 0.7048 - val_accuracy: 0.6832
Epoch 67/500
802/802 - 110s - loss: 0.2923 - accuracy: 0.8736 - val_loss: 0.7016 - val_accuracy: 0.6863
Epoch 68/500
802/802 - 110s - loss: 0.2941 - accuracy: 0.8758 - val_loss: 0.7134 - val_accuracy: 0.6810
Epoch 69/500
802/802 - 110s - loss: 0.2791 - accuracy: 0.8795 - val_loss: 0.7186 - val_accuracy: 0.6835
Epoch 70/500
802/802 - 110s - loss: 0.2735 - accuracy: 0.8840 - val_loss: 0.7177 - val_accuracy: 0.6873
Epoch 71/500
802/802 - 110s - loss: 0.2791 - accuracy: 0.8811 - val_loss: 0.7255 - val_accuracy: 0.6841
Epoch 72/500
802/802 - 110s - loss: 0.2639 - accuracy: 0.8893 - val_loss: 0.7315 - val_accuracy: 0.6832
Epoch 73/500
802/802 - 110s - loss: 0.2654 - accuracy: 0.8893 - val_loss: 0.7382 - val_accuracy: 0.6813
Epoch 74/500
802/802 - 110s - loss: 0.2601 - accuracy: 0.8901 - val_loss: 0.7431 - val_accuracy: 0.6838
Epoch 75/500
802/802 - 110s - loss: 0.2485 - accuracy: 0.8938 - val_loss: 0.7520 - val_accuracy: 0.6841
Epoch 76/500
802/802 - 110s - loss: 0.2478 - accuracy: 0.8961 - val_loss: 0.7538 - val_accuracy: 0.6835
Epoch 77/500
802/802 - 110s - loss: 0.2401 - accuracy: 0.8998 - val_loss: 0.7571 - val_accuracy: 0.6832
Epoch 78/500
802/802 - 110s - loss: 0.2344 - accuracy: 0.9022 - val_loss: 0.7651 - val_accuracy: 0.6832
Epoch 79/500
802/802 - 110s - loss: 0.2320 - accuracy: 0.9026 - val_loss: 0.7720 - val_accuracy: 0.6828
Epoch 80/500
802/802 - 110s - loss: 0.2224 - accuracy: 0.9082 - val_loss: 0.7772 - val_accuracy: 0.6841
Epoch 81/500
802/802 - 110s - loss: 0.2214 - accuracy: 0.9077 - val_loss: 0.7828 - val_accuracy: 0.6841
Epoch 82/500
802/802 - 110s - loss: 0.2121 - accuracy: 0.9129 - val_loss: 0.7876 - val_accuracy: 0.6835
========================================
save_weights
h5_weights/SX.po/onehot_embedding_cnn_two_branch.h5
========================================

end time >>> Mon Oct  4 18:00:27 2021

end time >>> Mon Oct  4 18:00:27 2021

end time >>> Mon Oct  4 18:00:27 2021

end time >>> Mon Oct  4 18:00:27 2021

end time >>> Mon Oct  4 18:00:27 2021












args.model = onehot_embedding_cnn_two_branch
time used = 9071.872344255447


