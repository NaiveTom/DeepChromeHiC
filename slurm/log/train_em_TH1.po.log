************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 03:23:37 2021

begin time >>> Mon Oct  4 03:23:37 2021

begin time >>> Mon Oct  4 03:23:37 2021

begin time >>> Mon Oct  4 03:23:37 2021

begin time >>> Mon Oct  4 03:23:37 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_dense
args.type = train
args.name = TH1.po
args.length = 10001
===========================


-> h5_weights/TH1.po folder already exist. pass.
-> result/TH1.po/onehot_cnn_one_branch folder already exist. pass.
-> result/TH1.po/onehot_cnn_two_branch folder already exist. pass.
-> result/TH1.po/onehot_embedding_dense folder already exist. pass.
-> result/TH1.po/onehot_dense folder already exist. pass.
-> result/TH1.po/onehot_resnet18 folder already exist. pass.
-> result/TH1.po/onehot_resnet34 folder already exist. pass.
-> result/TH1.po/embedding_cnn_one_branch folder already exist. pass.
-> result/TH1.po/embedding_cnn_two_branch folder already exist. pass.
-> result/TH1.po/embedding_dense folder already exist. pass.
-> result/TH1.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/TH1.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
TH1.po
########################################

########################################
model_name
embedding_dense
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 626, 64)      409664      concatenate[0][0]                
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, 38, 64)       0           conv1d[0][0]                     
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 38, 64)       256         max_pooling1d[0][0]              
__________________________________________________________________________________________________
flatten (Flatten)               (None, 2432)         0           batch_normalization[0][0]        
__________________________________________________________________________________________________
dropout (Dropout)               (None, 2432)         0           flatten[0][0]                    
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          1245696     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation (Activation)         (None, 512)          0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation[0][0]                 
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 512)          0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_1[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 512)          2048        dense_2[0][0]                    
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 512)          0           activation_2[0][0]               
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            513         dropout_3[0][0]                  
==================================================================================================
Total params: 3,006,985
Trainable params: 3,003,785
Non-trainable params: 3,200
__________________________________________________________________________________________________
Epoch 1/500
289/289 - 38s - loss: 0.8583 - accuracy: 0.4921 - val_loss: 0.6916 - val_accuracy: 0.5302
Epoch 2/500
289/289 - 38s - loss: 0.8493 - accuracy: 0.5043 - val_loss: 0.6939 - val_accuracy: 0.5267
Epoch 3/500
289/289 - 38s - loss: 0.8583 - accuracy: 0.5040 - val_loss: 0.6933 - val_accuracy: 0.5285
Epoch 4/500
289/289 - 38s - loss: 0.8478 - accuracy: 0.5104 - val_loss: 0.6926 - val_accuracy: 0.5294
Epoch 5/500
289/289 - 38s - loss: 0.8520 - accuracy: 0.5014 - val_loss: 0.6923 - val_accuracy: 0.5276
Epoch 6/500
289/289 - 38s - loss: 0.8452 - accuracy: 0.5038 - val_loss: 0.6915 - val_accuracy: 0.5232
Epoch 7/500
289/289 - 38s - loss: 0.8458 - accuracy: 0.5028 - val_loss: 0.6908 - val_accuracy: 0.5311
Epoch 8/500
289/289 - 38s - loss: 0.8393 - accuracy: 0.5009 - val_loss: 0.6903 - val_accuracy: 0.5355
Epoch 9/500
289/289 - 38s - loss: 0.8289 - accuracy: 0.5100 - val_loss: 0.6892 - val_accuracy: 0.5416
Epoch 10/500
289/289 - 38s - loss: 0.8403 - accuracy: 0.5053 - val_loss: 0.6889 - val_accuracy: 0.5381
Epoch 11/500
289/289 - 38s - loss: 0.8370 - accuracy: 0.5034 - val_loss: 0.6883 - val_accuracy: 0.5390
Epoch 12/500
289/289 - 38s - loss: 0.8299 - accuracy: 0.5041 - val_loss: 0.6880 - val_accuracy: 0.5469
Epoch 13/500
289/289 - 38s - loss: 0.8367 - accuracy: 0.5020 - val_loss: 0.6884 - val_accuracy: 0.5372
Epoch 14/500
289/289 - 38s - loss: 0.8269 - accuracy: 0.5021 - val_loss: 0.6887 - val_accuracy: 0.5320
Epoch 15/500
289/289 - 38s - loss: 0.8323 - accuracy: 0.5046 - val_loss: 0.6874 - val_accuracy: 0.5408
Epoch 16/500
289/289 - 38s - loss: 0.8455 - accuracy: 0.4933 - val_loss: 0.6873 - val_accuracy: 0.5416
Epoch 17/500
289/289 - 38s - loss: 0.8235 - accuracy: 0.5122 - val_loss: 0.6868 - val_accuracy: 0.5443
Epoch 18/500
289/289 - 38s - loss: 0.8326 - accuracy: 0.5021 - val_loss: 0.6874 - val_accuracy: 0.5372
Epoch 19/500
289/289 - 38s - loss: 0.8102 - accuracy: 0.5138 - val_loss: 0.6866 - val_accuracy: 0.5329
Epoch 20/500
289/289 - 38s - loss: 0.8118 - accuracy: 0.5182 - val_loss: 0.6868 - val_accuracy: 0.5408
Epoch 21/500
289/289 - 38s - loss: 0.8181 - accuracy: 0.5087 - val_loss: 0.6863 - val_accuracy: 0.5416
Epoch 22/500
289/289 - 38s - loss: 0.8151 - accuracy: 0.5091 - val_loss: 0.6859 - val_accuracy: 0.5425
Epoch 23/500
289/289 - 38s - loss: 0.8239 - accuracy: 0.4979 - val_loss: 0.6858 - val_accuracy: 0.5478
Epoch 24/500
289/289 - 38s - loss: 0.8187 - accuracy: 0.5061 - val_loss: 0.6859 - val_accuracy: 0.5425
Epoch 25/500
289/289 - 38s - loss: 0.8052 - accuracy: 0.5121 - val_loss: 0.6854 - val_accuracy: 0.5443
Epoch 26/500
289/289 - 38s - loss: 0.8012 - accuracy: 0.5168 - val_loss: 0.6858 - val_accuracy: 0.5460
Epoch 27/500
289/289 - 38s - loss: 0.8072 - accuracy: 0.5066 - val_loss: 0.6854 - val_accuracy: 0.5504
Epoch 28/500
289/289 - 38s - loss: 0.8126 - accuracy: 0.5094 - val_loss: 0.6847 - val_accuracy: 0.5513
Epoch 29/500
289/289 - 38s - loss: 0.8088 - accuracy: 0.5088 - val_loss: 0.6850 - val_accuracy: 0.5469
Epoch 30/500
289/289 - 38s - loss: 0.7979 - accuracy: 0.5201 - val_loss: 0.6852 - val_accuracy: 0.5521
Epoch 31/500
289/289 - 38s - loss: 0.8191 - accuracy: 0.5017 - val_loss: 0.6850 - val_accuracy: 0.5425
Epoch 32/500
289/289 - 38s - loss: 0.8043 - accuracy: 0.5065 - val_loss: 0.6847 - val_accuracy: 0.5521
Epoch 33/500
289/289 - 38s - loss: 0.7870 - accuracy: 0.5248 - val_loss: 0.6846 - val_accuracy: 0.5530
Epoch 34/500
289/289 - 38s - loss: 0.8021 - accuracy: 0.5178 - val_loss: 0.6843 - val_accuracy: 0.5513
Epoch 35/500
289/289 - 38s - loss: 0.8027 - accuracy: 0.5052 - val_loss: 0.6840 - val_accuracy: 0.5513
Epoch 36/500
289/289 - 38s - loss: 0.8043 - accuracy: 0.5110 - val_loss: 0.6837 - val_accuracy: 0.5548
Epoch 37/500
289/289 - 38s - loss: 0.8044 - accuracy: 0.5108 - val_loss: 0.6841 - val_accuracy: 0.5557
Epoch 38/500
289/289 - 38s - loss: 0.7888 - accuracy: 0.5210 - val_loss: 0.6837 - val_accuracy: 0.5548
Epoch 39/500
289/289 - 38s - loss: 0.8005 - accuracy: 0.5131 - val_loss: 0.6835 - val_accuracy: 0.5557
Epoch 40/500
289/289 - 38s - loss: 0.7926 - accuracy: 0.5220 - val_loss: 0.6832 - val_accuracy: 0.5539
Epoch 41/500
289/289 - 38s - loss: 0.7988 - accuracy: 0.5169 - val_loss: 0.6831 - val_accuracy: 0.5504
Epoch 42/500
289/289 - 38s - loss: 0.7904 - accuracy: 0.5221 - val_loss: 0.6830 - val_accuracy: 0.5557
Epoch 43/500
289/289 - 38s - loss: 0.7927 - accuracy: 0.5191 - val_loss: 0.6826 - val_accuracy: 0.5557
Epoch 44/500
289/289 - 38s - loss: 0.7920 - accuracy: 0.5253 - val_loss: 0.6831 - val_accuracy: 0.5539
Epoch 45/500
289/289 - 38s - loss: 0.7901 - accuracy: 0.5217 - val_loss: 0.6824 - val_accuracy: 0.5557
Epoch 46/500
289/289 - 38s - loss: 0.7821 - accuracy: 0.5313 - val_loss: 0.6826 - val_accuracy: 0.5548
Epoch 47/500
289/289 - 38s - loss: 0.7855 - accuracy: 0.5273 - val_loss: 0.6820 - val_accuracy: 0.5644
Epoch 48/500
289/289 - 38s - loss: 0.7873 - accuracy: 0.5222 - val_loss: 0.6822 - val_accuracy: 0.5574
Epoch 49/500
289/289 - 38s - loss: 0.7943 - accuracy: 0.5209 - val_loss: 0.6822 - val_accuracy: 0.5644
Epoch 50/500
289/289 - 38s - loss: 0.7812 - accuracy: 0.5306 - val_loss: 0.6817 - val_accuracy: 0.5627
Epoch 51/500
289/289 - 38s - loss: 0.7835 - accuracy: 0.5221 - val_loss: 0.6814 - val_accuracy: 0.5618
Epoch 52/500
289/289 - 38s - loss: 0.7957 - accuracy: 0.5126 - val_loss: 0.6813 - val_accuracy: 0.5627
Epoch 53/500
289/289 - 38s - loss: 0.7833 - accuracy: 0.5229 - val_loss: 0.6812 - val_accuracy: 0.5513
Epoch 54/500
289/289 - 38s - loss: 0.7844 - accuracy: 0.5231 - val_loss: 0.6807 - val_accuracy: 0.5600
Epoch 55/500
289/289 - 38s - loss: 0.7829 - accuracy: 0.5301 - val_loss: 0.6805 - val_accuracy: 0.5565
Epoch 56/500
289/289 - 38s - loss: 0.7741 - accuracy: 0.5301 - val_loss: 0.6807 - val_accuracy: 0.5574
Epoch 57/500
289/289 - 38s - loss: 0.7739 - accuracy: 0.5293 - val_loss: 0.6800 - val_accuracy: 0.5670
Epoch 58/500
289/289 - 38s - loss: 0.7818 - accuracy: 0.5234 - val_loss: 0.6801 - val_accuracy: 0.5609
Epoch 59/500
289/289 - 38s - loss: 0.7906 - accuracy: 0.5170 - val_loss: 0.6799 - val_accuracy: 0.5627
Epoch 60/500
289/289 - 38s - loss: 0.7757 - accuracy: 0.5281 - val_loss: 0.6800 - val_accuracy: 0.5662
Epoch 61/500
289/289 - 38s - loss: 0.7694 - accuracy: 0.5350 - val_loss: 0.6795 - val_accuracy: 0.5635
Epoch 62/500
289/289 - 38s - loss: 0.7690 - accuracy: 0.5316 - val_loss: 0.6790 - val_accuracy: 0.5618
Epoch 63/500
289/289 - 38s - loss: 0.7736 - accuracy: 0.5339 - val_loss: 0.6791 - val_accuracy: 0.5609
Epoch 64/500
289/289 - 38s - loss: 0.7713 - accuracy: 0.5333 - val_loss: 0.6789 - val_accuracy: 0.5644
Epoch 65/500
289/289 - 38s - loss: 0.7784 - accuracy: 0.5255 - val_loss: 0.6786 - val_accuracy: 0.5583
Epoch 66/500
289/289 - 38s - loss: 0.7712 - accuracy: 0.5312 - val_loss: 0.6787 - val_accuracy: 0.5600
Epoch 67/500
289/289 - 38s - loss: 0.7807 - accuracy: 0.5243 - val_loss: 0.6783 - val_accuracy: 0.5627
Epoch 68/500
289/289 - 38s - loss: 0.7669 - accuracy: 0.5361 - val_loss: 0.6779 - val_accuracy: 0.5662
Epoch 69/500
289/289 - 38s - loss: 0.7721 - accuracy: 0.5272 - val_loss: 0.6781 - val_accuracy: 0.5653
Epoch 70/500
289/289 - 38s - loss: 0.7678 - accuracy: 0.5362 - val_loss: 0.6777 - val_accuracy: 0.5627
Epoch 71/500
289/289 - 38s - loss: 0.7709 - accuracy: 0.5328 - val_loss: 0.6774 - val_accuracy: 0.5583
Epoch 72/500
289/289 - 38s - loss: 0.7632 - accuracy: 0.5426 - val_loss: 0.6773 - val_accuracy: 0.5627
Epoch 73/500
289/289 - 38s - loss: 0.7701 - accuracy: 0.5275 - val_loss: 0.6769 - val_accuracy: 0.5600
Epoch 74/500
289/289 - 38s - loss: 0.7681 - accuracy: 0.5323 - val_loss: 0.6768 - val_accuracy: 0.5592
Epoch 75/500
289/289 - 38s - loss: 0.7601 - accuracy: 0.5419 - val_loss: 0.6768 - val_accuracy: 0.5592
Epoch 76/500
289/289 - 38s - loss: 0.7617 - accuracy: 0.5345 - val_loss: 0.6760 - val_accuracy: 0.5627
Epoch 77/500
289/289 - 38s - loss: 0.7633 - accuracy: 0.5440 - val_loss: 0.6757 - val_accuracy: 0.5644
========================================
save_weights
h5_weights/TH1.po/embedding_dense.h5
========================================

end time >>> Mon Oct  4 04:12:35 2021

end time >>> Mon Oct  4 04:12:35 2021

end time >>> Mon Oct  4 04:12:35 2021

end time >>> Mon Oct  4 04:12:35 2021

end time >>> Mon Oct  4 04:12:35 2021












args.model = embedding_dense
time used = 2937.600405693054


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 04:12:36 2021

begin time >>> Mon Oct  4 04:12:36 2021

begin time >>> Mon Oct  4 04:12:36 2021

begin time >>> Mon Oct  4 04:12:36 2021

begin time >>> Mon Oct  4 04:12:36 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_cnn_one_branch
args.type = train
args.name = TH1.po
args.length = 10001
===========================


-> h5_weights/TH1.po folder already exist. pass.
-> result/TH1.po/onehot_cnn_one_branch folder already exist. pass.
-> result/TH1.po/onehot_cnn_two_branch folder already exist. pass.
-> result/TH1.po/onehot_embedding_dense folder already exist. pass.
-> result/TH1.po/onehot_dense folder already exist. pass.
-> result/TH1.po/onehot_resnet18 folder already exist. pass.
-> result/TH1.po/onehot_resnet34 folder already exist. pass.
-> result/TH1.po/embedding_cnn_one_branch folder already exist. pass.
-> result/TH1.po/embedding_cnn_two_branch folder already exist. pass.
-> result/TH1.po/embedding_dense folder already exist. pass.
-> result/TH1.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/TH1.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
TH1.po
########################################

########################################
model_name
embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
289/289 - 40s - loss: 0.9377 - accuracy: 0.5062 - val_loss: 0.7146 - val_accuracy: 0.4917
Epoch 2/500
289/289 - 40s - loss: 0.9222 - accuracy: 0.4970 - val_loss: 0.7228 - val_accuracy: 0.4943
Epoch 3/500
289/289 - 40s - loss: 0.8917 - accuracy: 0.5056 - val_loss: 0.7183 - val_accuracy: 0.4934
Epoch 4/500
289/289 - 40s - loss: 0.8726 - accuracy: 0.5089 - val_loss: 0.7124 - val_accuracy: 0.4908
Epoch 5/500
289/289 - 40s - loss: 0.8472 - accuracy: 0.5130 - val_loss: 0.7090 - val_accuracy: 0.4873
Epoch 6/500
289/289 - 40s - loss: 0.8352 - accuracy: 0.5198 - val_loss: 0.7049 - val_accuracy: 0.5039
Epoch 7/500
289/289 - 40s - loss: 0.8269 - accuracy: 0.5254 - val_loss: 0.7024 - val_accuracy: 0.4987
Epoch 8/500
289/289 - 40s - loss: 0.8278 - accuracy: 0.5158 - val_loss: 0.7002 - val_accuracy: 0.5118
Epoch 9/500
289/289 - 40s - loss: 0.8213 - accuracy: 0.5190 - val_loss: 0.6984 - val_accuracy: 0.5171
Epoch 10/500
289/289 - 40s - loss: 0.8035 - accuracy: 0.5330 - val_loss: 0.6968 - val_accuracy: 0.5276
Epoch 11/500
289/289 - 40s - loss: 0.7981 - accuracy: 0.5387 - val_loss: 0.6960 - val_accuracy: 0.5302
Epoch 12/500
289/289 - 40s - loss: 0.8004 - accuracy: 0.5356 - val_loss: 0.6945 - val_accuracy: 0.5320
Epoch 13/500
289/289 - 40s - loss: 0.7928 - accuracy: 0.5393 - val_loss: 0.6932 - val_accuracy: 0.5337
Epoch 14/500
289/289 - 40s - loss: 0.7982 - accuracy: 0.5292 - val_loss: 0.6924 - val_accuracy: 0.5355
Epoch 15/500
289/289 - 40s - loss: 0.7797 - accuracy: 0.5417 - val_loss: 0.6918 - val_accuracy: 0.5381
Epoch 16/500
289/289 - 40s - loss: 0.7883 - accuracy: 0.5384 - val_loss: 0.6907 - val_accuracy: 0.5355
Epoch 17/500
289/289 - 40s - loss: 0.7745 - accuracy: 0.5514 - val_loss: 0.6893 - val_accuracy: 0.5408
Epoch 18/500
289/289 - 40s - loss: 0.7716 - accuracy: 0.5581 - val_loss: 0.6888 - val_accuracy: 0.5434
Epoch 19/500
289/289 - 40s - loss: 0.7656 - accuracy: 0.5594 - val_loss: 0.6881 - val_accuracy: 0.5434
Epoch 20/500
289/289 - 40s - loss: 0.7704 - accuracy: 0.5506 - val_loss: 0.6873 - val_accuracy: 0.5460
Epoch 21/500
289/289 - 40s - loss: 0.7558 - accuracy: 0.5642 - val_loss: 0.6859 - val_accuracy: 0.5486
Epoch 22/500
289/289 - 40s - loss: 0.7530 - accuracy: 0.5635 - val_loss: 0.6848 - val_accuracy: 0.5539
Epoch 23/500
289/289 - 40s - loss: 0.7439 - accuracy: 0.5726 - val_loss: 0.6839 - val_accuracy: 0.5530
Epoch 24/500
289/289 - 40s - loss: 0.7469 - accuracy: 0.5717 - val_loss: 0.6833 - val_accuracy: 0.5521
Epoch 25/500
289/289 - 40s - loss: 0.7512 - accuracy: 0.5657 - val_loss: 0.6822 - val_accuracy: 0.5521
Epoch 26/500
289/289 - 40s - loss: 0.7447 - accuracy: 0.5717 - val_loss: 0.6814 - val_accuracy: 0.5600
Epoch 27/500
289/289 - 40s - loss: 0.7364 - accuracy: 0.5747 - val_loss: 0.6804 - val_accuracy: 0.5635
Epoch 28/500
289/289 - 40s - loss: 0.7399 - accuracy: 0.5807 - val_loss: 0.6805 - val_accuracy: 0.5592
Epoch 29/500
289/289 - 39s - loss: 0.7274 - accuracy: 0.5850 - val_loss: 0.6792 - val_accuracy: 0.5644
Epoch 30/500
289/289 - 40s - loss: 0.7250 - accuracy: 0.5841 - val_loss: 0.6782 - val_accuracy: 0.5644
Epoch 31/500
289/289 - 40s - loss: 0.7247 - accuracy: 0.5841 - val_loss: 0.6773 - val_accuracy: 0.5627
Epoch 32/500
289/289 - 40s - loss: 0.7189 - accuracy: 0.5951 - val_loss: 0.6767 - val_accuracy: 0.5635
Epoch 33/500
289/289 - 40s - loss: 0.7122 - accuracy: 0.5984 - val_loss: 0.6762 - val_accuracy: 0.5627
Epoch 34/500
289/289 - 40s - loss: 0.7106 - accuracy: 0.5999 - val_loss: 0.6751 - val_accuracy: 0.5662
Epoch 35/500
289/289 - 40s - loss: 0.7060 - accuracy: 0.6010 - val_loss: 0.6748 - val_accuracy: 0.5627
Epoch 36/500
289/289 - 40s - loss: 0.7048 - accuracy: 0.6049 - val_loss: 0.6739 - val_accuracy: 0.5697
Epoch 37/500
289/289 - 39s - loss: 0.6975 - accuracy: 0.6082 - val_loss: 0.6734 - val_accuracy: 0.5714
Epoch 38/500
289/289 - 40s - loss: 0.6898 - accuracy: 0.6122 - val_loss: 0.6725 - val_accuracy: 0.5758
Epoch 39/500
289/289 - 40s - loss: 0.6855 - accuracy: 0.6193 - val_loss: 0.6719 - val_accuracy: 0.5749
Epoch 40/500
289/289 - 40s - loss: 0.6857 - accuracy: 0.6203 - val_loss: 0.6716 - val_accuracy: 0.5811
Epoch 41/500
289/289 - 40s - loss: 0.6760 - accuracy: 0.6255 - val_loss: 0.6709 - val_accuracy: 0.5837
Epoch 42/500
289/289 - 40s - loss: 0.6666 - accuracy: 0.6302 - val_loss: 0.6703 - val_accuracy: 0.5863
Epoch 43/500
289/289 - 40s - loss: 0.6740 - accuracy: 0.6301 - val_loss: 0.6700 - val_accuracy: 0.5837
Epoch 44/500
289/289 - 40s - loss: 0.6686 - accuracy: 0.6305 - val_loss: 0.6686 - val_accuracy: 0.5811
Epoch 45/500
289/289 - 40s - loss: 0.6584 - accuracy: 0.6387 - val_loss: 0.6685 - val_accuracy: 0.5863
Epoch 46/500
289/289 - 40s - loss: 0.6642 - accuracy: 0.6360 - val_loss: 0.6684 - val_accuracy: 0.5872
Epoch 47/500
289/289 - 40s - loss: 0.6533 - accuracy: 0.6460 - val_loss: 0.6682 - val_accuracy: 0.5907
Epoch 48/500
289/289 - 40s - loss: 0.6331 - accuracy: 0.6575 - val_loss: 0.6680 - val_accuracy: 0.5890
Epoch 49/500
289/289 - 40s - loss: 0.6425 - accuracy: 0.6578 - val_loss: 0.6669 - val_accuracy: 0.5968
Epoch 50/500
289/289 - 40s - loss: 0.6388 - accuracy: 0.6518 - val_loss: 0.6676 - val_accuracy: 0.5986
Epoch 51/500
289/289 - 40s - loss: 0.6366 - accuracy: 0.6625 - val_loss: 0.6669 - val_accuracy: 0.5968
Epoch 52/500
289/289 - 40s - loss: 0.6240 - accuracy: 0.6684 - val_loss: 0.6675 - val_accuracy: 0.5960
Epoch 53/500
289/289 - 40s - loss: 0.6209 - accuracy: 0.6618 - val_loss: 0.6673 - val_accuracy: 0.6056
Epoch 54/500
289/289 - 40s - loss: 0.6190 - accuracy: 0.6767 - val_loss: 0.6675 - val_accuracy: 0.6039
Epoch 55/500
289/289 - 40s - loss: 0.6179 - accuracy: 0.6756 - val_loss: 0.6672 - val_accuracy: 0.6056
Epoch 56/500
289/289 - 40s - loss: 0.6032 - accuracy: 0.6752 - val_loss: 0.6676 - val_accuracy: 0.6021
Epoch 57/500
289/289 - 40s - loss: 0.6023 - accuracy: 0.6839 - val_loss: 0.6673 - val_accuracy: 0.6109
Epoch 58/500
289/289 - 40s - loss: 0.5924 - accuracy: 0.6851 - val_loss: 0.6675 - val_accuracy: 0.6109
Epoch 59/500
289/289 - 40s - loss: 0.5921 - accuracy: 0.6886 - val_loss: 0.6672 - val_accuracy: 0.6074
Epoch 60/500
289/289 - 39s - loss: 0.5899 - accuracy: 0.6942 - val_loss: 0.6679 - val_accuracy: 0.6117
Epoch 61/500
289/289 - 40s - loss: 0.5866 - accuracy: 0.6979 - val_loss: 0.6677 - val_accuracy: 0.6117
Epoch 62/500
289/289 - 39s - loss: 0.5766 - accuracy: 0.7017 - val_loss: 0.6686 - val_accuracy: 0.6144
Epoch 63/500
289/289 - 39s - loss: 0.5611 - accuracy: 0.7132 - val_loss: 0.6686 - val_accuracy: 0.6117
Epoch 64/500
289/289 - 39s - loss: 0.5613 - accuracy: 0.7097 - val_loss: 0.6700 - val_accuracy: 0.6126
Epoch 65/500
289/289 - 39s - loss: 0.5562 - accuracy: 0.7195 - val_loss: 0.6706 - val_accuracy: 0.6161
Epoch 66/500
289/289 - 39s - loss: 0.5500 - accuracy: 0.7221 - val_loss: 0.6715 - val_accuracy: 0.6188
Epoch 67/500
289/289 - 40s - loss: 0.5516 - accuracy: 0.7266 - val_loss: 0.6713 - val_accuracy: 0.6144
Epoch 68/500
289/289 - 39s - loss: 0.5418 - accuracy: 0.7253 - val_loss: 0.6725 - val_accuracy: 0.6135
Epoch 69/500
289/289 - 39s - loss: 0.5460 - accuracy: 0.7216 - val_loss: 0.6730 - val_accuracy: 0.6161
Epoch 70/500
289/289 - 39s - loss: 0.5295 - accuracy: 0.7378 - val_loss: 0.6733 - val_accuracy: 0.6196
Epoch 71/500
289/289 - 39s - loss: 0.5166 - accuracy: 0.7413 - val_loss: 0.6749 - val_accuracy: 0.6231
Epoch 72/500
289/289 - 39s - loss: 0.5303 - accuracy: 0.7347 - val_loss: 0.6750 - val_accuracy: 0.6188
Epoch 73/500
289/289 - 39s - loss: 0.5162 - accuracy: 0.7464 - val_loss: 0.6762 - val_accuracy: 0.6179
Epoch 74/500
289/289 - 39s - loss: 0.5106 - accuracy: 0.7480 - val_loss: 0.6769 - val_accuracy: 0.6205
Epoch 75/500
289/289 - 39s - loss: 0.5088 - accuracy: 0.7527 - val_loss: 0.6787 - val_accuracy: 0.6223
Epoch 76/500
289/289 - 39s - loss: 0.5051 - accuracy: 0.7491 - val_loss: 0.6795 - val_accuracy: 0.6205
Epoch 77/500
289/289 - 39s - loss: 0.5010 - accuracy: 0.7620 - val_loss: 0.6801 - val_accuracy: 0.6214
Epoch 78/500
289/289 - 39s - loss: 0.4954 - accuracy: 0.7581 - val_loss: 0.6816 - val_accuracy: 0.6249
Epoch 79/500
289/289 - 39s - loss: 0.4970 - accuracy: 0.7541 - val_loss: 0.6829 - val_accuracy: 0.6249
Epoch 80/500
289/289 - 39s - loss: 0.4826 - accuracy: 0.7679 - val_loss: 0.6842 - val_accuracy: 0.6231
Epoch 81/500
289/289 - 39s - loss: 0.4816 - accuracy: 0.7673 - val_loss: 0.6860 - val_accuracy: 0.6223
Epoch 82/500
289/289 - 39s - loss: 0.4721 - accuracy: 0.7688 - val_loss: 0.6878 - val_accuracy: 0.6284
Epoch 83/500
289/289 - 39s - loss: 0.4723 - accuracy: 0.7734 - val_loss: 0.6886 - val_accuracy: 0.6266
Epoch 84/500
289/289 - 39s - loss: 0.4699 - accuracy: 0.7789 - val_loss: 0.6910 - val_accuracy: 0.6275
Epoch 85/500
289/289 - 39s - loss: 0.4620 - accuracy: 0.7822 - val_loss: 0.6924 - val_accuracy: 0.6240
Epoch 86/500
289/289 - 39s - loss: 0.4604 - accuracy: 0.7824 - val_loss: 0.6937 - val_accuracy: 0.6301
Epoch 87/500
289/289 - 39s - loss: 0.4469 - accuracy: 0.7875 - val_loss: 0.6954 - val_accuracy: 0.6266
Epoch 88/500
289/289 - 39s - loss: 0.4404 - accuracy: 0.7948 - val_loss: 0.6960 - val_accuracy: 0.6266
Epoch 89/500
289/289 - 39s - loss: 0.4574 - accuracy: 0.7809 - val_loss: 0.6986 - val_accuracy: 0.6284
Epoch 90/500
289/289 - 39s - loss: 0.4278 - accuracy: 0.8011 - val_loss: 0.7007 - val_accuracy: 0.6266
Epoch 91/500
289/289 - 39s - loss: 0.4317 - accuracy: 0.7982 - val_loss: 0.7025 - val_accuracy: 0.6231
Epoch 92/500
289/289 - 39s - loss: 0.4304 - accuracy: 0.8010 - val_loss: 0.7037 - val_accuracy: 0.6275
Epoch 93/500
289/289 - 39s - loss: 0.4259 - accuracy: 0.8043 - val_loss: 0.7066 - val_accuracy: 0.6293
Epoch 94/500
289/289 - 39s - loss: 0.4166 - accuracy: 0.8109 - val_loss: 0.7080 - val_accuracy: 0.6319
Epoch 95/500
289/289 - 39s - loss: 0.4136 - accuracy: 0.8126 - val_loss: 0.7112 - val_accuracy: 0.6319
Epoch 96/500
289/289 - 39s - loss: 0.4033 - accuracy: 0.8164 - val_loss: 0.7139 - val_accuracy: 0.6310
Epoch 97/500
289/289 - 39s - loss: 0.3999 - accuracy: 0.8142 - val_loss: 0.7142 - val_accuracy: 0.6293
Epoch 98/500
289/289 - 39s - loss: 0.4023 - accuracy: 0.8136 - val_loss: 0.7174 - val_accuracy: 0.6354
Epoch 99/500
289/289 - 39s - loss: 0.3953 - accuracy: 0.8186 - val_loss: 0.7189 - val_accuracy: 0.6354
Epoch 100/500
289/289 - 39s - loss: 0.3937 - accuracy: 0.8222 - val_loss: 0.7218 - val_accuracy: 0.6380
Epoch 101/500
289/289 - 39s - loss: 0.3898 - accuracy: 0.8236 - val_loss: 0.7224 - val_accuracy: 0.6372
Epoch 102/500
289/289 - 39s - loss: 0.3901 - accuracy: 0.8217 - val_loss: 0.7243 - val_accuracy: 0.6337
Epoch 103/500
289/289 - 39s - loss: 0.3702 - accuracy: 0.8307 - val_loss: 0.7265 - val_accuracy: 0.6389
Epoch 104/500
289/289 - 39s - loss: 0.3776 - accuracy: 0.8357 - val_loss: 0.7283 - val_accuracy: 0.6407
Epoch 105/500
289/289 - 39s - loss: 0.3717 - accuracy: 0.8369 - val_loss: 0.7311 - val_accuracy: 0.6398
Epoch 106/500
289/289 - 39s - loss: 0.3649 - accuracy: 0.8307 - val_loss: 0.7335 - val_accuracy: 0.6372
Epoch 107/500
289/289 - 39s - loss: 0.3635 - accuracy: 0.8379 - val_loss: 0.7351 - val_accuracy: 0.6407
Epoch 108/500
289/289 - 39s - loss: 0.3674 - accuracy: 0.8334 - val_loss: 0.7384 - val_accuracy: 0.6442
Epoch 109/500
289/289 - 39s - loss: 0.3464 - accuracy: 0.8489 - val_loss: 0.7407 - val_accuracy: 0.6459
Epoch 110/500
289/289 - 39s - loss: 0.3506 - accuracy: 0.8460 - val_loss: 0.7436 - val_accuracy: 0.6459
Epoch 111/500
289/289 - 39s - loss: 0.3397 - accuracy: 0.8495 - val_loss: 0.7448 - val_accuracy: 0.6468
Epoch 112/500
289/289 - 39s - loss: 0.3475 - accuracy: 0.8442 - val_loss: 0.7474 - val_accuracy: 0.6415
Epoch 113/500
289/289 - 39s - loss: 0.3412 - accuracy: 0.8489 - val_loss: 0.7508 - val_accuracy: 0.6424
Epoch 114/500
289/289 - 39s - loss: 0.3329 - accuracy: 0.8572 - val_loss: 0.7531 - val_accuracy: 0.6424
Epoch 115/500
289/289 - 39s - loss: 0.3321 - accuracy: 0.8524 - val_loss: 0.7554 - val_accuracy: 0.6442
Epoch 116/500
289/289 - 39s - loss: 0.3234 - accuracy: 0.8598 - val_loss: 0.7587 - val_accuracy: 0.6459
Epoch 117/500
289/289 - 39s - loss: 0.3341 - accuracy: 0.8545 - val_loss: 0.7600 - val_accuracy: 0.6407
Epoch 118/500
289/289 - 39s - loss: 0.3205 - accuracy: 0.8623 - val_loss: 0.7633 - val_accuracy: 0.6433
Epoch 119/500
289/289 - 39s - loss: 0.3089 - accuracy: 0.8640 - val_loss: 0.7653 - val_accuracy: 0.6459
Epoch 120/500
289/289 - 39s - loss: 0.3117 - accuracy: 0.8662 - val_loss: 0.7666 - val_accuracy: 0.6468
Epoch 121/500
289/289 - 39s - loss: 0.3092 - accuracy: 0.8666 - val_loss: 0.7694 - val_accuracy: 0.6468
Epoch 122/500
289/289 - 39s - loss: 0.3100 - accuracy: 0.8650 - val_loss: 0.7726 - val_accuracy: 0.6450
Epoch 123/500
289/289 - 39s - loss: 0.2998 - accuracy: 0.8731 - val_loss: 0.7760 - val_accuracy: 0.6450
Epoch 124/500
289/289 - 39s - loss: 0.2925 - accuracy: 0.8734 - val_loss: 0.7799 - val_accuracy: 0.6442
Epoch 125/500
289/289 - 40s - loss: 0.3008 - accuracy: 0.8721 - val_loss: 0.7844 - val_accuracy: 0.6407
Epoch 126/500
289/289 - 39s - loss: 0.2864 - accuracy: 0.8739 - val_loss: 0.7872 - val_accuracy: 0.6433
Epoch 127/500
289/289 - 39s - loss: 0.2904 - accuracy: 0.8756 - val_loss: 0.7887 - val_accuracy: 0.6424
Epoch 128/500
289/289 - 39s - loss: 0.2883 - accuracy: 0.8741 - val_loss: 0.7894 - val_accuracy: 0.6424
Epoch 129/500
289/289 - 39s - loss: 0.2695 - accuracy: 0.8844 - val_loss: 0.7935 - val_accuracy: 0.6459
Epoch 130/500
289/289 - 39s - loss: 0.2841 - accuracy: 0.8769 - val_loss: 0.7970 - val_accuracy: 0.6468
Epoch 131/500
289/289 - 39s - loss: 0.2827 - accuracy: 0.8781 - val_loss: 0.8006 - val_accuracy: 0.6442
========================================
save_weights
h5_weights/TH1.po/embedding_cnn_one_branch.h5
========================================

end time >>> Mon Oct  4 05:39:24 2021

end time >>> Mon Oct  4 05:39:24 2021

end time >>> Mon Oct  4 05:39:24 2021

end time >>> Mon Oct  4 05:39:24 2021

end time >>> Mon Oct  4 05:39:24 2021












args.model = embedding_cnn_one_branch
time used = 5208.407384395599


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 05:39:26 2021

begin time >>> Mon Oct  4 05:39:26 2021

begin time >>> Mon Oct  4 05:39:26 2021

begin time >>> Mon Oct  4 05:39:26 2021

begin time >>> Mon Oct  4 05:39:26 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_cnn_two_branch
args.type = train
args.name = TH1.po
args.length = 10001
===========================


-> h5_weights/TH1.po folder already exist. pass.
-> result/TH1.po/onehot_cnn_one_branch folder already exist. pass.
-> result/TH1.po/onehot_cnn_two_branch folder already exist. pass.
-> result/TH1.po/onehot_embedding_dense folder already exist. pass.
-> result/TH1.po/onehot_dense folder already exist. pass.
-> result/TH1.po/onehot_resnet18 folder already exist. pass.
-> result/TH1.po/onehot_resnet34 folder already exist. pass.
-> result/TH1.po/embedding_cnn_one_branch folder already exist. pass.
-> result/TH1.po/embedding_cnn_two_branch folder already exist. pass.
-> result/TH1.po/embedding_dense folder already exist. pass.
-> result/TH1.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/TH1.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
TH1.po
########################################

########################################
model_name
embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
289/289 - 40s - loss: 0.8588 - accuracy: 0.5062 - val_loss: 0.7021 - val_accuracy: 0.5039
Epoch 2/500
289/289 - 40s - loss: 0.8585 - accuracy: 0.5052 - val_loss: 0.7107 - val_accuracy: 0.5004
Epoch 3/500
289/289 - 39s - loss: 0.8524 - accuracy: 0.5107 - val_loss: 0.7097 - val_accuracy: 0.5004
Epoch 4/500
289/289 - 40s - loss: 0.8490 - accuracy: 0.5047 - val_loss: 0.7073 - val_accuracy: 0.5118
Epoch 5/500
289/289 - 40s - loss: 0.8322 - accuracy: 0.5153 - val_loss: 0.7058 - val_accuracy: 0.5092
Epoch 6/500
289/289 - 39s - loss: 0.8355 - accuracy: 0.5156 - val_loss: 0.7037 - val_accuracy: 0.5118
Epoch 7/500
289/289 - 40s - loss: 0.8351 - accuracy: 0.5133 - val_loss: 0.7026 - val_accuracy: 0.5162
Epoch 8/500
289/289 - 39s - loss: 0.8168 - accuracy: 0.5278 - val_loss: 0.7005 - val_accuracy: 0.5206
Epoch 9/500
289/289 - 39s - loss: 0.8029 - accuracy: 0.5329 - val_loss: 0.6988 - val_accuracy: 0.5241
Epoch 10/500
289/289 - 39s - loss: 0.8022 - accuracy: 0.5364 - val_loss: 0.6979 - val_accuracy: 0.5302
Epoch 11/500
289/289 - 39s - loss: 0.7959 - accuracy: 0.5429 - val_loss: 0.6968 - val_accuracy: 0.5267
Epoch 12/500
289/289 - 39s - loss: 0.8001 - accuracy: 0.5390 - val_loss: 0.6952 - val_accuracy: 0.5267
Epoch 13/500
289/289 - 39s - loss: 0.7952 - accuracy: 0.5404 - val_loss: 0.6937 - val_accuracy: 0.5399
Epoch 14/500
289/289 - 39s - loss: 0.7827 - accuracy: 0.5454 - val_loss: 0.6926 - val_accuracy: 0.5425
Epoch 15/500
289/289 - 40s - loss: 0.7734 - accuracy: 0.5519 - val_loss: 0.6916 - val_accuracy: 0.5399
Epoch 16/500
289/289 - 40s - loss: 0.7651 - accuracy: 0.5544 - val_loss: 0.6903 - val_accuracy: 0.5478
Epoch 17/500
289/289 - 40s - loss: 0.7723 - accuracy: 0.5513 - val_loss: 0.6889 - val_accuracy: 0.5574
Epoch 18/500
289/289 - 40s - loss: 0.7686 - accuracy: 0.5591 - val_loss: 0.6879 - val_accuracy: 0.5557
Epoch 19/500
289/289 - 40s - loss: 0.7553 - accuracy: 0.5626 - val_loss: 0.6872 - val_accuracy: 0.5592
Epoch 20/500
289/289 - 40s - loss: 0.7593 - accuracy: 0.5642 - val_loss: 0.6865 - val_accuracy: 0.5574
Epoch 21/500
289/289 - 40s - loss: 0.7487 - accuracy: 0.5687 - val_loss: 0.6858 - val_accuracy: 0.5583
Epoch 22/500
289/289 - 39s - loss: 0.7402 - accuracy: 0.5758 - val_loss: 0.6846 - val_accuracy: 0.5627
Epoch 23/500
289/289 - 40s - loss: 0.7481 - accuracy: 0.5683 - val_loss: 0.6833 - val_accuracy: 0.5635
Epoch 24/500
289/289 - 40s - loss: 0.7442 - accuracy: 0.5792 - val_loss: 0.6826 - val_accuracy: 0.5706
Epoch 25/500
289/289 - 39s - loss: 0.7345 - accuracy: 0.5738 - val_loss: 0.6814 - val_accuracy: 0.5706
Epoch 26/500
289/289 - 39s - loss: 0.7412 - accuracy: 0.5724 - val_loss: 0.6805 - val_accuracy: 0.5723
Epoch 27/500
289/289 - 40s - loss: 0.7285 - accuracy: 0.5863 - val_loss: 0.6799 - val_accuracy: 0.5732
Epoch 28/500
289/289 - 40s - loss: 0.7163 - accuracy: 0.5975 - val_loss: 0.6790 - val_accuracy: 0.5723
Epoch 29/500
289/289 - 40s - loss: 0.7079 - accuracy: 0.6025 - val_loss: 0.6782 - val_accuracy: 0.5837
Epoch 30/500
289/289 - 40s - loss: 0.7057 - accuracy: 0.5963 - val_loss: 0.6771 - val_accuracy: 0.5793
Epoch 31/500
289/289 - 40s - loss: 0.6999 - accuracy: 0.6082 - val_loss: 0.6758 - val_accuracy: 0.5837
Epoch 32/500
289/289 - 40s - loss: 0.7173 - accuracy: 0.5987 - val_loss: 0.6748 - val_accuracy: 0.5863
Epoch 33/500
289/289 - 40s - loss: 0.7028 - accuracy: 0.6128 - val_loss: 0.6743 - val_accuracy: 0.5846
Epoch 34/500
289/289 - 40s - loss: 0.6767 - accuracy: 0.6256 - val_loss: 0.6732 - val_accuracy: 0.5855
Epoch 35/500
289/289 - 39s - loss: 0.6860 - accuracy: 0.6159 - val_loss: 0.6727 - val_accuracy: 0.5837
Epoch 36/500
289/289 - 40s - loss: 0.6770 - accuracy: 0.6279 - val_loss: 0.6715 - val_accuracy: 0.5907
Epoch 37/500
289/289 - 40s - loss: 0.6790 - accuracy: 0.6230 - val_loss: 0.6704 - val_accuracy: 0.5898
Epoch 38/500
289/289 - 39s - loss: 0.6663 - accuracy: 0.6310 - val_loss: 0.6701 - val_accuracy: 0.5951
Epoch 39/500
289/289 - 40s - loss: 0.6636 - accuracy: 0.6372 - val_loss: 0.6694 - val_accuracy: 0.5933
Epoch 40/500
289/289 - 40s - loss: 0.6680 - accuracy: 0.6358 - val_loss: 0.6688 - val_accuracy: 0.5933
Epoch 41/500
289/289 - 40s - loss: 0.6618 - accuracy: 0.6380 - val_loss: 0.6684 - val_accuracy: 0.5907
Epoch 42/500
289/289 - 39s - loss: 0.6457 - accuracy: 0.6459 - val_loss: 0.6677 - val_accuracy: 0.5960
Epoch 43/500
289/289 - 39s - loss: 0.6534 - accuracy: 0.6393 - val_loss: 0.6674 - val_accuracy: 0.5995
Epoch 44/500
289/289 - 40s - loss: 0.6319 - accuracy: 0.6624 - val_loss: 0.6666 - val_accuracy: 0.6021
Epoch 45/500
289/289 - 39s - loss: 0.6352 - accuracy: 0.6635 - val_loss: 0.6663 - val_accuracy: 0.5995
Epoch 46/500
289/289 - 39s - loss: 0.6209 - accuracy: 0.6714 - val_loss: 0.6663 - val_accuracy: 0.6056
Epoch 47/500
289/289 - 39s - loss: 0.6168 - accuracy: 0.6708 - val_loss: 0.6664 - val_accuracy: 0.6100
Epoch 48/500
289/289 - 40s - loss: 0.6096 - accuracy: 0.6784 - val_loss: 0.6657 - val_accuracy: 0.6117
Epoch 49/500
289/289 - 39s - loss: 0.6155 - accuracy: 0.6715 - val_loss: 0.6660 - val_accuracy: 0.6074
Epoch 50/500
289/289 - 39s - loss: 0.6031 - accuracy: 0.6880 - val_loss: 0.6648 - val_accuracy: 0.6117
Epoch 51/500
289/289 - 39s - loss: 0.5997 - accuracy: 0.6875 - val_loss: 0.6652 - val_accuracy: 0.6109
Epoch 52/500
289/289 - 40s - loss: 0.5953 - accuracy: 0.6911 - val_loss: 0.6652 - val_accuracy: 0.6082
Epoch 53/500
289/289 - 40s - loss: 0.5865 - accuracy: 0.6932 - val_loss: 0.6647 - val_accuracy: 0.6109
Epoch 54/500
289/289 - 39s - loss: 0.5886 - accuracy: 0.6988 - val_loss: 0.6644 - val_accuracy: 0.6117
Epoch 55/500
289/289 - 39s - loss: 0.5757 - accuracy: 0.7027 - val_loss: 0.6652 - val_accuracy: 0.6117
Epoch 56/500
289/289 - 40s - loss: 0.5737 - accuracy: 0.7063 - val_loss: 0.6655 - val_accuracy: 0.6126
Epoch 57/500
289/289 - 39s - loss: 0.5699 - accuracy: 0.7060 - val_loss: 0.6654 - val_accuracy: 0.6126
Epoch 58/500
289/289 - 40s - loss: 0.5611 - accuracy: 0.7170 - val_loss: 0.6653 - val_accuracy: 0.6144
Epoch 59/500
289/289 - 40s - loss: 0.5468 - accuracy: 0.7213 - val_loss: 0.6660 - val_accuracy: 0.6179
Epoch 60/500
289/289 - 39s - loss: 0.5446 - accuracy: 0.7272 - val_loss: 0.6664 - val_accuracy: 0.6170
Epoch 61/500
289/289 - 39s - loss: 0.5450 - accuracy: 0.7211 - val_loss: 0.6671 - val_accuracy: 0.6231
Epoch 62/500
289/289 - 40s - loss: 0.5368 - accuracy: 0.7332 - val_loss: 0.6675 - val_accuracy: 0.6196
Epoch 63/500
289/289 - 39s - loss: 0.5342 - accuracy: 0.7371 - val_loss: 0.6686 - val_accuracy: 0.6179
Epoch 64/500
289/289 - 40s - loss: 0.5229 - accuracy: 0.7419 - val_loss: 0.6692 - val_accuracy: 0.6188
Epoch 65/500
289/289 - 40s - loss: 0.5125 - accuracy: 0.7501 - val_loss: 0.6709 - val_accuracy: 0.6152
Epoch 66/500
289/289 - 39s - loss: 0.5213 - accuracy: 0.7451 - val_loss: 0.6721 - val_accuracy: 0.6170
Epoch 67/500
289/289 - 39s - loss: 0.5077 - accuracy: 0.7475 - val_loss: 0.6736 - val_accuracy: 0.6188
Epoch 68/500
289/289 - 39s - loss: 0.5008 - accuracy: 0.7607 - val_loss: 0.6742 - val_accuracy: 0.6196
Epoch 69/500
289/289 - 39s - loss: 0.4931 - accuracy: 0.7617 - val_loss: 0.6757 - val_accuracy: 0.6196
Epoch 70/500
289/289 - 40s - loss: 0.4818 - accuracy: 0.7654 - val_loss: 0.6773 - val_accuracy: 0.6240
Epoch 71/500
289/289 - 40s - loss: 0.4820 - accuracy: 0.7684 - val_loss: 0.6788 - val_accuracy: 0.6170
Epoch 72/500
289/289 - 39s - loss: 0.4698 - accuracy: 0.7756 - val_loss: 0.6801 - val_accuracy: 0.6214
Epoch 73/500
289/289 - 39s - loss: 0.4752 - accuracy: 0.7784 - val_loss: 0.6810 - val_accuracy: 0.6231
Epoch 74/500
289/289 - 39s - loss: 0.4652 - accuracy: 0.7792 - val_loss: 0.6822 - val_accuracy: 0.6240
Epoch 75/500
289/289 - 40s - loss: 0.4480 - accuracy: 0.7925 - val_loss: 0.6839 - val_accuracy: 0.6240
Epoch 76/500
289/289 - 40s - loss: 0.4468 - accuracy: 0.7937 - val_loss: 0.6862 - val_accuracy: 0.6249
Epoch 77/500
289/289 - 39s - loss: 0.4377 - accuracy: 0.7948 - val_loss: 0.6879 - val_accuracy: 0.6240
Epoch 78/500
289/289 - 40s - loss: 0.4336 - accuracy: 0.8025 - val_loss: 0.6907 - val_accuracy: 0.6205
Epoch 79/500
289/289 - 39s - loss: 0.4292 - accuracy: 0.8001 - val_loss: 0.6927 - val_accuracy: 0.6240
Epoch 80/500
289/289 - 40s - loss: 0.4318 - accuracy: 0.8039 - val_loss: 0.6936 - val_accuracy: 0.6214
Epoch 81/500
289/289 - 40s - loss: 0.4248 - accuracy: 0.8023 - val_loss: 0.6965 - val_accuracy: 0.6231
Epoch 82/500
289/289 - 40s - loss: 0.4238 - accuracy: 0.8061 - val_loss: 0.6995 - val_accuracy: 0.6188
Epoch 83/500
289/289 - 40s - loss: 0.4196 - accuracy: 0.8015 - val_loss: 0.7012 - val_accuracy: 0.6179
Epoch 84/500
289/289 - 40s - loss: 0.4046 - accuracy: 0.8217 - val_loss: 0.7037 - val_accuracy: 0.6161
Epoch 85/500
289/289 - 40s - loss: 0.4023 - accuracy: 0.8130 - val_loss: 0.7057 - val_accuracy: 0.6231
Epoch 86/500
289/289 - 40s - loss: 0.3879 - accuracy: 0.8274 - val_loss: 0.7072 - val_accuracy: 0.6223
Epoch 87/500
289/289 - 39s - loss: 0.3880 - accuracy: 0.8220 - val_loss: 0.7100 - val_accuracy: 0.6223
Epoch 88/500
289/289 - 39s - loss: 0.3884 - accuracy: 0.8267 - val_loss: 0.7134 - val_accuracy: 0.6205
Epoch 89/500
289/289 - 39s - loss: 0.3732 - accuracy: 0.8317 - val_loss: 0.7160 - val_accuracy: 0.6205
Epoch 90/500
289/289 - 40s - loss: 0.3788 - accuracy: 0.8314 - val_loss: 0.7182 - val_accuracy: 0.6223
Epoch 91/500
289/289 - 40s - loss: 0.3661 - accuracy: 0.8396 - val_loss: 0.7207 - val_accuracy: 0.6249
Epoch 92/500
289/289 - 40s - loss: 0.3625 - accuracy: 0.8352 - val_loss: 0.7237 - val_accuracy: 0.6231
Epoch 93/500
289/289 - 40s - loss: 0.3571 - accuracy: 0.8397 - val_loss: 0.7274 - val_accuracy: 0.6266
Epoch 94/500
289/289 - 40s - loss: 0.3415 - accuracy: 0.8541 - val_loss: 0.7293 - val_accuracy: 0.6266
Epoch 95/500
289/289 - 40s - loss: 0.3491 - accuracy: 0.8464 - val_loss: 0.7321 - val_accuracy: 0.6293
Epoch 96/500
289/289 - 40s - loss: 0.3414 - accuracy: 0.8511 - val_loss: 0.7359 - val_accuracy: 0.6301
Epoch 97/500
289/289 - 40s - loss: 0.3397 - accuracy: 0.8494 - val_loss: 0.7371 - val_accuracy: 0.6301
Epoch 98/500
289/289 - 40s - loss: 0.3333 - accuracy: 0.8585 - val_loss: 0.7413 - val_accuracy: 0.6301
Epoch 99/500
289/289 - 40s - loss: 0.3305 - accuracy: 0.8544 - val_loss: 0.7427 - val_accuracy: 0.6258
Epoch 100/500
289/289 - 40s - loss: 0.3253 - accuracy: 0.8615 - val_loss: 0.7463 - val_accuracy: 0.6249
Epoch 101/500
289/289 - 40s - loss: 0.3155 - accuracy: 0.8619 - val_loss: 0.7498 - val_accuracy: 0.6266
Epoch 102/500
289/289 - 40s - loss: 0.3198 - accuracy: 0.8586 - val_loss: 0.7543 - val_accuracy: 0.6284
Epoch 103/500
289/289 - 39s - loss: 0.3026 - accuracy: 0.8711 - val_loss: 0.7571 - val_accuracy: 0.6275
Epoch 104/500
289/289 - 40s - loss: 0.3047 - accuracy: 0.8671 - val_loss: 0.7625 - val_accuracy: 0.6301
Epoch 105/500
289/289 - 40s - loss: 0.3052 - accuracy: 0.8695 - val_loss: 0.7646 - val_accuracy: 0.6284
Epoch 106/500
289/289 - 40s - loss: 0.2943 - accuracy: 0.8751 - val_loss: 0.7685 - val_accuracy: 0.6284
Epoch 107/500
289/289 - 40s - loss: 0.2915 - accuracy: 0.8775 - val_loss: 0.7712 - val_accuracy: 0.6266
Epoch 108/500
289/289 - 40s - loss: 0.2814 - accuracy: 0.8850 - val_loss: 0.7743 - val_accuracy: 0.6249
Epoch 109/500
289/289 - 40s - loss: 0.2844 - accuracy: 0.8799 - val_loss: 0.7779 - val_accuracy: 0.6249
Epoch 110/500
289/289 - 39s - loss: 0.2729 - accuracy: 0.8855 - val_loss: 0.7834 - val_accuracy: 0.6301
Epoch 111/500
289/289 - 40s - loss: 0.2768 - accuracy: 0.8862 - val_loss: 0.7869 - val_accuracy: 0.6319
Epoch 112/500
289/289 - 39s - loss: 0.2675 - accuracy: 0.8917 - val_loss: 0.7898 - val_accuracy: 0.6310
Epoch 113/500
289/289 - 39s - loss: 0.2675 - accuracy: 0.8845 - val_loss: 0.7942 - val_accuracy: 0.6328
Epoch 114/500
289/289 - 40s - loss: 0.2761 - accuracy: 0.8838 - val_loss: 0.7947 - val_accuracy: 0.6372
Epoch 115/500
289/289 - 39s - loss: 0.2634 - accuracy: 0.8911 - val_loss: 0.7994 - val_accuracy: 0.6345
Epoch 116/500
289/289 - 39s - loss: 0.2556 - accuracy: 0.8930 - val_loss: 0.8041 - val_accuracy: 0.6319
Epoch 117/500
289/289 - 40s - loss: 0.2534 - accuracy: 0.8928 - val_loss: 0.8083 - val_accuracy: 0.6310
Epoch 118/500
289/289 - 39s - loss: 0.2475 - accuracy: 0.8949 - val_loss: 0.8119 - val_accuracy: 0.6301
Epoch 119/500
289/289 - 39s - loss: 0.2432 - accuracy: 0.8984 - val_loss: 0.8151 - val_accuracy: 0.6301
Epoch 120/500
289/289 - 39s - loss: 0.2482 - accuracy: 0.8962 - val_loss: 0.8210 - val_accuracy: 0.6319
Epoch 121/500
289/289 - 40s - loss: 0.2315 - accuracy: 0.9062 - val_loss: 0.8239 - val_accuracy: 0.6284
Epoch 122/500
289/289 - 39s - loss: 0.2302 - accuracy: 0.9070 - val_loss: 0.8277 - val_accuracy: 0.6328
Epoch 123/500
289/289 - 39s - loss: 0.2237 - accuracy: 0.9107 - val_loss: 0.8290 - val_accuracy: 0.6345
Epoch 124/500
289/289 - 39s - loss: 0.2192 - accuracy: 0.9119 - val_loss: 0.8335 - val_accuracy: 0.6363
Epoch 125/500
289/289 - 39s - loss: 0.2265 - accuracy: 0.9076 - val_loss: 0.8395 - val_accuracy: 0.6328
Epoch 126/500
289/289 - 39s - loss: 0.2270 - accuracy: 0.9073 - val_loss: 0.8426 - val_accuracy: 0.6372
Epoch 127/500
289/289 - 39s - loss: 0.2175 - accuracy: 0.9135 - val_loss: 0.8460 - val_accuracy: 0.6354
Epoch 128/500
289/289 - 39s - loss: 0.2095 - accuracy: 0.9127 - val_loss: 0.8478 - val_accuracy: 0.6372
Epoch 129/500
289/289 - 39s - loss: 0.2133 - accuracy: 0.9110 - val_loss: 0.8528 - val_accuracy: 0.6363
Epoch 130/500
289/289 - 39s - loss: 0.2027 - accuracy: 0.9164 - val_loss: 0.8560 - val_accuracy: 0.6372
Epoch 131/500
289/289 - 39s - loss: 0.2008 - accuracy: 0.9168 - val_loss: 0.8599 - val_accuracy: 0.6345
Epoch 132/500
289/289 - 39s - loss: 0.2010 - accuracy: 0.9181 - val_loss: 0.8636 - val_accuracy: 0.6354
Epoch 133/500
289/289 - 39s - loss: 0.1962 - accuracy: 0.9210 - val_loss: 0.8705 - val_accuracy: 0.6354
Epoch 134/500
289/289 - 39s - loss: 0.1985 - accuracy: 0.9187 - val_loss: 0.8725 - val_accuracy: 0.6354
========================================
save_weights
h5_weights/TH1.po/embedding_cnn_two_branch.h5
========================================

end time >>> Mon Oct  4 07:08:11 2021

end time >>> Mon Oct  4 07:08:11 2021

end time >>> Mon Oct  4 07:08:11 2021

end time >>> Mon Oct  4 07:08:11 2021

end time >>> Mon Oct  4 07:08:11 2021












args.model = embedding_cnn_two_branch
time used = 5325.356865406036


