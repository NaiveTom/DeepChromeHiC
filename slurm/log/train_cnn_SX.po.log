************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 08:58:35 2021

begin time >>> Mon Oct  4 08:58:35 2021

begin time >>> Mon Oct  4 08:58:35 2021

begin time >>> Mon Oct  4 08:58:35 2021

begin time >>> Mon Oct  4 08:58:35 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_cnn_one_branch
args.type = train
args.name = SX.po
args.length = 10001
===========================


-> make new folder: h5_weights/SX.po
-> make new folder: result/SX.po/onehot_cnn_one_branch
-> make new folder: result/SX.po/onehot_cnn_two_branch
-> make new folder: result/SX.po/onehot_embedding_dense
-> make new folder: result/SX.po/onehot_dense
-> make new folder: result/SX.po/onehot_resnet18
-> make new folder: result/SX.po/onehot_resnet34
-> make new folder: result/SX.po/embedding_cnn_one_branch
-> make new folder: result/SX.po/embedding_cnn_two_branch
-> make new folder: result/SX.po/embedding_dense
-> make new folder: result/SX.po/onehot_embedding_cnn_one_branch
-> make new folder: result/SX.po/onehot_embedding_cnn_two_branch
########################################
gen_name
SX.po
########################################

########################################
model_name
onehot_cnn_one_branch
########################################

Found 25666 images belonging to 2 classes.
Found 3170 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 5001, 5, 64)       1600      
_________________________________________________________________
batch_normalization (BatchNo (None, 5001, 5, 64)       256       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1251, 5, 64)       98368     
_________________________________________________________________
batch_normalization_1 (Batch (None, 1251, 5, 64)       256       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 313, 5, 64)        98368     
_________________________________________________________________
batch_normalization_2 (Batch (None, 313, 5, 64)        256       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 156, 5, 64)        0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 156, 5, 64)        256       
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 39, 5, 128)        196736    
_________________________________________________________________
batch_normalization_4 (Batch (None, 39, 5, 128)        512       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 10, 5, 128)        393344    
_________________________________________________________________
batch_normalization_5 (Batch (None, 10, 5, 128)        512       
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 3, 5, 128)         393344    
_________________________________________________________________
batch_normalization_6 (Batch (None, 3, 5, 128)         512       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 1, 5, 128)         0         
_________________________________________________________________
flatten (Flatten)            (None, 640)               0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 640)               2560      
_________________________________________________________________
dense (Dense)                (None, 512)               328192    
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
dense_3 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 2,041,410
Trainable params: 2,038,850
Non-trainable params: 2,560
_________________________________________________________________
Epoch 1/500
802/802 - 898s - loss: 0.7424 - accuracy: 0.5172 - val_loss: 0.7077 - val_accuracy: 0.5268
Epoch 2/500
802/802 - 100s - loss: 0.6880 - accuracy: 0.5735 - val_loss: 0.8735 - val_accuracy: 0.5003
Epoch 3/500
802/802 - 100s - loss: 0.6560 - accuracy: 0.6169 - val_loss: 0.7215 - val_accuracy: 0.5338
Epoch 4/500
802/802 - 99s - loss: 0.6026 - accuracy: 0.6743 - val_loss: 0.7215 - val_accuracy: 0.5672
Epoch 5/500
802/802 - 100s - loss: 0.4941 - accuracy: 0.7611 - val_loss: 0.8307 - val_accuracy: 0.6089
Epoch 6/500
802/802 - 99s - loss: 0.3470 - accuracy: 0.8500 - val_loss: 0.8188 - val_accuracy: 0.6490
Epoch 7/500
802/802 - 100s - loss: 0.2043 - accuracy: 0.9192 - val_loss: 2.5093 - val_accuracy: 0.5240
Epoch 8/500
802/802 - 100s - loss: 0.1139 - accuracy: 0.9560 - val_loss: 5.0585 - val_accuracy: 0.5234
Epoch 9/500
802/802 - 102s - loss: 0.0789 - accuracy: 0.9699 - val_loss: 3.3465 - val_accuracy: 0.5644
Epoch 10/500
802/802 - 100s - loss: 0.0620 - accuracy: 0.9775 - val_loss: 2.0064 - val_accuracy: 0.6547
Epoch 11/500
802/802 - 100s - loss: 0.0612 - accuracy: 0.9776 - val_loss: 2.2264 - val_accuracy: 0.6222
Epoch 12/500
802/802 - 100s - loss: 0.0559 - accuracy: 0.9783 - val_loss: 3.6823 - val_accuracy: 0.5578
Epoch 13/500
802/802 - 100s - loss: 0.0471 - accuracy: 0.9834 - val_loss: 5.4346 - val_accuracy: 0.5300
Epoch 14/500
802/802 - 100s - loss: 0.0401 - accuracy: 0.9865 - val_loss: 1.9906 - val_accuracy: 0.6616
Epoch 15/500
802/802 - 101s - loss: 0.0392 - accuracy: 0.9865 - val_loss: 2.2580 - val_accuracy: 0.6389
Epoch 16/500
802/802 - 100s - loss: 0.0376 - accuracy: 0.9868 - val_loss: 3.3748 - val_accuracy: 0.6357
Epoch 17/500
802/802 - 100s - loss: 0.0455 - accuracy: 0.9843 - val_loss: 2.2014 - val_accuracy: 0.6569
Epoch 18/500
802/802 - 100s - loss: 0.0384 - accuracy: 0.9857 - val_loss: 2.5489 - val_accuracy: 0.6398
Epoch 19/500
802/802 - 100s - loss: 0.0216 - accuracy: 0.9931 - val_loss: 2.6621 - val_accuracy: 0.6357
Epoch 20/500
802/802 - 100s - loss: 0.0274 - accuracy: 0.9905 - val_loss: 2.6844 - val_accuracy: 0.6477
Epoch 21/500
802/802 - 102s - loss: 0.0347 - accuracy: 0.9874 - val_loss: 1.8999 - val_accuracy: 0.6604
Epoch 22/500
802/802 - 101s - loss: 0.0228 - accuracy: 0.9917 - val_loss: 2.4318 - val_accuracy: 0.6556
Epoch 23/500
802/802 - 100s - loss: 0.0273 - accuracy: 0.9905 - val_loss: 2.3753 - val_accuracy: 0.6717
Epoch 24/500
802/802 - 100s - loss: 0.0281 - accuracy: 0.9898 - val_loss: 1.9851 - val_accuracy: 0.6676
Epoch 25/500
802/802 - 100s - loss: 0.0157 - accuracy: 0.9948 - val_loss: 2.3318 - val_accuracy: 0.6689
Epoch 26/500
802/802 - 100s - loss: 0.0222 - accuracy: 0.9926 - val_loss: 2.4408 - val_accuracy: 0.6477
Epoch 27/500
802/802 - 100s - loss: 0.0297 - accuracy: 0.9899 - val_loss: 2.6951 - val_accuracy: 0.6468
Epoch 28/500
802/802 - 100s - loss: 0.0205 - accuracy: 0.9933 - val_loss: 2.2051 - val_accuracy: 0.6682
Epoch 29/500
802/802 - 100s - loss: 0.0140 - accuracy: 0.9949 - val_loss: 2.6376 - val_accuracy: 0.6660
Epoch 30/500
802/802 - 101s - loss: 0.0175 - accuracy: 0.9937 - val_loss: 2.5430 - val_accuracy: 0.6648
Epoch 31/500
802/802 - 101s - loss: 0.0204 - accuracy: 0.9940 - val_loss: 2.2195 - val_accuracy: 0.6692
Epoch 32/500
802/802 - 100s - loss: 0.0236 - accuracy: 0.9912 - val_loss: 1.9808 - val_accuracy: 0.6632
Epoch 33/500
802/802 - 105s - loss: 0.0139 - accuracy: 0.9954 - val_loss: 2.5862 - val_accuracy: 0.6632
========================================
save_weights
h5_weights/SX.po/onehot_cnn_one_branch.h5
========================================

end time >>> Mon Oct  4 10:07:24 2021

end time >>> Mon Oct  4 10:07:24 2021

end time >>> Mon Oct  4 10:07:24 2021

end time >>> Mon Oct  4 10:07:24 2021

end time >>> Mon Oct  4 10:07:24 2021












args.model = onehot_cnn_one_branch
time used = 4128.410029172897


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 10:07:25 2021

begin time >>> Mon Oct  4 10:07:25 2021

begin time >>> Mon Oct  4 10:07:25 2021

begin time >>> Mon Oct  4 10:07:25 2021

begin time >>> Mon Oct  4 10:07:25 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_cnn_two_branch
args.type = train
args.name = SX.po
args.length = 10001
===========================


-> h5_weights/SX.po folder already exist. pass.
-> result/SX.po/onehot_cnn_one_branch folder already exist. pass.
-> result/SX.po/onehot_cnn_two_branch folder already exist. pass.
-> result/SX.po/onehot_embedding_dense folder already exist. pass.
-> result/SX.po/onehot_dense folder already exist. pass.
-> result/SX.po/onehot_resnet18 folder already exist. pass.
-> result/SX.po/onehot_resnet34 folder already exist. pass.
-> result/SX.po/embedding_cnn_one_branch folder already exist. pass.
-> result/SX.po/embedding_cnn_two_branch folder already exist. pass.
-> result/SX.po/embedding_dense folder already exist. pass.
-> result/SX.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/SX.po/onehot_embedding_cnn_two_branch folder already exist. pass.
Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001, 5, 1) 0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001, 5, 1) 0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 2501, 5, 64)  1600        input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 2501, 5, 64)  1600        input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 2501, 5, 64)  256         conv2d[0][0]                     
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 2501, 5, 64)  256         conv2d_6[0][0]                   
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 626, 5, 64)   98368       batch_normalization[0][0]        
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 626, 5, 64)   98368       batch_normalization_8[0][0]      
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 626, 5, 64)   256         conv2d_1[0][0]                   
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 626, 5, 64)   256         conv2d_7[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 157, 5, 64)   98368       batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 157, 5, 64)   98368       batch_normalization_9[0][0]      
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 157, 5, 64)   256         conv2d_2[0][0]                   
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 157, 5, 64)   256         conv2d_8[0][0]                   
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 78, 5, 64)    0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 78, 5, 64)    0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 78, 5, 64)    256         max_pooling2d[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 78, 5, 64)    256         max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 20, 5, 128)   196736      batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 20, 5, 128)   196736      batch_normalization_11[0][0]     
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 20, 5, 128)   512         conv2d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 20, 5, 128)   512         conv2d_9[0][0]                   
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 5, 5, 128)    393344      batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 5, 5, 128)    393344      batch_normalization_12[0][0]     
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 5, 5, 128)    512         conv2d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 5, 5, 128)    512         conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 2, 5, 128)    393344      batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 2, 5, 128)    393344      batch_normalization_13[0][0]     
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 2, 5, 128)    512         conv2d_5[0][0]                   
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 2, 5, 128)    512         conv2d_11[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 1, 5, 128)    0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 1, 5, 128)    0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
flatten (Flatten)               (None, 640)          0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 640)          0           max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 640)          2560        flatten[0][0]                    
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 640)          2560        flatten_1[0][0]                  
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          328192      batch_normalization_7[0][0]      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          328192      batch_normalization_15[0][0]     
__________________________________________________________________________________________________
dropout (Dropout)               (None, 512)          0           dense[0][0]                      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 1024)         0           dropout[0][0]                    
                                                                 dropout_1[0][0]                  
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          524800      concatenate[0][0]                
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 512)          262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 2)            1026        dense_3[0][0]                    
==================================================================================================
Total params: 3,818,626
Trainable params: 3,813,506
Non-trainable params: 5,120
__________________________________________________________________________________________________
Found 25666 images belonging to 2 classes.
Found 25666 images belonging to 2 classes.
Epoch 1/500
Found 3170 images belonging to 2 classes.
Found 3170 images belonging to 2 classes.
1535/1535 - 1739s - loss: 0.7047 - accuracy: 0.5682 - val_loss: 0.6836 - val_accuracy: 0.5960
Epoch 2/500
1535/1535 - 226s - loss: 0.5596 - accuracy: 0.7122 - val_loss: 0.8019 - val_accuracy: 0.5886
Epoch 3/500
1535/1535 - 226s - loss: 0.4038 - accuracy: 0.8180 - val_loss: 0.7148 - val_accuracy: 0.6869
Epoch 4/500
1535/1535 - 226s - loss: 0.2331 - accuracy: 0.9077 - val_loss: 1.1047 - val_accuracy: 0.6692
Epoch 5/500
1535/1535 - 220s - loss: 0.1508 - accuracy: 0.9451 - val_loss: 1.2169 - val_accuracy: 0.6522
Epoch 6/500
1535/1535 - 219s - loss: 0.1055 - accuracy: 0.9644 - val_loss: 1.4538 - val_accuracy: 0.6771
Epoch 7/500
1535/1535 - 220s - loss: 0.0846 - accuracy: 0.9725 - val_loss: 2.1171 - val_accuracy: 0.6366
Epoch 8/500
1535/1535 - 224s - loss: 0.0685 - accuracy: 0.9784 - val_loss: 1.3831 - val_accuracy: 0.6695
Epoch 9/500
1535/1535 - 223s - loss: 0.0643 - accuracy: 0.9796 - val_loss: 1.6401 - val_accuracy: 0.6966
Epoch 10/500
1535/1535 - 225s - loss: 0.0574 - accuracy: 0.9827 - val_loss: 1.7484 - val_accuracy: 0.6669
Epoch 11/500
1535/1535 - 224s - loss: 0.0513 - accuracy: 0.9843 - val_loss: 1.9677 - val_accuracy: 0.6262
Epoch 12/500
1535/1535 - 224s - loss: 0.0418 - accuracy: 0.9868 - val_loss: 1.4337 - val_accuracy: 0.6776
Epoch 13/500
1535/1535 - 223s - loss: 0.0399 - accuracy: 0.9872 - val_loss: 1.8974 - val_accuracy: 0.6643
Epoch 14/500
1535/1535 - 223s - loss: 0.0351 - accuracy: 0.9885 - val_loss: 2.3069 - val_accuracy: 0.6608
Epoch 15/500
1535/1535 - 224s - loss: 0.0359 - accuracy: 0.9888 - val_loss: 1.9069 - val_accuracy: 0.6588
Epoch 16/500
1535/1535 - 225s - loss: 0.0364 - accuracy: 0.9886 - val_loss: 1.7557 - val_accuracy: 0.6505
Epoch 17/500
1535/1535 - 225s - loss: 0.0350 - accuracy: 0.9886 - val_loss: 2.0067 - val_accuracy: 0.6732
Epoch 18/500
1535/1535 - 226s - loss: 0.0267 - accuracy: 0.9912 - val_loss: 2.0077 - val_accuracy: 0.6826
Epoch 19/500
1535/1535 - 225s - loss: 0.0281 - accuracy: 0.9909 - val_loss: 2.3896 - val_accuracy: 0.6593
========================================
save_weights
h5_weights/SX.po/onehot_cnn_two_branch.h5
========================================

end time >>> Mon Oct  4 11:43:51 2021

end time >>> Mon Oct  4 11:43:51 2021

end time >>> Mon Oct  4 11:43:51 2021

end time >>> Mon Oct  4 11:43:51 2021

end time >>> Mon Oct  4 11:43:51 2021












args.model = onehot_cnn_two_branch
time used = 5785.966707706451


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 11:43:53 2021

begin time >>> Mon Oct  4 11:43:53 2021

begin time >>> Mon Oct  4 11:43:53 2021

begin time >>> Mon Oct  4 11:43:53 2021

begin time >>> Mon Oct  4 11:43:53 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_dense
args.type = train
args.name = SX.po
args.length = 10001
===========================


-> h5_weights/SX.po folder already exist. pass.
-> result/SX.po/onehot_cnn_one_branch folder already exist. pass.
-> result/SX.po/onehot_cnn_two_branch folder already exist. pass.
-> result/SX.po/onehot_embedding_dense folder already exist. pass.
-> result/SX.po/onehot_dense folder already exist. pass.
-> result/SX.po/onehot_resnet18 folder already exist. pass.
-> result/SX.po/onehot_resnet34 folder already exist. pass.
-> result/SX.po/embedding_cnn_one_branch folder already exist. pass.
-> result/SX.po/embedding_cnn_two_branch folder already exist. pass.
-> result/SX.po/embedding_dense folder already exist. pass.
-> result/SX.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/SX.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
SX.po
########################################

########################################
model_name
onehot_dense
########################################

Found 25666 images belonging to 2 classes.
Found 3170 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 100010)            0         
_________________________________________________________________
batch_normalization (BatchNo (None, 100010)            400040    
_________________________________________________________________
dense (Dense)                (None, 512)               51205632  
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_3 (Batch (None, 512)               2048      
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_3 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 52,402,858
Trainable params: 52,198,742
Non-trainable params: 204,116
_________________________________________________________________
Epoch 1/500
802/802 - 307s - loss: 0.7211 - accuracy: 0.5723 - val_loss: 0.7225 - val_accuracy: 0.5761
Epoch 2/500
802/802 - 72s - loss: 0.5886 - accuracy: 0.7010 - val_loss: 1.0340 - val_accuracy: 0.5562
Epoch 3/500
802/802 - 71s - loss: 0.4477 - accuracy: 0.7979 - val_loss: 1.3733 - val_accuracy: 0.5603
Epoch 4/500
802/802 - 72s - loss: 0.3417 - accuracy: 0.8569 - val_loss: 1.7176 - val_accuracy: 0.5707
Epoch 5/500
802/802 - 72s - loss: 0.2651 - accuracy: 0.8940 - val_loss: 1.9275 - val_accuracy: 0.5792
Epoch 6/500
802/802 - 72s - loss: 0.2106 - accuracy: 0.9155 - val_loss: 2.0551 - val_accuracy: 0.5909
Epoch 7/500
802/802 - 71s - loss: 0.1716 - accuracy: 0.9315 - val_loss: 2.0853 - val_accuracy: 0.5931
Epoch 8/500
802/802 - 71s - loss: 0.1435 - accuracy: 0.9448 - val_loss: 2.1974 - val_accuracy: 0.6042
Epoch 9/500
802/802 - 72s - loss: 0.1269 - accuracy: 0.9508 - val_loss: 2.1874 - val_accuracy: 0.6098
Epoch 10/500
802/802 - 71s - loss: 0.1102 - accuracy: 0.9579 - val_loss: 2.1239 - val_accuracy: 0.6244
Epoch 11/500
802/802 - 72s - loss: 0.0969 - accuracy: 0.9629 - val_loss: 2.1724 - val_accuracy: 0.6256
Epoch 12/500
802/802 - 71s - loss: 0.0868 - accuracy: 0.9678 - val_loss: 2.1381 - val_accuracy: 0.6278
Epoch 13/500
802/802 - 71s - loss: 0.0772 - accuracy: 0.9708 - val_loss: 2.2200 - val_accuracy: 0.6187
Epoch 14/500
802/802 - 72s - loss: 0.0727 - accuracy: 0.9728 - val_loss: 2.1759 - val_accuracy: 0.6291
Epoch 15/500
802/802 - 71s - loss: 0.0681 - accuracy: 0.9742 - val_loss: 2.1632 - val_accuracy: 0.6187
Epoch 16/500
802/802 - 73s - loss: 0.0625 - accuracy: 0.9767 - val_loss: 2.1603 - val_accuracy: 0.6294
Epoch 17/500
802/802 - 72s - loss: 0.0645 - accuracy: 0.9771 - val_loss: 2.0733 - val_accuracy: 0.6354
Epoch 18/500
802/802 - 72s - loss: 0.0561 - accuracy: 0.9799 - val_loss: 2.0716 - val_accuracy: 0.6370
Epoch 19/500
802/802 - 72s - loss: 0.0551 - accuracy: 0.9807 - val_loss: 2.0498 - val_accuracy: 0.6379
Epoch 20/500
802/802 - 72s - loss: 0.0501 - accuracy: 0.9822 - val_loss: 2.0494 - val_accuracy: 0.6379
Epoch 21/500
802/802 - 72s - loss: 0.0525 - accuracy: 0.9814 - val_loss: 1.9881 - val_accuracy: 0.6449
Epoch 22/500
802/802 - 71s - loss: 0.0491 - accuracy: 0.9830 - val_loss: 1.9843 - val_accuracy: 0.6455
Epoch 23/500
802/802 - 71s - loss: 0.0426 - accuracy: 0.9852 - val_loss: 2.0413 - val_accuracy: 0.6392
Epoch 24/500
802/802 - 71s - loss: 0.0416 - accuracy: 0.9850 - val_loss: 1.9516 - val_accuracy: 0.6480
Epoch 25/500
802/802 - 71s - loss: 0.0388 - accuracy: 0.9861 - val_loss: 2.0224 - val_accuracy: 0.6468
Epoch 26/500
802/802 - 71s - loss: 0.0354 - accuracy: 0.9872 - val_loss: 1.9699 - val_accuracy: 0.6534
Epoch 27/500
802/802 - 72s - loss: 0.0372 - accuracy: 0.9873 - val_loss: 1.9553 - val_accuracy: 0.6550
Epoch 28/500
802/802 - 72s - loss: 0.0422 - accuracy: 0.9857 - val_loss: 1.9226 - val_accuracy: 0.6635
Epoch 29/500
802/802 - 72s - loss: 0.0336 - accuracy: 0.9884 - val_loss: 1.9560 - val_accuracy: 0.6604
Epoch 30/500
802/802 - 72s - loss: 0.0333 - accuracy: 0.9882 - val_loss: 2.0106 - val_accuracy: 0.6525
Epoch 31/500
802/802 - 72s - loss: 0.0334 - accuracy: 0.9884 - val_loss: 1.9560 - val_accuracy: 0.6591
Epoch 32/500
802/802 - 72s - loss: 0.0266 - accuracy: 0.9913 - val_loss: 2.0126 - val_accuracy: 0.6679
Epoch 33/500
802/802 - 72s - loss: 0.0319 - accuracy: 0.9893 - val_loss: 2.0459 - val_accuracy: 0.6695
Epoch 34/500
802/802 - 72s - loss: 0.0319 - accuracy: 0.9889 - val_loss: 2.0518 - val_accuracy: 0.6616
Epoch 35/500
802/802 - 71s - loss: 0.0279 - accuracy: 0.9906 - val_loss: 2.0456 - val_accuracy: 0.6689
Epoch 36/500
802/802 - 71s - loss: 0.0257 - accuracy: 0.9916 - val_loss: 2.0831 - val_accuracy: 0.6648
Epoch 37/500
802/802 - 71s - loss: 0.0268 - accuracy: 0.9917 - val_loss: 1.9379 - val_accuracy: 0.6758
Epoch 38/500
802/802 - 71s - loss: 0.0265 - accuracy: 0.9917 - val_loss: 1.9811 - val_accuracy: 0.6695
Epoch 39/500
802/802 - 71s - loss: 0.0260 - accuracy: 0.9920 - val_loss: 2.0235 - val_accuracy: 0.6638
Epoch 40/500
802/802 - 70s - loss: 0.0220 - accuracy: 0.9933 - val_loss: 1.9002 - val_accuracy: 0.6755
Epoch 41/500
802/802 - 71s - loss: 0.0216 - accuracy: 0.9930 - val_loss: 1.9542 - val_accuracy: 0.6733
Epoch 42/500
802/802 - 72s - loss: 0.0236 - accuracy: 0.9925 - val_loss: 1.9618 - val_accuracy: 0.6730
Epoch 43/500
802/802 - 71s - loss: 0.0230 - accuracy: 0.9928 - val_loss: 1.9834 - val_accuracy: 0.6657
Epoch 44/500
802/802 - 71s - loss: 0.0212 - accuracy: 0.9926 - val_loss: 2.0024 - val_accuracy: 0.6749
Epoch 45/500
802/802 - 71s - loss: 0.0215 - accuracy: 0.9928 - val_loss: 2.0306 - val_accuracy: 0.6761
Epoch 46/500
802/802 - 71s - loss: 0.0192 - accuracy: 0.9934 - val_loss: 2.0253 - val_accuracy: 0.6790
Epoch 47/500
802/802 - 71s - loss: 0.0195 - accuracy: 0.9937 - val_loss: 2.0536 - val_accuracy: 0.6742
Epoch 48/500
802/802 - 71s - loss: 0.0180 - accuracy: 0.9940 - val_loss: 2.0858 - val_accuracy: 0.6755
Epoch 49/500
802/802 - 71s - loss: 0.0175 - accuracy: 0.9948 - val_loss: 2.0773 - val_accuracy: 0.6755
Epoch 50/500
802/802 - 71s - loss: 0.0173 - accuracy: 0.9949 - val_loss: 2.1126 - val_accuracy: 0.6761
Epoch 51/500
802/802 - 71s - loss: 0.0200 - accuracy: 0.9940 - val_loss: 2.0912 - val_accuracy: 0.6812
Epoch 52/500
802/802 - 71s - loss: 0.0181 - accuracy: 0.9946 - val_loss: 1.9908 - val_accuracy: 0.6824
Epoch 53/500
802/802 - 71s - loss: 0.0179 - accuracy: 0.9942 - val_loss: 1.9812 - val_accuracy: 0.6815
Epoch 54/500
802/802 - 71s - loss: 0.0167 - accuracy: 0.9947 - val_loss: 2.0064 - val_accuracy: 0.6790
Epoch 55/500
802/802 - 71s - loss: 0.0161 - accuracy: 0.9955 - val_loss: 2.0096 - val_accuracy: 0.6780
Epoch 56/500
802/802 - 71s - loss: 0.0158 - accuracy: 0.9952 - val_loss: 1.9978 - val_accuracy: 0.6711
Epoch 57/500
802/802 - 71s - loss: 0.0138 - accuracy: 0.9951 - val_loss: 1.9746 - val_accuracy: 0.6717
Epoch 58/500
802/802 - 71s - loss: 0.0121 - accuracy: 0.9962 - val_loss: 2.0323 - val_accuracy: 0.6777
Epoch 59/500
802/802 - 71s - loss: 0.0143 - accuracy: 0.9949 - val_loss: 1.9717 - val_accuracy: 0.6783
Epoch 60/500
802/802 - 71s - loss: 0.0136 - accuracy: 0.9959 - val_loss: 1.9487 - val_accuracy: 0.6761
Epoch 61/500
802/802 - 71s - loss: 0.0122 - accuracy: 0.9967 - val_loss: 1.9282 - val_accuracy: 0.6755
Epoch 62/500
802/802 - 72s - loss: 0.0132 - accuracy: 0.9954 - val_loss: 2.0033 - val_accuracy: 0.6686
========================================
save_weights
h5_weights/SX.po/onehot_dense.h5
========================================

end time >>> Mon Oct  4 13:02:21 2021

end time >>> Mon Oct  4 13:02:21 2021

end time >>> Mon Oct  4 13:02:21 2021

end time >>> Mon Oct  4 13:02:21 2021

end time >>> Mon Oct  4 13:02:21 2021












args.model = onehot_dense
time used = 4708.506365537643


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 13:02:22 2021

begin time >>> Mon Oct  4 13:02:22 2021

begin time >>> Mon Oct  4 13:02:22 2021

begin time >>> Mon Oct  4 13:02:22 2021

begin time >>> Mon Oct  4 13:02:22 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_resnet18
args.type = train
args.name = SX.po
args.length = 10001
===========================


-> h5_weights/SX.po folder already exist. pass.
-> result/SX.po/onehot_cnn_one_branch folder already exist. pass.
-> result/SX.po/onehot_cnn_two_branch folder already exist. pass.
-> result/SX.po/onehot_embedding_dense folder already exist. pass.
-> result/SX.po/onehot_dense folder already exist. pass.
-> result/SX.po/onehot_resnet18 folder already exist. pass.
-> result/SX.po/onehot_resnet34 folder already exist. pass.
-> result/SX.po/embedding_cnn_one_branch folder already exist. pass.
-> result/SX.po/embedding_cnn_two_branch folder already exist. pass.
-> result/SX.po/embedding_dense folder already exist. pass.
-> result/SX.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/SX.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
SX.po
########################################

########################################
model_name
onehot_resnet18
########################################

Found 25666 images belonging to 2 classes.
Found 3170 images belonging to 2 classes.
Model: "res_net_type_i"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              multiple                  1600      
_________________________________________________________________
batch_normalization (BatchNo multiple                  256       
_________________________________________________________________
max_pooling2d (MaxPooling2D) multiple                  0         
_________________________________________________________________
sequential (Sequential)      (None, 313, 1, 64)        398912    
_________________________________________________________________
sequential_2 (Sequential)    (None, 20, 1, 64)         398912    
_________________________________________________________________
sequential_4 (Sequential)    (None, 2, 1, 64)          398912    
_________________________________________________________________
sequential_6 (Sequential)    (None, 1, 1, 64)          398912    
_________________________________________________________________
global_average_pooling2d (Gl multiple                  0         
_________________________________________________________________
dense (Dense)                multiple                  130       
=================================================================
Total params: 1,597,634
Trainable params: 1,594,946
Non-trainable params: 2,688
_________________________________________________________________
Epoch 1/500
802/802 - 94s - loss: 0.7697 - accuracy: 0.5025 - val_loss: 0.7294 - val_accuracy: 0.5000
Epoch 2/500
802/802 - 94s - loss: 0.7106 - accuracy: 0.5269 - val_loss: 0.7146 - val_accuracy: 0.5095
Epoch 3/500
802/802 - 94s - loss: 0.6878 - accuracy: 0.5588 - val_loss: 0.7067 - val_accuracy: 0.5265
Epoch 4/500
802/802 - 95s - loss: 0.6693 - accuracy: 0.5917 - val_loss: 0.6958 - val_accuracy: 0.5587
Epoch 5/500
802/802 - 95s - loss: 0.6621 - accuracy: 0.6054 - val_loss: 0.6884 - val_accuracy: 0.5701
Epoch 6/500
802/802 - 95s - loss: 0.6185 - accuracy: 0.6548 - val_loss: 0.6734 - val_accuracy: 0.6039
Epoch 7/500
802/802 - 95s - loss: 0.5718 - accuracy: 0.7024 - val_loss: 0.6889 - val_accuracy: 0.6045
Epoch 8/500
802/802 - 94s - loss: 0.5249 - accuracy: 0.7386 - val_loss: 0.6945 - val_accuracy: 0.6045
Epoch 9/500
802/802 - 95s - loss: 0.4621 - accuracy: 0.7837 - val_loss: 0.7508 - val_accuracy: 0.6225
Epoch 10/500
802/802 - 94s - loss: 0.3861 - accuracy: 0.8314 - val_loss: 0.7665 - val_accuracy: 0.6503
Epoch 11/500
802/802 - 94s - loss: 0.2507 - accuracy: 0.8991 - val_loss: 0.9268 - val_accuracy: 0.6241
Epoch 12/500
802/802 - 94s - loss: 0.2569 - accuracy: 0.8962 - val_loss: 0.8930 - val_accuracy: 0.6310
Epoch 13/500
802/802 - 94s - loss: 0.1922 - accuracy: 0.9251 - val_loss: 1.0219 - val_accuracy: 0.6408
Epoch 14/500
802/802 - 94s - loss: 0.1181 - accuracy: 0.9585 - val_loss: 1.1819 - val_accuracy: 0.6408
Epoch 15/500
802/802 - 94s - loss: 0.1091 - accuracy: 0.9594 - val_loss: 1.2251 - val_accuracy: 0.6408
Epoch 16/500
802/802 - 95s - loss: 0.1313 - accuracy: 0.9495 - val_loss: 1.2583 - val_accuracy: 0.6379
Epoch 17/500
802/802 - 94s - loss: 0.1257 - accuracy: 0.9534 - val_loss: 1.2687 - val_accuracy: 0.6332
Epoch 18/500
802/802 - 94s - loss: 0.1122 - accuracy: 0.9575 - val_loss: 1.1773 - val_accuracy: 0.6518
Epoch 19/500
802/802 - 93s - loss: 0.0742 - accuracy: 0.9736 - val_loss: 1.1932 - val_accuracy: 0.6604
Epoch 20/500
802/802 - 94s - loss: 0.0869 - accuracy: 0.9698 - val_loss: 1.3036 - val_accuracy: 0.6528
Epoch 21/500
802/802 - 95s - loss: 0.1153 - accuracy: 0.9580 - val_loss: 1.2997 - val_accuracy: 0.6534
Epoch 22/500
802/802 - 94s - loss: 0.1053 - accuracy: 0.9606 - val_loss: 1.2018 - val_accuracy: 0.6518
Epoch 23/500
802/802 - 94s - loss: 0.0468 - accuracy: 0.9850 - val_loss: 1.2847 - val_accuracy: 0.6616
Epoch 24/500
802/802 - 93s - loss: 0.0831 - accuracy: 0.9704 - val_loss: 1.2826 - val_accuracy: 0.6503
Epoch 25/500
802/802 - 94s - loss: 0.0512 - accuracy: 0.9822 - val_loss: 1.3384 - val_accuracy: 0.6686
Epoch 26/500
802/802 - 94s - loss: 0.0587 - accuracy: 0.9796 - val_loss: 1.3268 - val_accuracy: 0.6610
Epoch 27/500
802/802 - 95s - loss: 0.0589 - accuracy: 0.9794 - val_loss: 1.2908 - val_accuracy: 0.6588
Epoch 28/500
802/802 - 94s - loss: 0.0530 - accuracy: 0.9815 - val_loss: 1.3095 - val_accuracy: 0.6607
Epoch 29/500
802/802 - 94s - loss: 0.0585 - accuracy: 0.9795 - val_loss: 1.2964 - val_accuracy: 0.6553
Epoch 30/500
802/802 - 94s - loss: 0.0470 - accuracy: 0.9835 - val_loss: 1.3238 - val_accuracy: 0.6566
Epoch 31/500
802/802 - 94s - loss: 0.0480 - accuracy: 0.9836 - val_loss: 1.2916 - val_accuracy: 0.6600
Epoch 32/500
802/802 - 94s - loss: 0.0642 - accuracy: 0.9780 - val_loss: 1.3414 - val_accuracy: 0.6613
Epoch 33/500
802/802 - 94s - loss: 0.0828 - accuracy: 0.9700 - val_loss: 1.2314 - val_accuracy: 0.6806
Epoch 34/500
802/802 - 94s - loss: 0.0276 - accuracy: 0.9918 - val_loss: 1.3003 - val_accuracy: 0.6783
Epoch 35/500
802/802 - 94s - loss: 0.0139 - accuracy: 0.9963 - val_loss: 1.4278 - val_accuracy: 0.6717
Epoch 36/500
802/802 - 94s - loss: 0.0131 - accuracy: 0.9964 - val_loss: 1.4692 - val_accuracy: 0.6692
Epoch 37/500
802/802 - 93s - loss: 0.0285 - accuracy: 0.9910 - val_loss: 1.4592 - val_accuracy: 0.6607
Epoch 38/500
802/802 - 94s - loss: 0.0434 - accuracy: 0.9852 - val_loss: 1.4057 - val_accuracy: 0.6544
Epoch 39/500
802/802 - 93s - loss: 0.0480 - accuracy: 0.9836 - val_loss: 1.4167 - val_accuracy: 0.6585
Epoch 40/500
802/802 - 94s - loss: 0.0359 - accuracy: 0.9878 - val_loss: 1.4467 - val_accuracy: 0.6629
Epoch 41/500
802/802 - 95s - loss: 0.0351 - accuracy: 0.9887 - val_loss: 1.4202 - val_accuracy: 0.6638
Epoch 42/500
802/802 - 95s - loss: 0.0361 - accuracy: 0.9888 - val_loss: 1.4173 - val_accuracy: 0.6493
Epoch 43/500
802/802 - 95s - loss: 0.0375 - accuracy: 0.9874 - val_loss: 1.4512 - val_accuracy: 0.6449
========================================
save_weights
h5_weights/SX.po/onehot_resnet18.h5
========================================

end time >>> Mon Oct  4 14:10:36 2021

end time >>> Mon Oct  4 14:10:36 2021

end time >>> Mon Oct  4 14:10:36 2021

end time >>> Mon Oct  4 14:10:36 2021

end time >>> Mon Oct  4 14:10:36 2021












args.model = onehot_resnet18
time used = 4093.8547043800354


