************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sat Oct  2 23:03:53 2021

begin time >>> Sat Oct  2 23:03:53 2021

begin time >>> Sat Oct  2 23:03:53 2021

begin time >>> Sat Oct  2 23:03:53 2021

begin time >>> Sat Oct  2 23:03:53 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_dense
args.type = train
args.name = BL1.po
args.length = 10001
===========================


-> h5_weights/BL1.po folder already exist. pass.
-> result/BL1.po/onehot_cnn_one_branch folder already exist. pass.
-> result/BL1.po/onehot_cnn_two_branch folder already exist. pass.
-> result/BL1.po/onehot_embedding_dense folder already exist. pass.
-> result/BL1.po/onehot_dense folder already exist. pass.
-> result/BL1.po/onehot_resnet18 folder already exist. pass.
-> result/BL1.po/onehot_resnet34 folder already exist. pass.
-> result/BL1.po/embedding_cnn_one_branch folder already exist. pass.
-> result/BL1.po/embedding_cnn_two_branch folder already exist. pass.
-> result/BL1.po/embedding_dense folder already exist. pass.
-> result/BL1.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/BL1.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
BL1.po
########################################

########################################
model_name
embedding_dense
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 626, 64)      409664      concatenate[0][0]                
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, 38, 64)       0           conv1d[0][0]                     
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 38, 64)       256         max_pooling1d[0][0]              
__________________________________________________________________________________________________
flatten (Flatten)               (None, 2432)         0           batch_normalization[0][0]        
__________________________________________________________________________________________________
dropout (Dropout)               (None, 2432)         0           flatten[0][0]                    
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          1245696     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation (Activation)         (None, 512)          0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation[0][0]                 
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 512)          0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_1[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 512)          2048        dense_2[0][0]                    
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 512)          0           activation_2[0][0]               
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            513         dropout_3[0][0]                  
==================================================================================================
Total params: 3,006,985
Trainable params: 3,003,785
Non-trainable params: 3,200
__________________________________________________________________________________________________
Epoch 1/500
155/155 - 20s - loss: 0.8662 - accuracy: 0.4998 - val_loss: 0.6921 - val_accuracy: 0.5237
Epoch 2/500
155/155 - 20s - loss: 0.8713 - accuracy: 0.4976 - val_loss: 0.6961 - val_accuracy: 0.4943
Epoch 3/500
155/155 - 20s - loss: 0.8631 - accuracy: 0.5000 - val_loss: 0.7012 - val_accuracy: 0.4894
Epoch 4/500
155/155 - 20s - loss: 0.8761 - accuracy: 0.4964 - val_loss: 0.7033 - val_accuracy: 0.4763
Epoch 5/500
155/155 - 20s - loss: 0.8655 - accuracy: 0.5036 - val_loss: 0.7032 - val_accuracy: 0.4763
Epoch 6/500
155/155 - 20s - loss: 0.8516 - accuracy: 0.5105 - val_loss: 0.7036 - val_accuracy: 0.4746
Epoch 7/500
155/155 - 20s - loss: 0.8631 - accuracy: 0.5000 - val_loss: 0.7037 - val_accuracy: 0.4746
Epoch 8/500
155/155 - 20s - loss: 0.8288 - accuracy: 0.5107 - val_loss: 0.7024 - val_accuracy: 0.4763
Epoch 9/500
155/155 - 20s - loss: 0.8558 - accuracy: 0.4982 - val_loss: 0.7015 - val_accuracy: 0.4795
Epoch 10/500
155/155 - 20s - loss: 0.8504 - accuracy: 0.5132 - val_loss: 0.7017 - val_accuracy: 0.4845
Epoch 11/500
155/155 - 20s - loss: 0.8529 - accuracy: 0.4964 - val_loss: 0.7015 - val_accuracy: 0.4746
Epoch 12/500
155/155 - 20s - loss: 0.8367 - accuracy: 0.5113 - val_loss: 0.7013 - val_accuracy: 0.4812
Epoch 13/500
155/155 - 20s - loss: 0.8511 - accuracy: 0.5030 - val_loss: 0.7005 - val_accuracy: 0.4795
Epoch 14/500
155/155 - 20s - loss: 0.8308 - accuracy: 0.5156 - val_loss: 0.7004 - val_accuracy: 0.4845
Epoch 15/500
155/155 - 20s - loss: 0.8193 - accuracy: 0.5237 - val_loss: 0.6998 - val_accuracy: 0.4861
Epoch 16/500
155/155 - 20s - loss: 0.8379 - accuracy: 0.5081 - val_loss: 0.6995 - val_accuracy: 0.4845
Epoch 17/500
155/155 - 20s - loss: 0.8340 - accuracy: 0.5127 - val_loss: 0.6988 - val_accuracy: 0.4861
Epoch 18/500
155/155 - 20s - loss: 0.8168 - accuracy: 0.5283 - val_loss: 0.6990 - val_accuracy: 0.4877
Epoch 19/500
155/155 - 20s - loss: 0.8402 - accuracy: 0.5038 - val_loss: 0.6987 - val_accuracy: 0.4926
Epoch 20/500
155/155 - 20s - loss: 0.8256 - accuracy: 0.5227 - val_loss: 0.6981 - val_accuracy: 0.4910
Epoch 21/500
155/155 - 20s - loss: 0.8228 - accuracy: 0.5233 - val_loss: 0.6991 - val_accuracy: 0.4910
========================================
save_weights
h5_weights/BL1.po/embedding_dense.h5
========================================

end time >>> Sat Oct  2 23:11:04 2021

end time >>> Sat Oct  2 23:11:04 2021

end time >>> Sat Oct  2 23:11:04 2021

end time >>> Sat Oct  2 23:11:04 2021

end time >>> Sat Oct  2 23:11:04 2021












args.model = embedding_dense
time used = 430.72417521476746


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sat Oct  2 23:11:05 2021

begin time >>> Sat Oct  2 23:11:05 2021

begin time >>> Sat Oct  2 23:11:05 2021

begin time >>> Sat Oct  2 23:11:05 2021

begin time >>> Sat Oct  2 23:11:05 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_cnn_one_branch
args.type = train
args.name = BL1.po
args.length = 10001
===========================


-> h5_weights/BL1.po folder already exist. pass.
-> result/BL1.po/onehot_cnn_one_branch folder already exist. pass.
-> result/BL1.po/onehot_cnn_two_branch folder already exist. pass.
-> result/BL1.po/onehot_embedding_dense folder already exist. pass.
-> result/BL1.po/onehot_dense folder already exist. pass.
-> result/BL1.po/onehot_resnet18 folder already exist. pass.
-> result/BL1.po/onehot_resnet34 folder already exist. pass.
-> result/BL1.po/embedding_cnn_one_branch folder already exist. pass.
-> result/BL1.po/embedding_cnn_two_branch folder already exist. pass.
-> result/BL1.po/embedding_dense folder already exist. pass.
-> result/BL1.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/BL1.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
BL1.po
########################################

########################################
model_name
embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
155/155 - 21s - loss: 0.8788 - accuracy: 0.4945 - val_loss: 0.6966 - val_accuracy: 0.5254
Epoch 2/500
155/155 - 21s - loss: 0.8585 - accuracy: 0.5083 - val_loss: 0.7050 - val_accuracy: 0.5155
Epoch 3/500
155/155 - 21s - loss: 0.8644 - accuracy: 0.4962 - val_loss: 0.7064 - val_accuracy: 0.5155
Epoch 4/500
155/155 - 21s - loss: 0.8428 - accuracy: 0.5154 - val_loss: 0.7067 - val_accuracy: 0.5286
Epoch 5/500
155/155 - 21s - loss: 0.8419 - accuracy: 0.5188 - val_loss: 0.7061 - val_accuracy: 0.5237
Epoch 6/500
155/155 - 21s - loss: 0.8475 - accuracy: 0.5109 - val_loss: 0.7051 - val_accuracy: 0.5286
Epoch 7/500
155/155 - 21s - loss: 0.8304 - accuracy: 0.5130 - val_loss: 0.7042 - val_accuracy: 0.5352
Epoch 8/500
155/155 - 21s - loss: 0.8013 - accuracy: 0.5386 - val_loss: 0.7029 - val_accuracy: 0.5336
Epoch 9/500
155/155 - 21s - loss: 0.8297 - accuracy: 0.5192 - val_loss: 0.7022 - val_accuracy: 0.5319
Epoch 10/500
155/155 - 21s - loss: 0.8199 - accuracy: 0.5289 - val_loss: 0.7018 - val_accuracy: 0.5368
Epoch 11/500
155/155 - 21s - loss: 0.8182 - accuracy: 0.5210 - val_loss: 0.7016 - val_accuracy: 0.5352
Epoch 12/500
155/155 - 21s - loss: 0.8088 - accuracy: 0.5291 - val_loss: 0.7015 - val_accuracy: 0.5368
Epoch 13/500
155/155 - 21s - loss: 0.8177 - accuracy: 0.5330 - val_loss: 0.6996 - val_accuracy: 0.5417
Epoch 14/500
155/155 - 21s - loss: 0.7867 - accuracy: 0.5467 - val_loss: 0.6994 - val_accuracy: 0.5417
Epoch 15/500
155/155 - 21s - loss: 0.7901 - accuracy: 0.5405 - val_loss: 0.6978 - val_accuracy: 0.5450
Epoch 16/500
155/155 - 21s - loss: 0.7940 - accuracy: 0.5368 - val_loss: 0.6974 - val_accuracy: 0.5483
Epoch 17/500
155/155 - 21s - loss: 0.7737 - accuracy: 0.5492 - val_loss: 0.6966 - val_accuracy: 0.5434
Epoch 18/500
155/155 - 21s - loss: 0.7762 - accuracy: 0.5498 - val_loss: 0.6953 - val_accuracy: 0.5417
Epoch 19/500
155/155 - 21s - loss: 0.7803 - accuracy: 0.5512 - val_loss: 0.6943 - val_accuracy: 0.5466
Epoch 20/500
155/155 - 21s - loss: 0.7683 - accuracy: 0.5629 - val_loss: 0.6939 - val_accuracy: 0.5483
Epoch 21/500
155/155 - 21s - loss: 0.7774 - accuracy: 0.5526 - val_loss: 0.6932 - val_accuracy: 0.5434
Epoch 22/500
155/155 - 21s - loss: 0.7571 - accuracy: 0.5668 - val_loss: 0.6926 - val_accuracy: 0.5483
Epoch 23/500
155/155 - 21s - loss: 0.7573 - accuracy: 0.5660 - val_loss: 0.6916 - val_accuracy: 0.5565
Epoch 24/500
155/155 - 21s - loss: 0.7608 - accuracy: 0.5579 - val_loss: 0.6912 - val_accuracy: 0.5548
Epoch 25/500
155/155 - 21s - loss: 0.7514 - accuracy: 0.5658 - val_loss: 0.6911 - val_accuracy: 0.5499
Epoch 26/500
155/155 - 21s - loss: 0.7380 - accuracy: 0.5832 - val_loss: 0.6901 - val_accuracy: 0.5597
Epoch 27/500
155/155 - 21s - loss: 0.7445 - accuracy: 0.5728 - val_loss: 0.6895 - val_accuracy: 0.5597
Epoch 28/500
155/155 - 21s - loss: 0.7397 - accuracy: 0.5741 - val_loss: 0.6888 - val_accuracy: 0.5581
Epoch 29/500
155/155 - 21s - loss: 0.7373 - accuracy: 0.5797 - val_loss: 0.6884 - val_accuracy: 0.5581
Epoch 30/500
155/155 - 21s - loss: 0.7240 - accuracy: 0.5955 - val_loss: 0.6882 - val_accuracy: 0.5565
Epoch 31/500
155/155 - 21s - loss: 0.7277 - accuracy: 0.5864 - val_loss: 0.6876 - val_accuracy: 0.5565
Epoch 32/500
155/155 - 21s - loss: 0.7221 - accuracy: 0.5844 - val_loss: 0.6871 - val_accuracy: 0.5597
Epoch 33/500
155/155 - 21s - loss: 0.7187 - accuracy: 0.5919 - val_loss: 0.6867 - val_accuracy: 0.5646
Epoch 34/500
155/155 - 21s - loss: 0.7086 - accuracy: 0.6070 - val_loss: 0.6863 - val_accuracy: 0.5630
Epoch 35/500
155/155 - 21s - loss: 0.7209 - accuracy: 0.5890 - val_loss: 0.6857 - val_accuracy: 0.5630
Epoch 36/500
155/155 - 21s - loss: 0.6984 - accuracy: 0.6087 - val_loss: 0.6854 - val_accuracy: 0.5646
Epoch 37/500
155/155 - 21s - loss: 0.6871 - accuracy: 0.6222 - val_loss: 0.6851 - val_accuracy: 0.5679
Epoch 38/500
155/155 - 21s - loss: 0.7107 - accuracy: 0.6034 - val_loss: 0.6850 - val_accuracy: 0.5745
Epoch 39/500
155/155 - 21s - loss: 0.6886 - accuracy: 0.6200 - val_loss: 0.6841 - val_accuracy: 0.5696
Epoch 40/500
155/155 - 21s - loss: 0.7015 - accuracy: 0.6060 - val_loss: 0.6837 - val_accuracy: 0.5745
Epoch 41/500
155/155 - 21s - loss: 0.6784 - accuracy: 0.6226 - val_loss: 0.6833 - val_accuracy: 0.5712
Epoch 42/500
155/155 - 21s - loss: 0.6873 - accuracy: 0.6099 - val_loss: 0.6829 - val_accuracy: 0.5745
Epoch 43/500
155/155 - 21s - loss: 0.6905 - accuracy: 0.6261 - val_loss: 0.6822 - val_accuracy: 0.5696
Epoch 44/500
155/155 - 21s - loss: 0.6740 - accuracy: 0.6259 - val_loss: 0.6815 - val_accuracy: 0.5777
Epoch 45/500
155/155 - 21s - loss: 0.6761 - accuracy: 0.6257 - val_loss: 0.6815 - val_accuracy: 0.5794
Epoch 46/500
155/155 - 21s - loss: 0.6633 - accuracy: 0.6253 - val_loss: 0.6812 - val_accuracy: 0.5761
Epoch 47/500
155/155 - 21s - loss: 0.6726 - accuracy: 0.6287 - val_loss: 0.6811 - val_accuracy: 0.5761
Epoch 48/500
155/155 - 21s - loss: 0.6490 - accuracy: 0.6461 - val_loss: 0.6809 - val_accuracy: 0.5745
Epoch 49/500
155/155 - 21s - loss: 0.6625 - accuracy: 0.6366 - val_loss: 0.6804 - val_accuracy: 0.5761
Epoch 50/500
155/155 - 21s - loss: 0.6415 - accuracy: 0.6441 - val_loss: 0.6797 - val_accuracy: 0.5794
Epoch 51/500
155/155 - 21s - loss: 0.6486 - accuracy: 0.6481 - val_loss: 0.6795 - val_accuracy: 0.5876
Epoch 52/500
155/155 - 21s - loss: 0.6388 - accuracy: 0.6578 - val_loss: 0.6794 - val_accuracy: 0.5843
Epoch 53/500
155/155 - 21s - loss: 0.6455 - accuracy: 0.6491 - val_loss: 0.6796 - val_accuracy: 0.5843
Epoch 54/500
155/155 - 21s - loss: 0.6294 - accuracy: 0.6611 - val_loss: 0.6787 - val_accuracy: 0.5843
Epoch 55/500
155/155 - 21s - loss: 0.6273 - accuracy: 0.6661 - val_loss: 0.6791 - val_accuracy: 0.5843
Epoch 56/500
155/155 - 21s - loss: 0.6344 - accuracy: 0.6582 - val_loss: 0.6791 - val_accuracy: 0.5957
Epoch 57/500
155/155 - 21s - loss: 0.6337 - accuracy: 0.6560 - val_loss: 0.6787 - val_accuracy: 0.5925
Epoch 58/500
155/155 - 21s - loss: 0.6153 - accuracy: 0.6758 - val_loss: 0.6780 - val_accuracy: 0.5810
Epoch 59/500
155/155 - 21s - loss: 0.6208 - accuracy: 0.6653 - val_loss: 0.6790 - val_accuracy: 0.5925
Epoch 60/500
155/155 - 21s - loss: 0.6062 - accuracy: 0.6764 - val_loss: 0.6777 - val_accuracy: 0.5925
Epoch 61/500
155/155 - 21s - loss: 0.5960 - accuracy: 0.6890 - val_loss: 0.6774 - val_accuracy: 0.5925
Epoch 62/500
155/155 - 21s - loss: 0.5930 - accuracy: 0.6862 - val_loss: 0.6765 - val_accuracy: 0.5957
Epoch 63/500
155/155 - 21s - loss: 0.5847 - accuracy: 0.7003 - val_loss: 0.6765 - val_accuracy: 0.5957
Epoch 64/500
155/155 - 21s - loss: 0.5779 - accuracy: 0.7021 - val_loss: 0.6763 - val_accuracy: 0.5876
Epoch 65/500
155/155 - 21s - loss: 0.5786 - accuracy: 0.7015 - val_loss: 0.6762 - val_accuracy: 0.5876
Epoch 66/500
155/155 - 21s - loss: 0.5735 - accuracy: 0.7054 - val_loss: 0.6764 - val_accuracy: 0.5957
Epoch 67/500
155/155 - 21s - loss: 0.5730 - accuracy: 0.7034 - val_loss: 0.6771 - val_accuracy: 0.5974
Epoch 68/500
155/155 - 21s - loss: 0.5562 - accuracy: 0.7113 - val_loss: 0.6768 - val_accuracy: 0.5990
Epoch 69/500
155/155 - 21s - loss: 0.5586 - accuracy: 0.7179 - val_loss: 0.6764 - val_accuracy: 0.5990
Epoch 70/500
155/155 - 21s - loss: 0.5597 - accuracy: 0.7106 - val_loss: 0.6761 - val_accuracy: 0.6056
Epoch 71/500
155/155 - 21s - loss: 0.5500 - accuracy: 0.7262 - val_loss: 0.6753 - val_accuracy: 0.5974
Epoch 72/500
155/155 - 21s - loss: 0.5578 - accuracy: 0.7157 - val_loss: 0.6762 - val_accuracy: 0.6007
Epoch 73/500
155/155 - 21s - loss: 0.5374 - accuracy: 0.7248 - val_loss: 0.6759 - val_accuracy: 0.6007
Epoch 74/500
155/155 - 21s - loss: 0.5356 - accuracy: 0.7315 - val_loss: 0.6761 - val_accuracy: 0.5974
Epoch 75/500
155/155 - 21s - loss: 0.5216 - accuracy: 0.7398 - val_loss: 0.6767 - val_accuracy: 0.5990
Epoch 76/500
155/155 - 21s - loss: 0.5284 - accuracy: 0.7392 - val_loss: 0.6758 - val_accuracy: 0.5957
Epoch 77/500
155/155 - 21s - loss: 0.5269 - accuracy: 0.7353 - val_loss: 0.6757 - val_accuracy: 0.6023
Epoch 78/500
155/155 - 21s - loss: 0.5131 - accuracy: 0.7467 - val_loss: 0.6766 - val_accuracy: 0.6007
Epoch 79/500
155/155 - 21s - loss: 0.5034 - accuracy: 0.7519 - val_loss: 0.6770 - val_accuracy: 0.5974
Epoch 80/500
155/155 - 21s - loss: 0.5045 - accuracy: 0.7552 - val_loss: 0.6765 - val_accuracy: 0.6007
Epoch 81/500
155/155 - 21s - loss: 0.4908 - accuracy: 0.7602 - val_loss: 0.6776 - val_accuracy: 0.6023
Epoch 82/500
155/155 - 21s - loss: 0.4927 - accuracy: 0.7596 - val_loss: 0.6777 - val_accuracy: 0.5990
Epoch 83/500
155/155 - 21s - loss: 0.4999 - accuracy: 0.7533 - val_loss: 0.6774 - val_accuracy: 0.6056
Epoch 84/500
155/155 - 21s - loss: 0.4801 - accuracy: 0.7628 - val_loss: 0.6789 - val_accuracy: 0.6007
Epoch 85/500
155/155 - 21s - loss: 0.4746 - accuracy: 0.7720 - val_loss: 0.6789 - val_accuracy: 0.6088
Epoch 86/500
155/155 - 21s - loss: 0.4756 - accuracy: 0.7746 - val_loss: 0.6797 - val_accuracy: 0.6056
Epoch 87/500
155/155 - 21s - loss: 0.4662 - accuracy: 0.7728 - val_loss: 0.6797 - val_accuracy: 0.6072
Epoch 88/500
155/155 - 21s - loss: 0.4610 - accuracy: 0.7800 - val_loss: 0.6807 - val_accuracy: 0.6056
Epoch 89/500
155/155 - 21s - loss: 0.4623 - accuracy: 0.7780 - val_loss: 0.6820 - val_accuracy: 0.6072
Epoch 90/500
155/155 - 21s - loss: 0.4473 - accuracy: 0.7881 - val_loss: 0.6825 - val_accuracy: 0.6088
Epoch 91/500
155/155 - 21s - loss: 0.4416 - accuracy: 0.7912 - val_loss: 0.6823 - val_accuracy: 0.6088
Epoch 92/500
155/155 - 21s - loss: 0.4334 - accuracy: 0.7970 - val_loss: 0.6825 - val_accuracy: 0.6105
Epoch 93/500
155/155 - 21s - loss: 0.4420 - accuracy: 0.7890 - val_loss: 0.6830 - val_accuracy: 0.6137
Epoch 94/500
155/155 - 21s - loss: 0.4377 - accuracy: 0.7960 - val_loss: 0.6840 - val_accuracy: 0.6105
Epoch 95/500
155/155 - 21s - loss: 0.4364 - accuracy: 0.7956 - val_loss: 0.6848 - val_accuracy: 0.6121
Epoch 96/500
155/155 - 21s - loss: 0.4236 - accuracy: 0.8013 - val_loss: 0.6867 - val_accuracy: 0.6088
Epoch 97/500
155/155 - 21s - loss: 0.4140 - accuracy: 0.8098 - val_loss: 0.6863 - val_accuracy: 0.6187
Epoch 98/500
155/155 - 21s - loss: 0.4136 - accuracy: 0.8149 - val_loss: 0.6869 - val_accuracy: 0.6203
Epoch 99/500
155/155 - 21s - loss: 0.4171 - accuracy: 0.8072 - val_loss: 0.6877 - val_accuracy: 0.6170
Epoch 100/500
155/155 - 21s - loss: 0.4150 - accuracy: 0.8086 - val_loss: 0.6889 - val_accuracy: 0.6154
Epoch 101/500
155/155 - 21s - loss: 0.3951 - accuracy: 0.8142 - val_loss: 0.6913 - val_accuracy: 0.6252
Epoch 102/500
155/155 - 21s - loss: 0.3861 - accuracy: 0.8282 - val_loss: 0.6930 - val_accuracy: 0.6187
Epoch 103/500
155/155 - 21s - loss: 0.3956 - accuracy: 0.8242 - val_loss: 0.6941 - val_accuracy: 0.6219
Epoch 104/500
155/155 - 21s - loss: 0.3826 - accuracy: 0.8254 - val_loss: 0.6949 - val_accuracy: 0.6203
Epoch 105/500
155/155 - 21s - loss: 0.3742 - accuracy: 0.8298 - val_loss: 0.6960 - val_accuracy: 0.6137
Epoch 106/500
155/155 - 21s - loss: 0.3678 - accuracy: 0.8375 - val_loss: 0.6974 - val_accuracy: 0.6203
Epoch 107/500
155/155 - 21s - loss: 0.3705 - accuracy: 0.8385 - val_loss: 0.6993 - val_accuracy: 0.6203
Epoch 108/500
155/155 - 21s - loss: 0.3739 - accuracy: 0.8385 - val_loss: 0.7014 - val_accuracy: 0.6187
Epoch 109/500
155/155 - 21s - loss: 0.3691 - accuracy: 0.8353 - val_loss: 0.7019 - val_accuracy: 0.6187
Epoch 110/500
155/155 - 21s - loss: 0.3535 - accuracy: 0.8430 - val_loss: 0.7022 - val_accuracy: 0.6236
Epoch 111/500
155/155 - 21s - loss: 0.3614 - accuracy: 0.8420 - val_loss: 0.7026 - val_accuracy: 0.6203
Epoch 112/500
155/155 - 21s - loss: 0.3493 - accuracy: 0.8470 - val_loss: 0.7031 - val_accuracy: 0.6219
Epoch 113/500
155/155 - 21s - loss: 0.3393 - accuracy: 0.8478 - val_loss: 0.7051 - val_accuracy: 0.6219
Epoch 114/500
155/155 - 21s - loss: 0.3209 - accuracy: 0.8650 - val_loss: 0.7067 - val_accuracy: 0.6252
Epoch 115/500
155/155 - 21s - loss: 0.3222 - accuracy: 0.8588 - val_loss: 0.7083 - val_accuracy: 0.6252
Epoch 116/500
155/155 - 21s - loss: 0.3301 - accuracy: 0.8541 - val_loss: 0.7105 - val_accuracy: 0.6219
Epoch 117/500
155/155 - 21s - loss: 0.3230 - accuracy: 0.8588 - val_loss: 0.7114 - val_accuracy: 0.6219
Epoch 118/500
155/155 - 21s - loss: 0.3183 - accuracy: 0.8662 - val_loss: 0.7123 - val_accuracy: 0.6219
Epoch 119/500
155/155 - 21s - loss: 0.3069 - accuracy: 0.8656 - val_loss: 0.7142 - val_accuracy: 0.6236
Epoch 120/500
155/155 - 21s - loss: 0.3100 - accuracy: 0.8687 - val_loss: 0.7154 - val_accuracy: 0.6203
Epoch 121/500
155/155 - 21s - loss: 0.2976 - accuracy: 0.8723 - val_loss: 0.7183 - val_accuracy: 0.6219
========================================
save_weights
h5_weights/BL1.po/embedding_cnn_one_branch.h5
========================================

end time >>> Sat Oct  2 23:53:38 2021

end time >>> Sat Oct  2 23:53:38 2021

end time >>> Sat Oct  2 23:53:38 2021

end time >>> Sat Oct  2 23:53:38 2021

end time >>> Sat Oct  2 23:53:38 2021












args.model = embedding_cnn_one_branch
time used = 2552.7124552726746


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sat Oct  2 23:53:39 2021

begin time >>> Sat Oct  2 23:53:39 2021

begin time >>> Sat Oct  2 23:53:39 2021

begin time >>> Sat Oct  2 23:53:39 2021

begin time >>> Sat Oct  2 23:53:39 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_cnn_two_branch
args.type = train
args.name = BL1.po
args.length = 10001
===========================


-> h5_weights/BL1.po folder already exist. pass.
-> result/BL1.po/onehot_cnn_one_branch folder already exist. pass.
-> result/BL1.po/onehot_cnn_two_branch folder already exist. pass.
-> result/BL1.po/onehot_embedding_dense folder already exist. pass.
-> result/BL1.po/onehot_dense folder already exist. pass.
-> result/BL1.po/onehot_resnet18 folder already exist. pass.
-> result/BL1.po/onehot_resnet34 folder already exist. pass.
-> result/BL1.po/embedding_cnn_one_branch folder already exist. pass.
-> result/BL1.po/embedding_cnn_two_branch folder already exist. pass.
-> result/BL1.po/embedding_dense folder already exist. pass.
-> result/BL1.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/BL1.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
BL1.po
########################################

########################################
model_name
embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
155/155 - 21s - loss: 0.8594 - accuracy: 0.5101 - val_loss: 0.6968 - val_accuracy: 0.5008
Epoch 2/500
155/155 - 21s - loss: 0.8776 - accuracy: 0.4943 - val_loss: 0.6964 - val_accuracy: 0.5172
Epoch 3/500
155/155 - 21s - loss: 0.8674 - accuracy: 0.5085 - val_loss: 0.6946 - val_accuracy: 0.5483
Epoch 4/500
155/155 - 21s - loss: 0.8531 - accuracy: 0.5134 - val_loss: 0.6939 - val_accuracy: 0.5565
Epoch 5/500
155/155 - 21s - loss: 0.8407 - accuracy: 0.5166 - val_loss: 0.6933 - val_accuracy: 0.5581
Epoch 6/500
155/155 - 21s - loss: 0.8425 - accuracy: 0.5194 - val_loss: 0.6922 - val_accuracy: 0.5614
Epoch 7/500
155/155 - 21s - loss: 0.8455 - accuracy: 0.5239 - val_loss: 0.6908 - val_accuracy: 0.5614
Epoch 8/500
155/155 - 21s - loss: 0.8237 - accuracy: 0.5308 - val_loss: 0.6893 - val_accuracy: 0.5532
Epoch 9/500
155/155 - 21s - loss: 0.8378 - accuracy: 0.5134 - val_loss: 0.6883 - val_accuracy: 0.5532
Epoch 10/500
155/155 - 21s - loss: 0.8227 - accuracy: 0.5326 - val_loss: 0.6873 - val_accuracy: 0.5532
Epoch 11/500
155/155 - 21s - loss: 0.8142 - accuracy: 0.5273 - val_loss: 0.6867 - val_accuracy: 0.5565
Epoch 12/500
155/155 - 21s - loss: 0.8074 - accuracy: 0.5451 - val_loss: 0.6854 - val_accuracy: 0.5565
Epoch 13/500
155/155 - 21s - loss: 0.7886 - accuracy: 0.5490 - val_loss: 0.6848 - val_accuracy: 0.5597
Epoch 14/500
155/155 - 21s - loss: 0.7984 - accuracy: 0.5368 - val_loss: 0.6841 - val_accuracy: 0.5630
Epoch 15/500
155/155 - 21s - loss: 0.7929 - accuracy: 0.5437 - val_loss: 0.6841 - val_accuracy: 0.5646
Epoch 16/500
155/155 - 21s - loss: 0.7964 - accuracy: 0.5312 - val_loss: 0.6826 - val_accuracy: 0.5663
Epoch 17/500
155/155 - 21s - loss: 0.7937 - accuracy: 0.5469 - val_loss: 0.6820 - val_accuracy: 0.5696
Epoch 18/500
155/155 - 21s - loss: 0.7826 - accuracy: 0.5498 - val_loss: 0.6812 - val_accuracy: 0.5761
Epoch 19/500
155/155 - 21s - loss: 0.7818 - accuracy: 0.5500 - val_loss: 0.6800 - val_accuracy: 0.5761
Epoch 20/500
155/155 - 21s - loss: 0.7624 - accuracy: 0.5599 - val_loss: 0.6801 - val_accuracy: 0.5859
Epoch 21/500
155/155 - 21s - loss: 0.7730 - accuracy: 0.5579 - val_loss: 0.6796 - val_accuracy: 0.5728
Epoch 22/500
155/155 - 21s - loss: 0.7628 - accuracy: 0.5637 - val_loss: 0.6785 - val_accuracy: 0.5908
Epoch 23/500
155/155 - 21s - loss: 0.7573 - accuracy: 0.5615 - val_loss: 0.6772 - val_accuracy: 0.5908
Epoch 24/500
155/155 - 21s - loss: 0.7676 - accuracy: 0.5656 - val_loss: 0.6767 - val_accuracy: 0.5941
Epoch 25/500
155/155 - 21s - loss: 0.7513 - accuracy: 0.5728 - val_loss: 0.6766 - val_accuracy: 0.5925
Epoch 26/500
155/155 - 21s - loss: 0.7569 - accuracy: 0.5629 - val_loss: 0.6756 - val_accuracy: 0.6007
Epoch 27/500
155/155 - 21s - loss: 0.7559 - accuracy: 0.5737 - val_loss: 0.6750 - val_accuracy: 0.5957
Epoch 28/500
155/155 - 21s - loss: 0.7533 - accuracy: 0.5737 - val_loss: 0.6743 - val_accuracy: 0.5957
Epoch 29/500
155/155 - 21s - loss: 0.7453 - accuracy: 0.5791 - val_loss: 0.6740 - val_accuracy: 0.6007
Epoch 30/500
155/155 - 21s - loss: 0.7355 - accuracy: 0.5894 - val_loss: 0.6732 - val_accuracy: 0.6007
Epoch 31/500
155/155 - 21s - loss: 0.7338 - accuracy: 0.5789 - val_loss: 0.6726 - val_accuracy: 0.6039
Epoch 32/500
155/155 - 21s - loss: 0.7355 - accuracy: 0.5866 - val_loss: 0.6722 - val_accuracy: 0.6007
Epoch 33/500
155/155 - 21s - loss: 0.7273 - accuracy: 0.5864 - val_loss: 0.6714 - val_accuracy: 0.6056
Epoch 34/500
155/155 - 21s - loss: 0.7184 - accuracy: 0.6008 - val_loss: 0.6702 - val_accuracy: 0.5974
Epoch 35/500
155/155 - 21s - loss: 0.7067 - accuracy: 0.6038 - val_loss: 0.6694 - val_accuracy: 0.6023
Epoch 36/500
155/155 - 21s - loss: 0.6977 - accuracy: 0.6123 - val_loss: 0.6693 - val_accuracy: 0.6023
Epoch 37/500
155/155 - 21s - loss: 0.7085 - accuracy: 0.6012 - val_loss: 0.6681 - val_accuracy: 0.6072
Epoch 38/500
155/155 - 21s - loss: 0.6930 - accuracy: 0.6115 - val_loss: 0.6676 - val_accuracy: 0.6072
Epoch 39/500
155/155 - 21s - loss: 0.6923 - accuracy: 0.6131 - val_loss: 0.6669 - val_accuracy: 0.6072
Epoch 40/500
155/155 - 21s - loss: 0.7088 - accuracy: 0.6008 - val_loss: 0.6667 - val_accuracy: 0.6088
Epoch 41/500
155/155 - 21s - loss: 0.6933 - accuracy: 0.6200 - val_loss: 0.6660 - val_accuracy: 0.6072
Epoch 42/500
155/155 - 21s - loss: 0.6756 - accuracy: 0.6242 - val_loss: 0.6658 - val_accuracy: 0.6023
Epoch 43/500
155/155 - 21s - loss: 0.6752 - accuracy: 0.6244 - val_loss: 0.6649 - val_accuracy: 0.6039
Epoch 44/500
155/155 - 21s - loss: 0.6647 - accuracy: 0.6380 - val_loss: 0.6644 - val_accuracy: 0.6121
Epoch 45/500
155/155 - 21s - loss: 0.6619 - accuracy: 0.6376 - val_loss: 0.6637 - val_accuracy: 0.6039
Epoch 46/500
155/155 - 21s - loss: 0.6676 - accuracy: 0.6279 - val_loss: 0.6633 - val_accuracy: 0.6088
Epoch 47/500
155/155 - 21s - loss: 0.6561 - accuracy: 0.6396 - val_loss: 0.6630 - val_accuracy: 0.6121
Epoch 48/500
155/155 - 21s - loss: 0.6572 - accuracy: 0.6429 - val_loss: 0.6622 - val_accuracy: 0.6121
Epoch 49/500
155/155 - 21s - loss: 0.6533 - accuracy: 0.6410 - val_loss: 0.6619 - val_accuracy: 0.6154
Epoch 50/500
155/155 - 21s - loss: 0.6383 - accuracy: 0.6497 - val_loss: 0.6604 - val_accuracy: 0.6105
Epoch 51/500
155/155 - 21s - loss: 0.6458 - accuracy: 0.6526 - val_loss: 0.6607 - val_accuracy: 0.6121
Epoch 52/500
155/155 - 21s - loss: 0.6438 - accuracy: 0.6493 - val_loss: 0.6595 - val_accuracy: 0.6236
Epoch 53/500
155/155 - 21s - loss: 0.6250 - accuracy: 0.6641 - val_loss: 0.6585 - val_accuracy: 0.6252
Epoch 54/500
155/155 - 21s - loss: 0.6247 - accuracy: 0.6627 - val_loss: 0.6583 - val_accuracy: 0.6252
Epoch 55/500
155/155 - 21s - loss: 0.6071 - accuracy: 0.6787 - val_loss: 0.6578 - val_accuracy: 0.6203
Epoch 56/500
155/155 - 21s - loss: 0.6247 - accuracy: 0.6694 - val_loss: 0.6569 - val_accuracy: 0.6268
Epoch 57/500
155/155 - 21s - loss: 0.6103 - accuracy: 0.6734 - val_loss: 0.6569 - val_accuracy: 0.6268
Epoch 58/500
155/155 - 21s - loss: 0.5900 - accuracy: 0.6945 - val_loss: 0.6565 - val_accuracy: 0.6285
Epoch 59/500
155/155 - 21s - loss: 0.5981 - accuracy: 0.6872 - val_loss: 0.6557 - val_accuracy: 0.6301
Epoch 60/500
155/155 - 21s - loss: 0.5920 - accuracy: 0.7021 - val_loss: 0.6555 - val_accuracy: 0.6334
Epoch 61/500
155/155 - 21s - loss: 0.5872 - accuracy: 0.6932 - val_loss: 0.6546 - val_accuracy: 0.6432
Epoch 62/500
155/155 - 21s - loss: 0.5754 - accuracy: 0.7044 - val_loss: 0.6545 - val_accuracy: 0.6383
Epoch 63/500
155/155 - 21s - loss: 0.5611 - accuracy: 0.7135 - val_loss: 0.6544 - val_accuracy: 0.6383
Epoch 64/500
155/155 - 21s - loss: 0.5790 - accuracy: 0.6993 - val_loss: 0.6536 - val_accuracy: 0.6383
Epoch 65/500
155/155 - 21s - loss: 0.5612 - accuracy: 0.7155 - val_loss: 0.6538 - val_accuracy: 0.6367
Epoch 66/500
155/155 - 21s - loss: 0.5540 - accuracy: 0.7220 - val_loss: 0.6538 - val_accuracy: 0.6383
Epoch 67/500
155/155 - 21s - loss: 0.5574 - accuracy: 0.7133 - val_loss: 0.6541 - val_accuracy: 0.6399
Epoch 68/500
155/155 - 21s - loss: 0.5587 - accuracy: 0.7216 - val_loss: 0.6533 - val_accuracy: 0.6367
Epoch 69/500
155/155 - 21s - loss: 0.5404 - accuracy: 0.7309 - val_loss: 0.6530 - val_accuracy: 0.6448
Epoch 70/500
155/155 - 21s - loss: 0.5397 - accuracy: 0.7319 - val_loss: 0.6529 - val_accuracy: 0.6432
Epoch 71/500
155/155 - 21s - loss: 0.5213 - accuracy: 0.7428 - val_loss: 0.6535 - val_accuracy: 0.6416
Epoch 72/500
155/155 - 21s - loss: 0.5170 - accuracy: 0.7489 - val_loss: 0.6537 - val_accuracy: 0.6416
Epoch 73/500
155/155 - 21s - loss: 0.5070 - accuracy: 0.7487 - val_loss: 0.6530 - val_accuracy: 0.6432
Epoch 74/500
155/155 - 21s - loss: 0.5166 - accuracy: 0.7400 - val_loss: 0.6533 - val_accuracy: 0.6399
Epoch 75/500
155/155 - 21s - loss: 0.4966 - accuracy: 0.7590 - val_loss: 0.6531 - val_accuracy: 0.6367
Epoch 76/500
155/155 - 21s - loss: 0.4934 - accuracy: 0.7641 - val_loss: 0.6539 - val_accuracy: 0.6383
Epoch 77/500
155/155 - 21s - loss: 0.4877 - accuracy: 0.7588 - val_loss: 0.6543 - val_accuracy: 0.6481
Epoch 78/500
155/155 - 21s - loss: 0.4949 - accuracy: 0.7576 - val_loss: 0.6547 - val_accuracy: 0.6448
Epoch 79/500
155/155 - 21s - loss: 0.4908 - accuracy: 0.7643 - val_loss: 0.6554 - val_accuracy: 0.6465
Epoch 80/500
155/155 - 21s - loss: 0.4783 - accuracy: 0.7683 - val_loss: 0.6553 - val_accuracy: 0.6432
Epoch 81/500
155/155 - 21s - loss: 0.4777 - accuracy: 0.7606 - val_loss: 0.6557 - val_accuracy: 0.6448
Epoch 82/500
155/155 - 21s - loss: 0.4661 - accuracy: 0.7762 - val_loss: 0.6560 - val_accuracy: 0.6416
Epoch 83/500
155/155 - 20s - loss: 0.4546 - accuracy: 0.7867 - val_loss: 0.6568 - val_accuracy: 0.6399
Epoch 84/500
155/155 - 21s - loss: 0.4511 - accuracy: 0.7833 - val_loss: 0.6574 - val_accuracy: 0.6416
Epoch 85/500
155/155 - 21s - loss: 0.4492 - accuracy: 0.7879 - val_loss: 0.6579 - val_accuracy: 0.6432
Epoch 86/500
155/155 - 20s - loss: 0.4463 - accuracy: 0.7898 - val_loss: 0.6584 - val_accuracy: 0.6399
Epoch 87/500
155/155 - 21s - loss: 0.4416 - accuracy: 0.7922 - val_loss: 0.6591 - val_accuracy: 0.6448
Epoch 88/500
155/155 - 21s - loss: 0.4235 - accuracy: 0.7997 - val_loss: 0.6604 - val_accuracy: 0.6416
Epoch 89/500
155/155 - 21s - loss: 0.4230 - accuracy: 0.8062 - val_loss: 0.6612 - val_accuracy: 0.6416
Epoch 90/500
155/155 - 21s - loss: 0.4181 - accuracy: 0.8053 - val_loss: 0.6615 - val_accuracy: 0.6399
Epoch 91/500
155/155 - 21s - loss: 0.4072 - accuracy: 0.8219 - val_loss: 0.6630 - val_accuracy: 0.6448
Epoch 92/500
155/155 - 21s - loss: 0.4156 - accuracy: 0.8049 - val_loss: 0.6635 - val_accuracy: 0.6432
Epoch 93/500
155/155 - 21s - loss: 0.3978 - accuracy: 0.8159 - val_loss: 0.6644 - val_accuracy: 0.6448
Epoch 94/500
155/155 - 21s - loss: 0.4063 - accuracy: 0.8153 - val_loss: 0.6653 - val_accuracy: 0.6383
Epoch 95/500
155/155 - 21s - loss: 0.3911 - accuracy: 0.8213 - val_loss: 0.6665 - val_accuracy: 0.6399
Epoch 96/500
155/155 - 21s - loss: 0.3636 - accuracy: 0.8375 - val_loss: 0.6671 - val_accuracy: 0.6448
Epoch 97/500
155/155 - 21s - loss: 0.3815 - accuracy: 0.8284 - val_loss: 0.6685 - val_accuracy: 0.6465
========================================
save_weights
h5_weights/BL1.po/embedding_cnn_two_branch.h5
========================================

end time >>> Sun Oct  3 00:27:24 2021

end time >>> Sun Oct  3 00:27:24 2021

end time >>> Sun Oct  3 00:27:24 2021

end time >>> Sun Oct  3 00:27:24 2021

end time >>> Sun Oct  3 00:27:24 2021












args.model = embedding_cnn_two_branch
time used = 2025.1522784233093


