************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sat Oct  2 21:20:10 2021

begin time >>> Sat Oct  2 21:20:10 2021

begin time >>> Sat Oct  2 21:20:10 2021

begin time >>> Sat Oct  2 21:20:10 2021

begin time >>> Sat Oct  2 21:20:10 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_cnn_one_branch
args.type = train
args.name = AO.po
args.length = 10001
===========================


-> make new folder: h5_weights/AO.po
-> make new folder: result/AO.po/onehot_cnn_one_branch
-> make new folder: result/AO.po/onehot_cnn_two_branch
-> make new folder: result/AO.po/onehot_embedding_dense
-> make new folder: result/AO.po/onehot_dense
-> make new folder: result/AO.po/onehot_resnet18
-> make new folder: result/AO.po/onehot_resnet34
-> make new folder: result/AO.po/embedding_cnn_one_branch
-> make new folder: result/AO.po/embedding_cnn_two_branch
-> make new folder: result/AO.po/embedding_dense
-> make new folder: result/AO.po/onehot_embedding_cnn_one_branch
-> make new folder: result/AO.po/onehot_embedding_cnn_two_branch
########################################
gen_name
AO.po
########################################

########################################
model_name
onehot_cnn_one_branch
########################################

Found 5178 images belonging to 2 classes.
Found 638 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 5001, 5, 64)       1600      
_________________________________________________________________
batch_normalization (BatchNo (None, 5001, 5, 64)       256       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1251, 5, 64)       98368     
_________________________________________________________________
batch_normalization_1 (Batch (None, 1251, 5, 64)       256       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 313, 5, 64)        98368     
_________________________________________________________________
batch_normalization_2 (Batch (None, 313, 5, 64)        256       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 156, 5, 64)        0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 156, 5, 64)        256       
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 39, 5, 128)        196736    
_________________________________________________________________
batch_normalization_4 (Batch (None, 39, 5, 128)        512       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 10, 5, 128)        393344    
_________________________________________________________________
batch_normalization_5 (Batch (None, 10, 5, 128)        512       
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 3, 5, 128)         393344    
_________________________________________________________________
batch_normalization_6 (Batch (None, 3, 5, 128)         512       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 1, 5, 128)         0         
_________________________________________________________________
flatten (Flatten)            (None, 640)               0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 640)               2560      
_________________________________________________________________
dense (Dense)                (None, 512)               328192    
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
dense_3 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 2,041,410
Trainable params: 2,038,850
Non-trainable params: 2,560
_________________________________________________________________
Epoch 1/500
161/161 - 103s - loss: 0.7940 - accuracy: 0.5029 - val_loss: 0.6991 - val_accuracy: 0.5000
Epoch 2/500
161/161 - 19s - loss: 0.7244 - accuracy: 0.5443 - val_loss: 0.7914 - val_accuracy: 0.5016
Epoch 3/500
161/161 - 19s - loss: 0.6898 - accuracy: 0.5746 - val_loss: 0.7122 - val_accuracy: 0.5115
Epoch 4/500
161/161 - 19s - loss: 0.6404 - accuracy: 0.6372 - val_loss: 1.0889 - val_accuracy: 0.5033
Epoch 5/500
161/161 - 19s - loss: 0.5880 - accuracy: 0.6891 - val_loss: 1.3393 - val_accuracy: 0.4984
Epoch 6/500
161/161 - 19s - loss: 0.4972 - accuracy: 0.7551 - val_loss: 1.0269 - val_accuracy: 0.5263
Epoch 7/500
161/161 - 19s - loss: 0.3680 - accuracy: 0.8403 - val_loss: 0.8712 - val_accuracy: 0.5592
Epoch 8/500
161/161 - 20s - loss: 0.2200 - accuracy: 0.9195 - val_loss: 1.3194 - val_accuracy: 0.5247
Epoch 9/500
161/161 - 19s - loss: 0.1357 - accuracy: 0.9512 - val_loss: 4.2946 - val_accuracy: 0.5099
Epoch 10/500
161/161 - 19s - loss: 0.0837 - accuracy: 0.9724 - val_loss: 1.5630 - val_accuracy: 0.5444
Epoch 11/500
161/161 - 19s - loss: 0.0556 - accuracy: 0.9796 - val_loss: 2.6950 - val_accuracy: 0.5296
Epoch 12/500
161/161 - 19s - loss: 0.0378 - accuracy: 0.9878 - val_loss: 5.6692 - val_accuracy: 0.5049
Epoch 13/500
161/161 - 19s - loss: 0.0282 - accuracy: 0.9901 - val_loss: 11.0832 - val_accuracy: 0.5000
Epoch 14/500
161/161 - 19s - loss: 0.0238 - accuracy: 0.9920 - val_loss: 2.3523 - val_accuracy: 0.5789
Epoch 15/500
161/161 - 19s - loss: 0.0146 - accuracy: 0.9967 - val_loss: 1.9559 - val_accuracy: 0.5707
Epoch 16/500
161/161 - 19s - loss: 0.0196 - accuracy: 0.9926 - val_loss: 7.0413 - val_accuracy: 0.4918
Epoch 17/500
161/161 - 19s - loss: 0.0141 - accuracy: 0.9963 - val_loss: 8.2844 - val_accuracy: 0.4967
Epoch 18/500
161/161 - 19s - loss: 0.0177 - accuracy: 0.9934 - val_loss: 4.6010 - val_accuracy: 0.5263
Epoch 19/500
161/161 - 19s - loss: 0.0163 - accuracy: 0.9940 - val_loss: 2.6832 - val_accuracy: 0.6069
Epoch 20/500
161/161 - 19s - loss: 0.0167 - accuracy: 0.9949 - val_loss: 15.6628 - val_accuracy: 0.4967
Epoch 21/500
161/161 - 19s - loss: 0.0272 - accuracy: 0.9909 - val_loss: 8.5562 - val_accuracy: 0.5000
Epoch 22/500
161/161 - 19s - loss: 0.0276 - accuracy: 0.9899 - val_loss: 5.1452 - val_accuracy: 0.5263
Epoch 23/500
161/161 - 19s - loss: 0.0338 - accuracy: 0.9887 - val_loss: 8.6282 - val_accuracy: 0.5082
Epoch 24/500
161/161 - 19s - loss: 0.0289 - accuracy: 0.9905 - val_loss: 7.8368 - val_accuracy: 0.5099
Epoch 25/500
161/161 - 19s - loss: 0.0239 - accuracy: 0.9907 - val_loss: 7.8884 - val_accuracy: 0.5000
Epoch 26/500
161/161 - 19s - loss: 0.0189 - accuracy: 0.9934 - val_loss: 4.9541 - val_accuracy: 0.5855
Epoch 27/500
161/161 - 19s - loss: 0.0249 - accuracy: 0.9895 - val_loss: 3.5652 - val_accuracy: 0.5395
Epoch 28/500
161/161 - 19s - loss: 0.0255 - accuracy: 0.9909 - val_loss: 4.5056 - val_accuracy: 0.5214
Epoch 29/500
161/161 - 19s - loss: 0.0143 - accuracy: 0.9946 - val_loss: 5.4620 - val_accuracy: 0.5329
========================================
save_weights
h5_weights/AO.po/onehot_cnn_one_branch.h5
========================================

end time >>> Sat Oct  2 21:31:53 2021

end time >>> Sat Oct  2 21:31:53 2021

end time >>> Sat Oct  2 21:31:53 2021

end time >>> Sat Oct  2 21:31:53 2021

end time >>> Sat Oct  2 21:31:53 2021












args.model = onehot_cnn_one_branch
time used = 703.0840919017792


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sat Oct  2 21:31:55 2021

begin time >>> Sat Oct  2 21:31:55 2021

begin time >>> Sat Oct  2 21:31:55 2021

begin time >>> Sat Oct  2 21:31:55 2021

begin time >>> Sat Oct  2 21:31:55 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_cnn_two_branch
args.type = train
args.name = AO.po
args.length = 10001
===========================


-> h5_weights/AO.po folder already exist. pass.
-> result/AO.po/onehot_cnn_one_branch folder already exist. pass.
-> result/AO.po/onehot_cnn_two_branch folder already exist. pass.
-> result/AO.po/onehot_embedding_dense folder already exist. pass.
-> result/AO.po/onehot_dense folder already exist. pass.
-> result/AO.po/onehot_resnet18 folder already exist. pass.
-> result/AO.po/onehot_resnet34 folder already exist. pass.
-> result/AO.po/embedding_cnn_one_branch folder already exist. pass.
-> result/AO.po/embedding_cnn_two_branch folder already exist. pass.
-> result/AO.po/embedding_dense folder already exist. pass.
-> result/AO.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/AO.po/onehot_embedding_cnn_two_branch folder already exist. pass.
Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001, 5, 1) 0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001, 5, 1) 0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 2501, 5, 64)  1600        input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 2501, 5, 64)  1600        input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 2501, 5, 64)  256         conv2d[0][0]                     
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 2501, 5, 64)  256         conv2d_6[0][0]                   
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 626, 5, 64)   98368       batch_normalization[0][0]        
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 626, 5, 64)   98368       batch_normalization_8[0][0]      
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 626, 5, 64)   256         conv2d_1[0][0]                   
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 626, 5, 64)   256         conv2d_7[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 157, 5, 64)   98368       batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 157, 5, 64)   98368       batch_normalization_9[0][0]      
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 157, 5, 64)   256         conv2d_2[0][0]                   
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 157, 5, 64)   256         conv2d_8[0][0]                   
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 78, 5, 64)    0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 78, 5, 64)    0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 78, 5, 64)    256         max_pooling2d[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 78, 5, 64)    256         max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 20, 5, 128)   196736      batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 20, 5, 128)   196736      batch_normalization_11[0][0]     
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 20, 5, 128)   512         conv2d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 20, 5, 128)   512         conv2d_9[0][0]                   
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 5, 5, 128)    393344      batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 5, 5, 128)    393344      batch_normalization_12[0][0]     
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 5, 5, 128)    512         conv2d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 5, 5, 128)    512         conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 2, 5, 128)    393344      batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 2, 5, 128)    393344      batch_normalization_13[0][0]     
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 2, 5, 128)    512         conv2d_5[0][0]                   
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 2, 5, 128)    512         conv2d_11[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 1, 5, 128)    0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 1, 5, 128)    0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
flatten (Flatten)               (None, 640)          0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 640)          0           max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 640)          2560        flatten[0][0]                    
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 640)          2560        flatten_1[0][0]                  
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          328192      batch_normalization_7[0][0]      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          328192      batch_normalization_15[0][0]     
__________________________________________________________________________________________________
dropout (Dropout)               (None, 512)          0           dense[0][0]                      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 1024)         0           dropout[0][0]                    
                                                                 dropout_1[0][0]                  
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          524800      concatenate[0][0]                
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 512)          262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 2)            1026        dense_3[0][0]                    
==================================================================================================
Total params: 3,818,626
Trainable params: 3,813,506
Non-trainable params: 5,120
__________________________________________________________________________________________________
Found 5178 images belonging to 2 classes.
Found 5178 images belonging to 2 classes.
Epoch 1/500
Found 638 images belonging to 2 classes.
Found 638 images belonging to 2 classes.
1535/1535 - 438s - loss: 0.4437 - accuracy: 0.7658 - val_loss: 1.4506 - val_accuracy: 0.5965
Epoch 2/500
1535/1535 - 231s - loss: 0.0321 - accuracy: 0.9890 - val_loss: 3.0624 - val_accuracy: 0.5949
Epoch 3/500
1535/1535 - 232s - loss: 0.0239 - accuracy: 0.9924 - val_loss: 8.4470 - val_accuracy: 0.5236
Epoch 4/500
1535/1535 - 233s - loss: 0.0202 - accuracy: 0.9940 - val_loss: 2.9641 - val_accuracy: 0.5583
Epoch 5/500
1535/1535 - 228s - loss: 0.0159 - accuracy: 0.9949 - val_loss: 5.5124 - val_accuracy: 0.5374
Epoch 6/500
1535/1535 - 227s - loss: 0.0137 - accuracy: 0.9960 - val_loss: 4.9022 - val_accuracy: 0.5217
Epoch 7/500
1535/1535 - 225s - loss: 0.0123 - accuracy: 0.9962 - val_loss: 2.7734 - val_accuracy: 0.5936
Epoch 8/500
1535/1535 - 228s - loss: 0.0085 - accuracy: 0.9975 - val_loss: 3.4303 - val_accuracy: 0.5712
Epoch 9/500
1535/1535 - 229s - loss: 0.0113 - accuracy: 0.9967 - val_loss: 6.3109 - val_accuracy: 0.5504
Epoch 10/500
1535/1535 - 228s - loss: 0.0086 - accuracy: 0.9976 - val_loss: 3.5056 - val_accuracy: 0.5655
Epoch 11/500
1535/1535 - 228s - loss: 0.0086 - accuracy: 0.9972 - val_loss: 2.7938 - val_accuracy: 0.6226
Epoch 12/500
1535/1535 - 227s - loss: 0.0072 - accuracy: 0.9975 - val_loss: 3.0645 - val_accuracy: 0.5909
Epoch 13/500
1535/1535 - 225s - loss: 0.0060 - accuracy: 0.9981 - val_loss: 4.1802 - val_accuracy: 0.5922
Epoch 14/500
1535/1535 - 228s - loss: 0.0092 - accuracy: 0.9970 - val_loss: 2.6560 - val_accuracy: 0.6067
Epoch 15/500
1535/1535 - 225s - loss: 0.0067 - accuracy: 0.9980 - val_loss: 2.3800 - val_accuracy: 0.6155
Epoch 16/500
1535/1535 - 223s - loss: 0.0062 - accuracy: 0.9977 - val_loss: 3.1213 - val_accuracy: 0.5996
Epoch 17/500
1535/1535 - 224s - loss: 0.0053 - accuracy: 0.9982 - val_loss: 2.3829 - val_accuracy: 0.6021
Epoch 18/500
1535/1535 - 222s - loss: 0.0062 - accuracy: 0.9983 - val_loss: 2.8276 - val_accuracy: 0.6045
Epoch 19/500
1535/1535 - 222s - loss: 0.0061 - accuracy: 0.9980 - val_loss: 2.5904 - val_accuracy: 0.6100
Epoch 20/500
1535/1535 - 225s - loss: 0.0043 - accuracy: 0.9988 - val_loss: 2.5361 - val_accuracy: 0.6250
Epoch 21/500
1535/1535 - 226s - loss: 0.0047 - accuracy: 0.9985 - val_loss: 2.6224 - val_accuracy: 0.5909
Epoch 22/500
1535/1535 - 229s - loss: 0.0038 - accuracy: 0.9986 - val_loss: 3.1170 - val_accuracy: 0.6266
Epoch 23/500
1535/1535 - 231s - loss: 0.0055 - accuracy: 0.9985 - val_loss: 2.5996 - val_accuracy: 0.5950
Epoch 24/500
1535/1535 - 227s - loss: 0.0042 - accuracy: 0.9986 - val_loss: 3.1779 - val_accuracy: 0.6316
Epoch 25/500
1535/1535 - 226s - loss: 0.0043 - accuracy: 0.9986 - val_loss: 3.2495 - val_accuracy: 0.5971
Epoch 26/500
1535/1535 - 226s - loss: 0.0040 - accuracy: 0.9988 - val_loss: 2.8502 - val_accuracy: 0.6183
Epoch 27/500
1535/1535 - 226s - loss: 0.0034 - accuracy: 0.9988 - val_loss: 3.0839 - val_accuracy: 0.5886
Epoch 28/500
1535/1535 - 227s - loss: 0.0019 - accuracy: 0.9994 - val_loss: 3.1929 - val_accuracy: 0.6203
Epoch 29/500
1535/1535 - 225s - loss: 0.0029 - accuracy: 0.9991 - val_loss: 2.9359 - val_accuracy: 0.5831
Epoch 30/500
1535/1535 - 225s - loss: 0.0040 - accuracy: 0.9989 - val_loss: 2.8427 - val_accuracy: 0.6385
Epoch 31/500
1535/1535 - 226s - loss: 0.0051 - accuracy: 0.9985 - val_loss: 2.0802 - val_accuracy: 0.6354
Epoch 32/500
1535/1535 - 228s - loss: 0.0021 - accuracy: 0.9993 - val_loss: 3.1242 - val_accuracy: 0.6417
Epoch 33/500
1535/1535 - 228s - loss: 0.0027 - accuracy: 0.9993 - val_loss: 2.5271 - val_accuracy: 0.6073
Epoch 34/500
1535/1535 - 226s - loss: 0.0048 - accuracy: 0.9985 - val_loss: 2.9516 - val_accuracy: 0.5908
Epoch 35/500
1535/1535 - 229s - loss: 0.0022 - accuracy: 0.9992 - val_loss: 2.0232 - val_accuracy: 0.6229
Epoch 36/500
1535/1535 - 226s - loss: 0.0027 - accuracy: 0.9991 - val_loss: 4.4956 - val_accuracy: 0.5801
Epoch 37/500
1535/1535 - 249s - loss: 0.0025 - accuracy: 0.9993 - val_loss: 2.9128 - val_accuracy: 0.6229
Epoch 38/500
1535/1535 - 229s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 4.2620 - val_accuracy: 0.6242
Epoch 39/500
1535/1535 - 224s - loss: 0.0047 - accuracy: 0.9988 - val_loss: 2.5998 - val_accuracy: 0.6282
Epoch 40/500
1535/1535 - 225s - loss: 0.0029 - accuracy: 0.9992 - val_loss: 2.8412 - val_accuracy: 0.6237
Epoch 41/500
1535/1535 - 222s - loss: 0.0021 - accuracy: 0.9993 - val_loss: 4.0013 - val_accuracy: 0.6372
Epoch 42/500
1535/1535 - 223s - loss: 0.0021 - accuracy: 0.9993 - val_loss: 2.4381 - val_accuracy: 0.6229
========================================
save_weights
h5_weights/AO.po/onehot_cnn_two_branch.h5
========================================

end time >>> Sun Oct  3 00:14:44 2021

end time >>> Sun Oct  3 00:14:44 2021

end time >>> Sun Oct  3 00:14:44 2021

end time >>> Sun Oct  3 00:14:44 2021

end time >>> Sun Oct  3 00:14:44 2021












args.model = onehot_cnn_two_branch
time used = 9769.383424043655


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 00:14:45 2021

begin time >>> Sun Oct  3 00:14:45 2021

begin time >>> Sun Oct  3 00:14:45 2021

begin time >>> Sun Oct  3 00:14:45 2021

begin time >>> Sun Oct  3 00:14:45 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_dense
args.type = train
args.name = AO.po
args.length = 10001
===========================


-> h5_weights/AO.po folder already exist. pass.
-> result/AO.po/onehot_cnn_one_branch folder already exist. pass.
-> result/AO.po/onehot_cnn_two_branch folder already exist. pass.
-> result/AO.po/onehot_embedding_dense folder already exist. pass.
-> result/AO.po/onehot_dense folder already exist. pass.
-> result/AO.po/onehot_resnet18 folder already exist. pass.
-> result/AO.po/onehot_resnet34 folder already exist. pass.
-> result/AO.po/embedding_cnn_one_branch folder already exist. pass.
-> result/AO.po/embedding_cnn_two_branch folder already exist. pass.
-> result/AO.po/embedding_dense folder already exist. pass.
-> result/AO.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/AO.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
AO.po
########################################

########################################
model_name
onehot_dense
########################################

Found 5178 images belonging to 2 classes.
Found 638 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 100010)            0         
_________________________________________________________________
batch_normalization (BatchNo (None, 100010)            400040    
_________________________________________________________________
dense (Dense)                (None, 512)               51205632  
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_3 (Batch (None, 512)               2048      
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_3 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 52,402,858
Trainable params: 52,198,742
Non-trainable params: 204,116
_________________________________________________________________
Epoch 1/500
161/161 - 112s - loss: 0.8498 - accuracy: 0.5052 - val_loss: 0.6790 - val_accuracy: 0.5609
Epoch 2/500
161/161 - 15s - loss: 0.6974 - accuracy: 0.5925 - val_loss: 0.6727 - val_accuracy: 0.5641
Epoch 3/500
161/161 - 15s - loss: 0.6203 - accuracy: 0.6572 - val_loss: 0.6613 - val_accuracy: 0.5970
Epoch 4/500
161/161 - 15s - loss: 0.5295 - accuracy: 0.7373 - val_loss: 0.6694 - val_accuracy: 0.6102
Epoch 5/500
161/161 - 15s - loss: 0.3991 - accuracy: 0.8177 - val_loss: 0.7059 - val_accuracy: 0.6168
Epoch 6/500
161/161 - 15s - loss: 0.2711 - accuracy: 0.8933 - val_loss: 0.7801 - val_accuracy: 0.6316
Epoch 7/500
161/161 - 15s - loss: 0.1917 - accuracy: 0.9277 - val_loss: 0.8767 - val_accuracy: 0.6283
Epoch 8/500
161/161 - 15s - loss: 0.1314 - accuracy: 0.9534 - val_loss: 0.9597 - val_accuracy: 0.6365
Epoch 9/500
161/161 - 15s - loss: 0.1088 - accuracy: 0.9627 - val_loss: 1.0020 - val_accuracy: 0.6431
Epoch 10/500
161/161 - 15s - loss: 0.0815 - accuracy: 0.9724 - val_loss: 1.1412 - val_accuracy: 0.6201
Epoch 11/500
161/161 - 15s - loss: 0.0666 - accuracy: 0.9792 - val_loss: 1.1665 - val_accuracy: 0.6201
Epoch 12/500
161/161 - 15s - loss: 0.0590 - accuracy: 0.9798 - val_loss: 1.1703 - val_accuracy: 0.6316
Epoch 13/500
161/161 - 15s - loss: 0.0484 - accuracy: 0.9839 - val_loss: 1.2266 - val_accuracy: 0.6414
Epoch 14/500
161/161 - 15s - loss: 0.0454 - accuracy: 0.9858 - val_loss: 1.2577 - val_accuracy: 0.6382
Epoch 15/500
161/161 - 15s - loss: 0.0405 - accuracy: 0.9864 - val_loss: 1.3227 - val_accuracy: 0.6332
Epoch 16/500
161/161 - 15s - loss: 0.0466 - accuracy: 0.9833 - val_loss: 1.3183 - val_accuracy: 0.6316
Epoch 17/500
161/161 - 15s - loss: 0.0278 - accuracy: 0.9907 - val_loss: 1.3581 - val_accuracy: 0.6382
Epoch 18/500
161/161 - 15s - loss: 0.0359 - accuracy: 0.9878 - val_loss: 1.3969 - val_accuracy: 0.6349
Epoch 19/500
161/161 - 15s - loss: 0.0344 - accuracy: 0.9887 - val_loss: 1.4140 - val_accuracy: 0.6217
========================================
save_weights
h5_weights/AO.po/onehot_dense.h5
========================================

end time >>> Sun Oct  3 00:21:20 2021

end time >>> Sun Oct  3 00:21:20 2021

end time >>> Sun Oct  3 00:21:20 2021

end time >>> Sun Oct  3 00:21:20 2021

end time >>> Sun Oct  3 00:21:20 2021












args.model = onehot_dense
time used = 394.2474944591522


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 00:21:20 2021

begin time >>> Sun Oct  3 00:21:20 2021

begin time >>> Sun Oct  3 00:21:20 2021

begin time >>> Sun Oct  3 00:21:20 2021

begin time >>> Sun Oct  3 00:21:20 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_resnet18
args.type = train
args.name = AO.po
args.length = 10001
===========================


-> h5_weights/AO.po folder already exist. pass.
-> result/AO.po/onehot_cnn_one_branch folder already exist. pass.
-> result/AO.po/onehot_cnn_two_branch folder already exist. pass.
-> result/AO.po/onehot_embedding_dense folder already exist. pass.
-> result/AO.po/onehot_dense folder already exist. pass.
-> result/AO.po/onehot_resnet18 folder already exist. pass.
-> result/AO.po/onehot_resnet34 folder already exist. pass.
-> result/AO.po/embedding_cnn_one_branch folder already exist. pass.
-> result/AO.po/embedding_cnn_two_branch folder already exist. pass.
-> result/AO.po/embedding_dense folder already exist. pass.
-> result/AO.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/AO.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
AO.po
########################################

########################################
model_name
onehot_resnet18
########################################

Found 5178 images belonging to 2 classes.
Found 638 images belonging to 2 classes.
Model: "res_net_type_i"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              multiple                  1600      
_________________________________________________________________
batch_normalization (BatchNo multiple                  256       
_________________________________________________________________
max_pooling2d (MaxPooling2D) multiple                  0         
_________________________________________________________________
sequential (Sequential)      (None, 313, 1, 64)        398912    
_________________________________________________________________
sequential_2 (Sequential)    (None, 20, 1, 64)         398912    
_________________________________________________________________
sequential_4 (Sequential)    (None, 2, 1, 64)          398912    
_________________________________________________________________
sequential_6 (Sequential)    (None, 1, 1, 64)          398912    
_________________________________________________________________
global_average_pooling2d (Gl multiple                  0         
_________________________________________________________________
dense (Dense)                multiple                  130       
=================================================================
Total params: 1,597,634
Trainable params: 1,594,946
Non-trainable params: 2,688
_________________________________________________________________
Epoch 1/500
161/161 - 20s - loss: 0.7836 - accuracy: 0.5051 - val_loss: 0.6972 - val_accuracy: 0.4967
Epoch 2/500
161/161 - 19s - loss: 0.6249 - accuracy: 0.6380 - val_loss: 0.7022 - val_accuracy: 0.5049
Epoch 3/500
161/161 - 19s - loss: 0.5217 - accuracy: 0.7540 - val_loss: 0.7105 - val_accuracy: 0.5312
Epoch 4/500
161/161 - 19s - loss: 0.4288 - accuracy: 0.8290 - val_loss: 0.7785 - val_accuracy: 0.5148
Epoch 5/500
161/161 - 19s - loss: 0.3448 - accuracy: 0.8737 - val_loss: 0.8926 - val_accuracy: 0.4852
Epoch 6/500
161/161 - 19s - loss: 0.2599 - accuracy: 0.9131 - val_loss: 1.0007 - val_accuracy: 0.4786
Epoch 7/500
161/161 - 19s - loss: 0.2043 - accuracy: 0.9366 - val_loss: 1.0329 - val_accuracy: 0.4967
Epoch 8/500
161/161 - 19s - loss: 0.1498 - accuracy: 0.9565 - val_loss: 1.1098 - val_accuracy: 0.5066
Epoch 9/500
161/161 - 19s - loss: 0.1196 - accuracy: 0.9642 - val_loss: 1.1874 - val_accuracy: 0.5115
Epoch 10/500
161/161 - 19s - loss: 0.1108 - accuracy: 0.9637 - val_loss: 1.2535 - val_accuracy: 0.4967
Epoch 11/500
161/161 - 19s - loss: 0.0831 - accuracy: 0.9767 - val_loss: 1.2503 - val_accuracy: 0.5247
Epoch 12/500
161/161 - 20s - loss: 0.0849 - accuracy: 0.9728 - val_loss: 1.2917 - val_accuracy: 0.5214
Epoch 13/500
161/161 - 20s - loss: 0.0940 - accuracy: 0.9681 - val_loss: 1.4804 - val_accuracy: 0.4704
========================================
save_weights
h5_weights/AO.po/onehot_resnet18.h5
========================================

end time >>> Sun Oct  3 00:25:47 2021

end time >>> Sun Oct  3 00:25:47 2021

end time >>> Sun Oct  3 00:25:47 2021

end time >>> Sun Oct  3 00:25:47 2021

end time >>> Sun Oct  3 00:25:47 2021












args.model = onehot_resnet18
time used = 266.8694236278534


