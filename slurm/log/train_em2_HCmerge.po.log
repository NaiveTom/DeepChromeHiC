************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 09:04:05 2021

begin time >>> Sun Oct  3 09:04:05 2021

begin time >>> Sun Oct  3 09:04:05 2021

begin time >>> Sun Oct  3 09:04:05 2021

begin time >>> Sun Oct  3 09:04:05 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_dense
args.type = train
args.name = HCmerge.po
args.length = 10001
===========================


-> h5_weights/HCmerge.po folder already exist. pass.
-> result/HCmerge.po/onehot_cnn_one_branch folder already exist. pass.
-> result/HCmerge.po/onehot_cnn_two_branch folder already exist. pass.
-> result/HCmerge.po/onehot_embedding_dense folder already exist. pass.
-> result/HCmerge.po/onehot_dense folder already exist. pass.
-> result/HCmerge.po/onehot_resnet18 folder already exist. pass.
-> result/HCmerge.po/onehot_resnet34 folder already exist. pass.
-> result/HCmerge.po/embedding_cnn_one_branch folder already exist. pass.
-> result/HCmerge.po/embedding_cnn_two_branch folder already exist. pass.
-> result/HCmerge.po/embedding_dense folder already exist. pass.
-> result/HCmerge.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/HCmerge.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
HCmerge.po
########################################

########################################
model_name
onehot_embedding_dense
########################################

Found 20140 images belonging to 2 classes.
Found 2488 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 20002, 5, 1, 8)    32776     
_________________________________________________________________
flatten (Flatten)            (None, 800080)            0         
_________________________________________________________________
batch_normalization (BatchNo (None, 800080)            3200320   
_________________________________________________________________
dense (Dense)                (None, 512)               409641472 
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_3 (Batch (None, 512)               2048      
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_3 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 413,671,754
Trainable params: 412,067,498
Non-trainable params: 1,604,256
_________________________________________________________________
Epoch 1/500
629/629 - 116s - loss: 0.6928 - accuracy: 0.6068 - val_loss: 0.8997 - val_accuracy: 0.5065
Epoch 2/500
629/629 - 117s - loss: 0.5386 - accuracy: 0.7426 - val_loss: 1.1116 - val_accuracy: 0.5588
Epoch 3/500
629/629 - 117s - loss: 0.4142 - accuracy: 0.8180 - val_loss: 1.4774 - val_accuracy: 0.5649
Epoch 4/500
629/629 - 115s - loss: 0.3246 - accuracy: 0.8654 - val_loss: 1.7796 - val_accuracy: 0.5609
Epoch 5/500
629/629 - 118s - loss: 0.2663 - accuracy: 0.8885 - val_loss: 1.8304 - val_accuracy: 0.5881
Epoch 6/500
629/629 - 120s - loss: 0.2157 - accuracy: 0.9154 - val_loss: 2.0969 - val_accuracy: 0.5795
Epoch 7/500
629/629 - 121s - loss: 0.1861 - accuracy: 0.9267 - val_loss: 2.1295 - val_accuracy: 0.5986
Epoch 8/500
629/629 - 121s - loss: 0.1586 - accuracy: 0.9380 - val_loss: 2.2191 - val_accuracy: 0.6015
Epoch 9/500
629/629 - 119s - loss: 0.1388 - accuracy: 0.9447 - val_loss: 2.2234 - val_accuracy: 0.6067
Epoch 10/500
629/629 - 121s - loss: 0.1216 - accuracy: 0.9525 - val_loss: 2.3199 - val_accuracy: 0.6096
Epoch 11/500
629/629 - 119s - loss: 0.1153 - accuracy: 0.9550 - val_loss: 2.2664 - val_accuracy: 0.6039
Epoch 12/500
629/629 - 119s - loss: 0.0970 - accuracy: 0.9638 - val_loss: 2.3281 - val_accuracy: 0.6136
Epoch 13/500
629/629 - 119s - loss: 0.0903 - accuracy: 0.9663 - val_loss: 2.2885 - val_accuracy: 0.6144
Epoch 14/500
629/629 - 118s - loss: 0.0794 - accuracy: 0.9704 - val_loss: 2.3580 - val_accuracy: 0.6096
Epoch 15/500
629/629 - 120s - loss: 0.0688 - accuracy: 0.9742 - val_loss: 2.3816 - val_accuracy: 0.6153
Epoch 16/500
629/629 - 119s - loss: 0.0733 - accuracy: 0.9725 - val_loss: 2.2852 - val_accuracy: 0.6189
Epoch 17/500
629/629 - 120s - loss: 0.0666 - accuracy: 0.9767 - val_loss: 2.2600 - val_accuracy: 0.6274
Epoch 18/500
629/629 - 119s - loss: 0.0598 - accuracy: 0.9801 - val_loss: 2.3039 - val_accuracy: 0.6205
Epoch 19/500
629/629 - 119s - loss: 0.0543 - accuracy: 0.9815 - val_loss: 2.2553 - val_accuracy: 0.6303
Epoch 20/500
629/629 - 121s - loss: 0.0518 - accuracy: 0.9820 - val_loss: 2.2433 - val_accuracy: 0.6343
Epoch 21/500
629/629 - 120s - loss: 0.0505 - accuracy: 0.9822 - val_loss: 2.2184 - val_accuracy: 0.6412
Epoch 22/500
629/629 - 120s - loss: 0.0469 - accuracy: 0.9827 - val_loss: 2.2069 - val_accuracy: 0.6376
Epoch 23/500
629/629 - 121s - loss: 0.0431 - accuracy: 0.9855 - val_loss: 2.2308 - val_accuracy: 0.6396
Epoch 24/500
629/629 - 128s - loss: 0.0415 - accuracy: 0.9854 - val_loss: 2.2851 - val_accuracy: 0.6441
Epoch 25/500
629/629 - 120s - loss: 0.0435 - accuracy: 0.9855 - val_loss: 2.2160 - val_accuracy: 0.6437
Epoch 26/500
629/629 - 117s - loss: 0.0393 - accuracy: 0.9861 - val_loss: 2.2328 - val_accuracy: 0.6412
Epoch 27/500
629/629 - 119s - loss: 0.0349 - accuracy: 0.9879 - val_loss: 2.3391 - val_accuracy: 0.6412
Epoch 28/500
629/629 - 124s - loss: 0.0391 - accuracy: 0.9874 - val_loss: 2.2798 - val_accuracy: 0.6469
Epoch 29/500
629/629 - 122s - loss: 0.0332 - accuracy: 0.9882 - val_loss: 2.2939 - val_accuracy: 0.6477
Epoch 30/500
629/629 - 119s - loss: 0.0320 - accuracy: 0.9891 - val_loss: 2.2284 - val_accuracy: 0.6530
Epoch 31/500
629/629 - 122s - loss: 0.0278 - accuracy: 0.9910 - val_loss: 2.3085 - val_accuracy: 0.6481
Epoch 32/500
629/629 - 121s - loss: 0.0316 - accuracy: 0.9902 - val_loss: 2.2179 - val_accuracy: 0.6469
Epoch 33/500
629/629 - 126s - loss: 0.0266 - accuracy: 0.9916 - val_loss: 2.3181 - val_accuracy: 0.6485
Epoch 34/500
629/629 - 118s - loss: 0.0230 - accuracy: 0.9914 - val_loss: 2.2769 - val_accuracy: 0.6514
Epoch 35/500
629/629 - 120s - loss: 0.0264 - accuracy: 0.9917 - val_loss: 2.3327 - val_accuracy: 0.6473
Epoch 36/500
629/629 - 120s - loss: 0.0252 - accuracy: 0.9921 - val_loss: 2.3518 - val_accuracy: 0.6494
Epoch 37/500
629/629 - 120s - loss: 0.0240 - accuracy: 0.9925 - val_loss: 2.3342 - val_accuracy: 0.6550
Epoch 38/500
629/629 - 118s - loss: 0.0222 - accuracy: 0.9927 - val_loss: 2.3300 - val_accuracy: 0.6595
Epoch 39/500
629/629 - 119s - loss: 0.0258 - accuracy: 0.9925 - val_loss: 2.2844 - val_accuracy: 0.6640
Epoch 40/500
629/629 - 119s - loss: 0.0212 - accuracy: 0.9930 - val_loss: 2.4086 - val_accuracy: 0.6591
Epoch 41/500
629/629 - 119s - loss: 0.0205 - accuracy: 0.9937 - val_loss: 2.3250 - val_accuracy: 0.6664
Epoch 42/500
629/629 - 124s - loss: 0.0174 - accuracy: 0.9944 - val_loss: 2.4663 - val_accuracy: 0.6567
Epoch 43/500
629/629 - 123s - loss: 0.0241 - accuracy: 0.9920 - val_loss: 2.3672 - val_accuracy: 0.6652
Epoch 44/500
629/629 - 124s - loss: 0.0248 - accuracy: 0.9921 - val_loss: 2.2960 - val_accuracy: 0.6769
Epoch 45/500
629/629 - 119s - loss: 0.0232 - accuracy: 0.9922 - val_loss: 2.3009 - val_accuracy: 0.6810
Epoch 46/500
629/629 - 116s - loss: 0.0169 - accuracy: 0.9945 - val_loss: 2.3195 - val_accuracy: 0.6790
Epoch 47/500
629/629 - 121s - loss: 0.0130 - accuracy: 0.9961 - val_loss: 2.4645 - val_accuracy: 0.6676
Epoch 48/500
629/629 - 117s - loss: 0.0173 - accuracy: 0.9947 - val_loss: 2.3522 - val_accuracy: 0.6753
Epoch 49/500
629/629 - 117s - loss: 0.0161 - accuracy: 0.9947 - val_loss: 2.4886 - val_accuracy: 0.6696
Epoch 50/500
629/629 - 119s - loss: 0.0150 - accuracy: 0.9952 - val_loss: 2.3909 - val_accuracy: 0.6765
Epoch 51/500
629/629 - 118s - loss: 0.0179 - accuracy: 0.9948 - val_loss: 2.4627 - val_accuracy: 0.6713
Epoch 52/500
629/629 - 119s - loss: 0.0128 - accuracy: 0.9957 - val_loss: 2.4742 - val_accuracy: 0.6761
Epoch 53/500
629/629 - 126s - loss: 0.0177 - accuracy: 0.9950 - val_loss: 2.4023 - val_accuracy: 0.6729
Epoch 54/500
629/629 - 124s - loss: 0.0170 - accuracy: 0.9945 - val_loss: 2.5173 - val_accuracy: 0.6656
Epoch 55/500
629/629 - 125s - loss: 0.0138 - accuracy: 0.9954 - val_loss: 2.3371 - val_accuracy: 0.6717
========================================
save_weights
h5_weights/HCmerge.po/onehot_embedding_dense.h5
========================================

end time >>> Sun Oct  3 10:54:40 2021

end time >>> Sun Oct  3 10:54:40 2021

end time >>> Sun Oct  3 10:54:40 2021

end time >>> Sun Oct  3 10:54:40 2021

end time >>> Sun Oct  3 10:54:40 2021












args.model = onehot_embedding_dense
time used = 6634.597033500671


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 10:54:41 2021

begin time >>> Sun Oct  3 10:54:41 2021

begin time >>> Sun Oct  3 10:54:41 2021

begin time >>> Sun Oct  3 10:54:41 2021

begin time >>> Sun Oct  3 10:54:41 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_one_branch
args.type = train
args.name = HCmerge.po
args.length = 10001
===========================


-> h5_weights/HCmerge.po folder already exist. pass.
-> result/HCmerge.po/onehot_cnn_one_branch folder already exist. pass.
-> result/HCmerge.po/onehot_cnn_two_branch folder already exist. pass.
-> result/HCmerge.po/onehot_embedding_dense folder already exist. pass.
-> result/HCmerge.po/onehot_dense folder already exist. pass.
-> result/HCmerge.po/onehot_resnet18 folder already exist. pass.
-> result/HCmerge.po/onehot_resnet34 folder already exist. pass.
-> result/HCmerge.po/embedding_cnn_one_branch folder already exist. pass.
-> result/HCmerge.po/embedding_cnn_two_branch folder already exist. pass.
-> result/HCmerge.po/embedding_dense folder already exist. pass.
-> result/HCmerge.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/HCmerge.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
HCmerge.po
########################################

########################################
model_name
onehot_embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
630/630 - 87s - loss: 0.8893 - accuracy: 0.5027 - val_loss: 0.6935 - val_accuracy: 0.5301
Epoch 2/500
630/630 - 87s - loss: 0.8581 - accuracy: 0.5094 - val_loss: 0.6970 - val_accuracy: 0.5301
Epoch 3/500
630/630 - 87s - loss: 0.8395 - accuracy: 0.5171 - val_loss: 0.6931 - val_accuracy: 0.5325
Epoch 4/500
630/630 - 87s - loss: 0.8301 - accuracy: 0.5187 - val_loss: 0.6904 - val_accuracy: 0.5430
Epoch 5/500
630/630 - 87s - loss: 0.8143 - accuracy: 0.5254 - val_loss: 0.6875 - val_accuracy: 0.5474
Epoch 6/500
630/630 - 87s - loss: 0.8128 - accuracy: 0.5280 - val_loss: 0.6848 - val_accuracy: 0.5538
Epoch 7/500
630/630 - 87s - loss: 0.8063 - accuracy: 0.5307 - val_loss: 0.6817 - val_accuracy: 0.5590
Epoch 8/500
630/630 - 87s - loss: 0.7953 - accuracy: 0.5345 - val_loss: 0.6788 - val_accuracy: 0.5647
Epoch 9/500
630/630 - 87s - loss: 0.7901 - accuracy: 0.5421 - val_loss: 0.6764 - val_accuracy: 0.5751
Epoch 10/500
630/630 - 86s - loss: 0.7837 - accuracy: 0.5469 - val_loss: 0.6743 - val_accuracy: 0.5791
Epoch 11/500
630/630 - 86s - loss: 0.7785 - accuracy: 0.5459 - val_loss: 0.6721 - val_accuracy: 0.5823
Epoch 12/500
630/630 - 86s - loss: 0.7667 - accuracy: 0.5584 - val_loss: 0.6701 - val_accuracy: 0.5871
Epoch 13/500
630/630 - 86s - loss: 0.7619 - accuracy: 0.5620 - val_loss: 0.6672 - val_accuracy: 0.5847
Epoch 14/500
630/630 - 87s - loss: 0.7592 - accuracy: 0.5595 - val_loss: 0.6646 - val_accuracy: 0.5904
Epoch 15/500
630/630 - 87s - loss: 0.7498 - accuracy: 0.5700 - val_loss: 0.6625 - val_accuracy: 0.5996
Epoch 16/500
630/630 - 86s - loss: 0.7435 - accuracy: 0.5741 - val_loss: 0.6603 - val_accuracy: 0.5976
Epoch 17/500
630/630 - 87s - loss: 0.7381 - accuracy: 0.5798 - val_loss: 0.6584 - val_accuracy: 0.5972
Epoch 18/500
630/630 - 87s - loss: 0.7208 - accuracy: 0.5886 - val_loss: 0.6550 - val_accuracy: 0.6036
Epoch 19/500
630/630 - 86s - loss: 0.7206 - accuracy: 0.5912 - val_loss: 0.6532 - val_accuracy: 0.6028
Epoch 20/500
630/630 - 87s - loss: 0.7143 - accuracy: 0.5936 - val_loss: 0.6506 - val_accuracy: 0.6080
Epoch 21/500
630/630 - 86s - loss: 0.7107 - accuracy: 0.5990 - val_loss: 0.6487 - val_accuracy: 0.6120
Epoch 22/500
630/630 - 87s - loss: 0.6954 - accuracy: 0.6113 - val_loss: 0.6474 - val_accuracy: 0.6124
Epoch 23/500
630/630 - 87s - loss: 0.6869 - accuracy: 0.6201 - val_loss: 0.6452 - val_accuracy: 0.6145
Epoch 24/500
630/630 - 87s - loss: 0.6813 - accuracy: 0.6239 - val_loss: 0.6432 - val_accuracy: 0.6169
Epoch 25/500
630/630 - 87s - loss: 0.6713 - accuracy: 0.6311 - val_loss: 0.6414 - val_accuracy: 0.6177
Epoch 26/500
630/630 - 87s - loss: 0.6618 - accuracy: 0.6387 - val_loss: 0.6385 - val_accuracy: 0.6209
Epoch 27/500
630/630 - 87s - loss: 0.6610 - accuracy: 0.6397 - val_loss: 0.6367 - val_accuracy: 0.6277
Epoch 28/500
630/630 - 87s - loss: 0.6452 - accuracy: 0.6514 - val_loss: 0.6354 - val_accuracy: 0.6285
Epoch 29/500
630/630 - 88s - loss: 0.6361 - accuracy: 0.6635 - val_loss: 0.6331 - val_accuracy: 0.6337
Epoch 30/500
630/630 - 87s - loss: 0.6354 - accuracy: 0.6626 - val_loss: 0.6329 - val_accuracy: 0.6337
Epoch 31/500
630/630 - 87s - loss: 0.6181 - accuracy: 0.6763 - val_loss: 0.6315 - val_accuracy: 0.6341
Epoch 32/500
630/630 - 88s - loss: 0.6055 - accuracy: 0.6830 - val_loss: 0.6297 - val_accuracy: 0.6361
Epoch 33/500
630/630 - 88s - loss: 0.5968 - accuracy: 0.6867 - val_loss: 0.6291 - val_accuracy: 0.6390
Epoch 34/500
630/630 - 87s - loss: 0.5849 - accuracy: 0.6983 - val_loss: 0.6277 - val_accuracy: 0.6462
Epoch 35/500
630/630 - 88s - loss: 0.5839 - accuracy: 0.7022 - val_loss: 0.6278 - val_accuracy: 0.6446
Epoch 36/500
630/630 - 87s - loss: 0.5775 - accuracy: 0.7048 - val_loss: 0.6288 - val_accuracy: 0.6414
Epoch 37/500
630/630 - 87s - loss: 0.5653 - accuracy: 0.7122 - val_loss: 0.6290 - val_accuracy: 0.6442
Epoch 38/500
630/630 - 87s - loss: 0.5552 - accuracy: 0.7203 - val_loss: 0.6281 - val_accuracy: 0.6470
Epoch 39/500
630/630 - 87s - loss: 0.5411 - accuracy: 0.7317 - val_loss: 0.6287 - val_accuracy: 0.6478
Epoch 40/500
630/630 - 87s - loss: 0.5309 - accuracy: 0.7374 - val_loss: 0.6287 - val_accuracy: 0.6538
Epoch 41/500
630/630 - 87s - loss: 0.5268 - accuracy: 0.7435 - val_loss: 0.6295 - val_accuracy: 0.6538
Epoch 42/500
630/630 - 87s - loss: 0.5198 - accuracy: 0.7487 - val_loss: 0.6320 - val_accuracy: 0.6526
Epoch 43/500
630/630 - 87s - loss: 0.5060 - accuracy: 0.7562 - val_loss: 0.6323 - val_accuracy: 0.6538
Epoch 44/500
630/630 - 87s - loss: 0.5016 - accuracy: 0.7613 - val_loss: 0.6314 - val_accuracy: 0.6550
Epoch 45/500
630/630 - 87s - loss: 0.4850 - accuracy: 0.7687 - val_loss: 0.6355 - val_accuracy: 0.6542
Epoch 46/500
630/630 - 87s - loss: 0.4748 - accuracy: 0.7742 - val_loss: 0.6354 - val_accuracy: 0.6590
Epoch 47/500
630/630 - 87s - loss: 0.4719 - accuracy: 0.7767 - val_loss: 0.6399 - val_accuracy: 0.6546
Epoch 48/500
630/630 - 87s - loss: 0.4616 - accuracy: 0.7811 - val_loss: 0.6434 - val_accuracy: 0.6498
Epoch 49/500
630/630 - 87s - loss: 0.4564 - accuracy: 0.7868 - val_loss: 0.6434 - val_accuracy: 0.6574
Epoch 50/500
630/630 - 87s - loss: 0.4576 - accuracy: 0.7889 - val_loss: 0.6442 - val_accuracy: 0.6578
Epoch 51/500
630/630 - 87s - loss: 0.4409 - accuracy: 0.7940 - val_loss: 0.6473 - val_accuracy: 0.6635
Epoch 52/500
630/630 - 87s - loss: 0.4332 - accuracy: 0.8019 - val_loss: 0.6506 - val_accuracy: 0.6598
Epoch 53/500
630/630 - 87s - loss: 0.4205 - accuracy: 0.8076 - val_loss: 0.6529 - val_accuracy: 0.6602
Epoch 54/500
630/630 - 87s - loss: 0.4210 - accuracy: 0.8116 - val_loss: 0.6592 - val_accuracy: 0.6570
Epoch 55/500
630/630 - 87s - loss: 0.4122 - accuracy: 0.8160 - val_loss: 0.6602 - val_accuracy: 0.6631
Epoch 56/500
630/630 - 87s - loss: 0.3973 - accuracy: 0.8225 - val_loss: 0.6661 - val_accuracy: 0.6627
Epoch 57/500
630/630 - 87s - loss: 0.3941 - accuracy: 0.8241 - val_loss: 0.6697 - val_accuracy: 0.6614
Epoch 58/500
630/630 - 87s - loss: 0.3876 - accuracy: 0.8261 - val_loss: 0.6688 - val_accuracy: 0.6679
Epoch 59/500
630/630 - 87s - loss: 0.3786 - accuracy: 0.8322 - val_loss: 0.6736 - val_accuracy: 0.6659
Epoch 60/500
630/630 - 87s - loss: 0.3743 - accuracy: 0.8309 - val_loss: 0.6760 - val_accuracy: 0.6719
Epoch 61/500
630/630 - 87s - loss: 0.3679 - accuracy: 0.8384 - val_loss: 0.6807 - val_accuracy: 0.6691
Epoch 62/500
630/630 - 87s - loss: 0.3592 - accuracy: 0.8426 - val_loss: 0.6842 - val_accuracy: 0.6699
Epoch 63/500
630/630 - 87s - loss: 0.3548 - accuracy: 0.8408 - val_loss: 0.6863 - val_accuracy: 0.6735
Epoch 64/500
630/630 - 87s - loss: 0.3455 - accuracy: 0.8496 - val_loss: 0.6940 - val_accuracy: 0.6687
Epoch 65/500
630/630 - 87s - loss: 0.3449 - accuracy: 0.8495 - val_loss: 0.6964 - val_accuracy: 0.6691
Epoch 66/500
630/630 - 87s - loss: 0.3351 - accuracy: 0.8550 - val_loss: 0.6994 - val_accuracy: 0.6719
Epoch 67/500
630/630 - 87s - loss: 0.3263 - accuracy: 0.8583 - val_loss: 0.7047 - val_accuracy: 0.6715
Epoch 68/500
630/630 - 87s - loss: 0.3185 - accuracy: 0.8612 - val_loss: 0.7091 - val_accuracy: 0.6727
Epoch 69/500
630/630 - 87s - loss: 0.3082 - accuracy: 0.8652 - val_loss: 0.7146 - val_accuracy: 0.6723
Epoch 70/500
630/630 - 87s - loss: 0.3046 - accuracy: 0.8715 - val_loss: 0.7192 - val_accuracy: 0.6723
Epoch 71/500
630/630 - 87s - loss: 0.2987 - accuracy: 0.8706 - val_loss: 0.7212 - val_accuracy: 0.6731
Epoch 72/500
630/630 - 87s - loss: 0.2955 - accuracy: 0.8722 - val_loss: 0.7275 - val_accuracy: 0.6735
Epoch 73/500
630/630 - 87s - loss: 0.2840 - accuracy: 0.8795 - val_loss: 0.7340 - val_accuracy: 0.6747
Epoch 74/500
630/630 - 87s - loss: 0.2827 - accuracy: 0.8842 - val_loss: 0.7352 - val_accuracy: 0.6751
Epoch 75/500
630/630 - 87s - loss: 0.2787 - accuracy: 0.8844 - val_loss: 0.7411 - val_accuracy: 0.6747
Epoch 76/500
630/630 - 87s - loss: 0.2804 - accuracy: 0.8822 - val_loss: 0.7481 - val_accuracy: 0.6739
Epoch 77/500
630/630 - 87s - loss: 0.2663 - accuracy: 0.8871 - val_loss: 0.7533 - val_accuracy: 0.6763
Epoch 78/500
630/630 - 87s - loss: 0.2568 - accuracy: 0.8927 - val_loss: 0.7580 - val_accuracy: 0.6747
Epoch 79/500
630/630 - 87s - loss: 0.2544 - accuracy: 0.8933 - val_loss: 0.7636 - val_accuracy: 0.6739
Epoch 80/500
630/630 - 87s - loss: 0.2542 - accuracy: 0.8945 - val_loss: 0.7689 - val_accuracy: 0.6739
Epoch 81/500
630/630 - 87s - loss: 0.2455 - accuracy: 0.8958 - val_loss: 0.7748 - val_accuracy: 0.6743
Epoch 82/500
630/630 - 87s - loss: 0.2373 - accuracy: 0.9013 - val_loss: 0.7783 - val_accuracy: 0.6727
Epoch 83/500
630/630 - 87s - loss: 0.2312 - accuracy: 0.9039 - val_loss: 0.7817 - val_accuracy: 0.6751
Epoch 84/500
630/630 - 87s - loss: 0.2327 - accuracy: 0.9044 - val_loss: 0.7899 - val_accuracy: 0.6779
Epoch 85/500
630/630 - 87s - loss: 0.2203 - accuracy: 0.9103 - val_loss: 0.7920 - val_accuracy: 0.6743
Epoch 86/500
630/630 - 87s - loss: 0.2186 - accuracy: 0.9114 - val_loss: 0.8018 - val_accuracy: 0.6715
Epoch 87/500
630/630 - 87s - loss: 0.2106 - accuracy: 0.9158 - val_loss: 0.8050 - val_accuracy: 0.6735
Epoch 88/500
630/630 - 87s - loss: 0.2186 - accuracy: 0.9101 - val_loss: 0.8090 - val_accuracy: 0.6759
Epoch 89/500
630/630 - 87s - loss: 0.2050 - accuracy: 0.9166 - val_loss: 0.8167 - val_accuracy: 0.6711
Epoch 90/500
630/630 - 87s - loss: 0.2041 - accuracy: 0.9144 - val_loss: 0.8220 - val_accuracy: 0.6727
Epoch 91/500
630/630 - 87s - loss: 0.2021 - accuracy: 0.9184 - val_loss: 0.8244 - val_accuracy: 0.6743
Epoch 92/500
630/630 - 87s - loss: 0.1955 - accuracy: 0.9184 - val_loss: 0.8314 - val_accuracy: 0.6711
Epoch 93/500
630/630 - 87s - loss: 0.1886 - accuracy: 0.9244 - val_loss: 0.8356 - val_accuracy: 0.6747
Epoch 94/500
630/630 - 87s - loss: 0.1818 - accuracy: 0.9275 - val_loss: 0.8440 - val_accuracy: 0.6731
Epoch 95/500
630/630 - 87s - loss: 0.1873 - accuracy: 0.9242 - val_loss: 0.8501 - val_accuracy: 0.6723
Epoch 96/500
630/630 - 87s - loss: 0.1730 - accuracy: 0.9314 - val_loss: 0.8549 - val_accuracy: 0.6751
Epoch 97/500
630/630 - 87s - loss: 0.1790 - accuracy: 0.9289 - val_loss: 0.8617 - val_accuracy: 0.6751
Epoch 98/500
630/630 - 87s - loss: 0.1738 - accuracy: 0.9308 - val_loss: 0.8649 - val_accuracy: 0.6739
Epoch 99/500
630/630 - 87s - loss: 0.1767 - accuracy: 0.9280 - val_loss: 0.8694 - val_accuracy: 0.6707
Epoch 100/500
630/630 - 87s - loss: 0.1769 - accuracy: 0.9299 - val_loss: 0.8771 - val_accuracy: 0.6727
Epoch 101/500
630/630 - 87s - loss: 0.1644 - accuracy: 0.9350 - val_loss: 0.8811 - val_accuracy: 0.6723
Epoch 102/500
630/630 - 87s - loss: 0.1597 - accuracy: 0.9386 - val_loss: 0.8871 - val_accuracy: 0.6763
Epoch 103/500
630/630 - 87s - loss: 0.1569 - accuracy: 0.9387 - val_loss: 0.8915 - val_accuracy: 0.6703
Epoch 104/500
630/630 - 87s - loss: 0.1519 - accuracy: 0.9411 - val_loss: 0.8959 - val_accuracy: 0.6711
========================================
save_weights
h5_weights/HCmerge.po/onehot_embedding_cnn_one_branch.h5
========================================

end time >>> Sun Oct  3 13:26:10 2021

end time >>> Sun Oct  3 13:26:10 2021

end time >>> Sun Oct  3 13:26:10 2021

end time >>> Sun Oct  3 13:26:10 2021

end time >>> Sun Oct  3 13:26:10 2021












args.model = onehot_embedding_cnn_one_branch
time used = 9089.133280277252


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 13:26:11 2021

begin time >>> Sun Oct  3 13:26:11 2021

begin time >>> Sun Oct  3 13:26:11 2021

begin time >>> Sun Oct  3 13:26:11 2021

begin time >>> Sun Oct  3 13:26:11 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_two_branch
args.type = train
args.name = HCmerge.po
args.length = 10001
===========================


-> h5_weights/HCmerge.po folder already exist. pass.
-> result/HCmerge.po/onehot_cnn_one_branch folder already exist. pass.
-> result/HCmerge.po/onehot_cnn_two_branch folder already exist. pass.
-> result/HCmerge.po/onehot_embedding_dense folder already exist. pass.
-> result/HCmerge.po/onehot_dense folder already exist. pass.
-> result/HCmerge.po/onehot_resnet18 folder already exist. pass.
-> result/HCmerge.po/onehot_resnet34 folder already exist. pass.
-> result/HCmerge.po/embedding_cnn_one_branch folder already exist. pass.
-> result/HCmerge.po/embedding_cnn_two_branch folder already exist. pass.
-> result/HCmerge.po/embedding_dense folder already exist. pass.
-> result/HCmerge.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/HCmerge.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
HCmerge.po
########################################

########################################
model_name
onehot_embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
630/630 - 87s - loss: 0.8941 - accuracy: 0.5009 - val_loss: 0.7271 - val_accuracy: 0.5032
Epoch 2/500
630/630 - 87s - loss: 0.8643 - accuracy: 0.5075 - val_loss: 0.7094 - val_accuracy: 0.5185
Epoch 3/500
630/630 - 87s - loss: 0.8503 - accuracy: 0.5095 - val_loss: 0.7023 - val_accuracy: 0.5313
Epoch 4/500
630/630 - 87s - loss: 0.8380 - accuracy: 0.5157 - val_loss: 0.6963 - val_accuracy: 0.5341
Epoch 5/500
630/630 - 87s - loss: 0.8221 - accuracy: 0.5247 - val_loss: 0.6926 - val_accuracy: 0.5482
Epoch 6/500
630/630 - 87s - loss: 0.8106 - accuracy: 0.5329 - val_loss: 0.6882 - val_accuracy: 0.5562
Epoch 7/500
630/630 - 87s - loss: 0.8053 - accuracy: 0.5351 - val_loss: 0.6859 - val_accuracy: 0.5586
Epoch 8/500
630/630 - 87s - loss: 0.7913 - accuracy: 0.5461 - val_loss: 0.6823 - val_accuracy: 0.5679
Epoch 9/500
630/630 - 87s - loss: 0.7936 - accuracy: 0.5365 - val_loss: 0.6794 - val_accuracy: 0.5695
Epoch 10/500
630/630 - 86s - loss: 0.7787 - accuracy: 0.5527 - val_loss: 0.6764 - val_accuracy: 0.5683
Epoch 11/500
630/630 - 86s - loss: 0.7830 - accuracy: 0.5468 - val_loss: 0.6742 - val_accuracy: 0.5827
Epoch 12/500
630/630 - 86s - loss: 0.7623 - accuracy: 0.5598 - val_loss: 0.6715 - val_accuracy: 0.5839
Epoch 13/500
630/630 - 86s - loss: 0.7628 - accuracy: 0.5632 - val_loss: 0.6692 - val_accuracy: 0.5855
Epoch 14/500
630/630 - 86s - loss: 0.7527 - accuracy: 0.5665 - val_loss: 0.6671 - val_accuracy: 0.5904
Epoch 15/500
630/630 - 86s - loss: 0.7399 - accuracy: 0.5779 - val_loss: 0.6639 - val_accuracy: 0.5964
Epoch 16/500
630/630 - 86s - loss: 0.7433 - accuracy: 0.5773 - val_loss: 0.6616 - val_accuracy: 0.6000
Epoch 17/500
630/630 - 86s - loss: 0.7367 - accuracy: 0.5813 - val_loss: 0.6600 - val_accuracy: 0.6008
Epoch 18/500
630/630 - 87s - loss: 0.7284 - accuracy: 0.5837 - val_loss: 0.6570 - val_accuracy: 0.6096
Epoch 19/500
630/630 - 86s - loss: 0.7214 - accuracy: 0.5913 - val_loss: 0.6549 - val_accuracy: 0.6145
Epoch 20/500
630/630 - 86s - loss: 0.7133 - accuracy: 0.5988 - val_loss: 0.6521 - val_accuracy: 0.6177
Epoch 21/500
630/630 - 86s - loss: 0.7010 - accuracy: 0.6066 - val_loss: 0.6503 - val_accuracy: 0.6209
Epoch 22/500
630/630 - 86s - loss: 0.6880 - accuracy: 0.6190 - val_loss: 0.6473 - val_accuracy: 0.6257
Epoch 23/500
630/630 - 86s - loss: 0.6828 - accuracy: 0.6238 - val_loss: 0.6453 - val_accuracy: 0.6257
Epoch 24/500
630/630 - 86s - loss: 0.6732 - accuracy: 0.6304 - val_loss: 0.6432 - val_accuracy: 0.6273
Epoch 25/500
630/630 - 86s - loss: 0.6617 - accuracy: 0.6386 - val_loss: 0.6408 - val_accuracy: 0.6337
Epoch 26/500
630/630 - 86s - loss: 0.6536 - accuracy: 0.6472 - val_loss: 0.6392 - val_accuracy: 0.6357
Epoch 27/500
630/630 - 86s - loss: 0.6381 - accuracy: 0.6580 - val_loss: 0.6373 - val_accuracy: 0.6450
Epoch 28/500
630/630 - 86s - loss: 0.6352 - accuracy: 0.6592 - val_loss: 0.6352 - val_accuracy: 0.6454
Epoch 29/500
630/630 - 87s - loss: 0.6254 - accuracy: 0.6734 - val_loss: 0.6342 - val_accuracy: 0.6482
Epoch 30/500
630/630 - 86s - loss: 0.6113 - accuracy: 0.6817 - val_loss: 0.6329 - val_accuracy: 0.6526
Epoch 31/500
630/630 - 86s - loss: 0.5989 - accuracy: 0.6866 - val_loss: 0.6321 - val_accuracy: 0.6522
Epoch 32/500
630/630 - 86s - loss: 0.5904 - accuracy: 0.6933 - val_loss: 0.6316 - val_accuracy: 0.6546
Epoch 33/500
630/630 - 86s - loss: 0.5778 - accuracy: 0.7051 - val_loss: 0.6309 - val_accuracy: 0.6578
Epoch 34/500
630/630 - 86s - loss: 0.5739 - accuracy: 0.7073 - val_loss: 0.6312 - val_accuracy: 0.6554
Epoch 35/500
630/630 - 86s - loss: 0.5677 - accuracy: 0.7102 - val_loss: 0.6296 - val_accuracy: 0.6618
Epoch 36/500
630/630 - 86s - loss: 0.5528 - accuracy: 0.7228 - val_loss: 0.6313 - val_accuracy: 0.6582
Epoch 37/500
630/630 - 86s - loss: 0.5491 - accuracy: 0.7240 - val_loss: 0.6314 - val_accuracy: 0.6643
Epoch 38/500
630/630 - 86s - loss: 0.5322 - accuracy: 0.7397 - val_loss: 0.6321 - val_accuracy: 0.6622
Epoch 39/500
630/630 - 86s - loss: 0.5276 - accuracy: 0.7406 - val_loss: 0.6322 - val_accuracy: 0.6639
Epoch 40/500
630/630 - 86s - loss: 0.5119 - accuracy: 0.7509 - val_loss: 0.6327 - val_accuracy: 0.6651
Epoch 41/500
630/630 - 86s - loss: 0.5030 - accuracy: 0.7575 - val_loss: 0.6346 - val_accuracy: 0.6683
Epoch 42/500
630/630 - 86s - loss: 0.4925 - accuracy: 0.7623 - val_loss: 0.6355 - val_accuracy: 0.6699
Epoch 43/500
630/630 - 86s - loss: 0.4831 - accuracy: 0.7700 - val_loss: 0.6368 - val_accuracy: 0.6699
Epoch 44/500
630/630 - 86s - loss: 0.4746 - accuracy: 0.7748 - val_loss: 0.6383 - val_accuracy: 0.6727
Epoch 45/500
630/630 - 86s - loss: 0.4671 - accuracy: 0.7809 - val_loss: 0.6398 - val_accuracy: 0.6755
Epoch 46/500
630/630 - 86s - loss: 0.4592 - accuracy: 0.7854 - val_loss: 0.6415 - val_accuracy: 0.6763
Epoch 47/500
630/630 - 86s - loss: 0.4459 - accuracy: 0.7936 - val_loss: 0.6436 - val_accuracy: 0.6759
Epoch 48/500
630/630 - 86s - loss: 0.4394 - accuracy: 0.7969 - val_loss: 0.6455 - val_accuracy: 0.6771
Epoch 49/500
630/630 - 86s - loss: 0.4265 - accuracy: 0.8031 - val_loss: 0.6498 - val_accuracy: 0.6803
Epoch 50/500
630/630 - 86s - loss: 0.4207 - accuracy: 0.8111 - val_loss: 0.6519 - val_accuracy: 0.6807
Epoch 51/500
630/630 - 86s - loss: 0.4150 - accuracy: 0.8115 - val_loss: 0.6528 - val_accuracy: 0.6767
Epoch 52/500
630/630 - 86s - loss: 0.4021 - accuracy: 0.8187 - val_loss: 0.6550 - val_accuracy: 0.6795
Epoch 53/500
630/630 - 86s - loss: 0.3985 - accuracy: 0.8204 - val_loss: 0.6576 - val_accuracy: 0.6771
Epoch 54/500
630/630 - 86s - loss: 0.3855 - accuracy: 0.8298 - val_loss: 0.6629 - val_accuracy: 0.6727
Epoch 55/500
630/630 - 86s - loss: 0.3849 - accuracy: 0.8278 - val_loss: 0.6672 - val_accuracy: 0.6755
Epoch 56/500
630/630 - 86s - loss: 0.3785 - accuracy: 0.8337 - val_loss: 0.6716 - val_accuracy: 0.6735
Epoch 57/500
630/630 - 86s - loss: 0.3659 - accuracy: 0.8373 - val_loss: 0.6744 - val_accuracy: 0.6739
Epoch 58/500
630/630 - 86s - loss: 0.3562 - accuracy: 0.8429 - val_loss: 0.6784 - val_accuracy: 0.6767
Epoch 59/500
630/630 - 86s - loss: 0.3556 - accuracy: 0.8456 - val_loss: 0.6818 - val_accuracy: 0.6775
Epoch 60/500
630/630 - 86s - loss: 0.3501 - accuracy: 0.8465 - val_loss: 0.6851 - val_accuracy: 0.6775
Epoch 61/500
630/630 - 86s - loss: 0.3317 - accuracy: 0.8569 - val_loss: 0.6894 - val_accuracy: 0.6763
Epoch 62/500
630/630 - 86s - loss: 0.3253 - accuracy: 0.8585 - val_loss: 0.6953 - val_accuracy: 0.6763
Epoch 63/500
630/630 - 86s - loss: 0.3253 - accuracy: 0.8584 - val_loss: 0.6984 - val_accuracy: 0.6791
Epoch 64/500
630/630 - 86s - loss: 0.3153 - accuracy: 0.8644 - val_loss: 0.7028 - val_accuracy: 0.6771
Epoch 65/500
630/630 - 86s - loss: 0.3087 - accuracy: 0.8671 - val_loss: 0.7075 - val_accuracy: 0.6767
Epoch 66/500
630/630 - 86s - loss: 0.3016 - accuracy: 0.8725 - val_loss: 0.7120 - val_accuracy: 0.6767
Epoch 67/500
630/630 - 86s - loss: 0.2914 - accuracy: 0.8765 - val_loss: 0.7180 - val_accuracy: 0.6767
Epoch 68/500
630/630 - 86s - loss: 0.2799 - accuracy: 0.8796 - val_loss: 0.7254 - val_accuracy: 0.6767
Epoch 69/500
630/630 - 86s - loss: 0.2765 - accuracy: 0.8839 - val_loss: 0.7279 - val_accuracy: 0.6771
Epoch 70/500
630/630 - 86s - loss: 0.2680 - accuracy: 0.8870 - val_loss: 0.7359 - val_accuracy: 0.6751
========================================
save_weights
h5_weights/HCmerge.po/onehot_embedding_cnn_two_branch.h5
========================================

end time >>> Sun Oct  3 15:07:22 2021

end time >>> Sun Oct  3 15:07:22 2021

end time >>> Sun Oct  3 15:07:22 2021

end time >>> Sun Oct  3 15:07:22 2021

end time >>> Sun Oct  3 15:07:22 2021












args.model = onehot_embedding_cnn_two_branch
time used = 6071.323788404465


