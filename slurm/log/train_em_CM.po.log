************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 00:27:45 2021

begin time >>> Sun Oct  3 00:27:45 2021

begin time >>> Sun Oct  3 00:27:45 2021

begin time >>> Sun Oct  3 00:27:45 2021

begin time >>> Sun Oct  3 00:27:45 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_dense
args.type = train
args.name = CM.po
args.length = 10001
===========================


-> h5_weights/CM.po folder already exist. pass.
-> result/CM.po/onehot_cnn_one_branch folder already exist. pass.
-> result/CM.po/onehot_cnn_two_branch folder already exist. pass.
-> result/CM.po/onehot_embedding_dense folder already exist. pass.
-> result/CM.po/onehot_dense folder already exist. pass.
-> result/CM.po/onehot_resnet18 folder already exist. pass.
-> result/CM.po/onehot_resnet34 folder already exist. pass.
-> result/CM.po/embedding_cnn_one_branch folder already exist. pass.
-> result/CM.po/embedding_cnn_two_branch folder already exist. pass.
-> result/CM.po/embedding_dense folder already exist. pass.
-> result/CM.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/CM.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
CM.po
########################################

########################################
model_name
embedding_dense
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 626, 64)      409664      concatenate[0][0]                
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, 38, 64)       0           conv1d[0][0]                     
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 38, 64)       256         max_pooling1d[0][0]              
__________________________________________________________________________________________________
flatten (Flatten)               (None, 2432)         0           batch_normalization[0][0]        
__________________________________________________________________________________________________
dropout (Dropout)               (None, 2432)         0           flatten[0][0]                    
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          1245696     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation (Activation)         (None, 512)          0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation[0][0]                 
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 512)          0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_1[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 512)          2048        dense_2[0][0]                    
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 512)          0           activation_2[0][0]               
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            513         dropout_3[0][0]                  
==================================================================================================
Total params: 3,006,985
Trainable params: 3,003,785
Non-trainable params: 3,200
__________________________________________________________________________________________________
Epoch 1/500
231/231 - 30s - loss: 0.8963 - accuracy: 0.4945 - val_loss: 0.6943 - val_accuracy: 0.5137
Epoch 2/500
231/231 - 29s - loss: 0.8810 - accuracy: 0.4949 - val_loss: 0.6950 - val_accuracy: 0.5115
Epoch 3/500
231/231 - 29s - loss: 0.8844 - accuracy: 0.4988 - val_loss: 0.6950 - val_accuracy: 0.5214
Epoch 4/500
231/231 - 29s - loss: 0.8930 - accuracy: 0.4899 - val_loss: 0.6950 - val_accuracy: 0.5126
Epoch 5/500
231/231 - 29s - loss: 0.8750 - accuracy: 0.4942 - val_loss: 0.6942 - val_accuracy: 0.5104
Epoch 6/500
231/231 - 29s - loss: 0.8566 - accuracy: 0.5050 - val_loss: 0.6937 - val_accuracy: 0.5137
Epoch 7/500
231/231 - 29s - loss: 0.8704 - accuracy: 0.5014 - val_loss: 0.6937 - val_accuracy: 0.5181
Epoch 8/500
231/231 - 29s - loss: 0.8576 - accuracy: 0.5026 - val_loss: 0.6931 - val_accuracy: 0.5225
Epoch 9/500
231/231 - 29s - loss: 0.8479 - accuracy: 0.5136 - val_loss: 0.6930 - val_accuracy: 0.5214
Epoch 10/500
231/231 - 30s - loss: 0.8497 - accuracy: 0.5082 - val_loss: 0.6929 - val_accuracy: 0.5137
Epoch 11/500
231/231 - 29s - loss: 0.8623 - accuracy: 0.4978 - val_loss: 0.6918 - val_accuracy: 0.5126
Epoch 12/500
231/231 - 29s - loss: 0.8533 - accuracy: 0.4978 - val_loss: 0.6917 - val_accuracy: 0.5170
Epoch 13/500
231/231 - 29s - loss: 0.8328 - accuracy: 0.5107 - val_loss: 0.6920 - val_accuracy: 0.5126
Epoch 14/500
231/231 - 29s - loss: 0.8496 - accuracy: 0.5069 - val_loss: 0.6919 - val_accuracy: 0.5159
Epoch 15/500
231/231 - 29s - loss: 0.8311 - accuracy: 0.5141 - val_loss: 0.6915 - val_accuracy: 0.5181
Epoch 16/500
231/231 - 29s - loss: 0.8481 - accuracy: 0.5024 - val_loss: 0.6910 - val_accuracy: 0.5181
Epoch 17/500
231/231 - 29s - loss: 0.8475 - accuracy: 0.4990 - val_loss: 0.6903 - val_accuracy: 0.5258
Epoch 18/500
231/231 - 29s - loss: 0.8452 - accuracy: 0.5039 - val_loss: 0.6902 - val_accuracy: 0.5236
Epoch 19/500
231/231 - 29s - loss: 0.8372 - accuracy: 0.5092 - val_loss: 0.6899 - val_accuracy: 0.5269
Epoch 20/500
231/231 - 29s - loss: 0.8251 - accuracy: 0.5192 - val_loss: 0.6899 - val_accuracy: 0.5258
Epoch 21/500
231/231 - 30s - loss: 0.8290 - accuracy: 0.5082 - val_loss: 0.6896 - val_accuracy: 0.5236
Epoch 22/500
231/231 - 30s - loss: 0.8375 - accuracy: 0.5089 - val_loss: 0.6900 - val_accuracy: 0.5214
Epoch 23/500
231/231 - 30s - loss: 0.8241 - accuracy: 0.5075 - val_loss: 0.6897 - val_accuracy: 0.5181
Epoch 24/500
231/231 - 30s - loss: 0.8179 - accuracy: 0.5184 - val_loss: 0.6885 - val_accuracy: 0.5159
Epoch 25/500
231/231 - 30s - loss: 0.8202 - accuracy: 0.5165 - val_loss: 0.6891 - val_accuracy: 0.5236
Epoch 26/500
231/231 - 30s - loss: 0.8349 - accuracy: 0.4982 - val_loss: 0.6892 - val_accuracy: 0.5203
Epoch 27/500
231/231 - 30s - loss: 0.8268 - accuracy: 0.5100 - val_loss: 0.6889 - val_accuracy: 0.5236
Epoch 28/500
231/231 - 30s - loss: 0.8235 - accuracy: 0.5112 - val_loss: 0.6889 - val_accuracy: 0.5313
Epoch 29/500
231/231 - 30s - loss: 0.8173 - accuracy: 0.5107 - val_loss: 0.6877 - val_accuracy: 0.5280
Epoch 30/500
231/231 - 30s - loss: 0.8132 - accuracy: 0.5166 - val_loss: 0.6881 - val_accuracy: 0.5269
Epoch 31/500
231/231 - 30s - loss: 0.8060 - accuracy: 0.5219 - val_loss: 0.6870 - val_accuracy: 0.5302
Epoch 32/500
231/231 - 30s - loss: 0.8170 - accuracy: 0.5127 - val_loss: 0.6884 - val_accuracy: 0.5335
Epoch 33/500
231/231 - 30s - loss: 0.8296 - accuracy: 0.5055 - val_loss: 0.6877 - val_accuracy: 0.5291
Epoch 34/500
231/231 - 30s - loss: 0.8138 - accuracy: 0.5210 - val_loss: 0.6867 - val_accuracy: 0.5313
Epoch 35/500
231/231 - 30s - loss: 0.8158 - accuracy: 0.5160 - val_loss: 0.6868 - val_accuracy: 0.5302
Epoch 36/500
231/231 - 30s - loss: 0.8096 - accuracy: 0.5173 - val_loss: 0.6863 - val_accuracy: 0.5313
Epoch 37/500
231/231 - 30s - loss: 0.8139 - accuracy: 0.5174 - val_loss: 0.6869 - val_accuracy: 0.5346
Epoch 38/500
231/231 - 30s - loss: 0.8078 - accuracy: 0.5214 - val_loss: 0.6872 - val_accuracy: 0.5401
Epoch 39/500
231/231 - 30s - loss: 0.8009 - accuracy: 0.5240 - val_loss: 0.6872 - val_accuracy: 0.5368
Epoch 40/500
231/231 - 30s - loss: 0.8184 - accuracy: 0.5100 - val_loss: 0.6870 - val_accuracy: 0.5335
Epoch 41/500
231/231 - 30s - loss: 0.8053 - accuracy: 0.5233 - val_loss: 0.6870 - val_accuracy: 0.5379
Epoch 42/500
231/231 - 30s - loss: 0.8116 - accuracy: 0.5113 - val_loss: 0.6867 - val_accuracy: 0.5335
Epoch 43/500
231/231 - 30s - loss: 0.8052 - accuracy: 0.5170 - val_loss: 0.6865 - val_accuracy: 0.5390
Epoch 44/500
231/231 - 30s - loss: 0.7972 - accuracy: 0.5253 - val_loss: 0.6862 - val_accuracy: 0.5346
Epoch 45/500
231/231 - 30s - loss: 0.8080 - accuracy: 0.5134 - val_loss: 0.6864 - val_accuracy: 0.5423
Epoch 46/500
231/231 - 30s - loss: 0.8129 - accuracy: 0.5150 - val_loss: 0.6858 - val_accuracy: 0.5499
Epoch 47/500
231/231 - 30s - loss: 0.7990 - accuracy: 0.5147 - val_loss: 0.6862 - val_accuracy: 0.5445
Epoch 48/500
231/231 - 30s - loss: 0.8055 - accuracy: 0.5195 - val_loss: 0.6861 - val_accuracy: 0.5423
Epoch 49/500
231/231 - 30s - loss: 0.7941 - accuracy: 0.5195 - val_loss: 0.6856 - val_accuracy: 0.5456
Epoch 50/500
231/231 - 30s - loss: 0.8224 - accuracy: 0.5123 - val_loss: 0.6854 - val_accuracy: 0.5434
Epoch 51/500
231/231 - 30s - loss: 0.8024 - accuracy: 0.5164 - val_loss: 0.6852 - val_accuracy: 0.5368
Epoch 52/500
231/231 - 30s - loss: 0.7851 - accuracy: 0.5384 - val_loss: 0.6856 - val_accuracy: 0.5434
Epoch 53/500
231/231 - 30s - loss: 0.7965 - accuracy: 0.5215 - val_loss: 0.6844 - val_accuracy: 0.5456
Epoch 54/500
231/231 - 30s - loss: 0.7954 - accuracy: 0.5259 - val_loss: 0.6849 - val_accuracy: 0.5390
Epoch 55/500
231/231 - 30s - loss: 0.7850 - accuracy: 0.5261 - val_loss: 0.6841 - val_accuracy: 0.5401
Epoch 56/500
231/231 - 30s - loss: 0.8059 - accuracy: 0.5124 - val_loss: 0.6847 - val_accuracy: 0.5335
Epoch 57/500
231/231 - 30s - loss: 0.7951 - accuracy: 0.5248 - val_loss: 0.6839 - val_accuracy: 0.5357
Epoch 58/500
231/231 - 30s - loss: 0.7873 - accuracy: 0.5354 - val_loss: 0.6841 - val_accuracy: 0.5412
Epoch 59/500
231/231 - 30s - loss: 0.7868 - accuracy: 0.5280 - val_loss: 0.6841 - val_accuracy: 0.5390
Epoch 60/500
231/231 - 30s - loss: 0.7954 - accuracy: 0.5265 - val_loss: 0.6834 - val_accuracy: 0.5423
Epoch 61/500
231/231 - 30s - loss: 0.7950 - accuracy: 0.5241 - val_loss: 0.6832 - val_accuracy: 0.5456
Epoch 62/500
231/231 - 30s - loss: 0.7911 - accuracy: 0.5215 - val_loss: 0.6839 - val_accuracy: 0.5401
Epoch 63/500
231/231 - 30s - loss: 0.7862 - accuracy: 0.5255 - val_loss: 0.6824 - val_accuracy: 0.5521
Epoch 64/500
231/231 - 30s - loss: 0.7931 - accuracy: 0.5286 - val_loss: 0.6827 - val_accuracy: 0.5456
Epoch 65/500
231/231 - 30s - loss: 0.7876 - accuracy: 0.5203 - val_loss: 0.6823 - val_accuracy: 0.5467
Epoch 66/500
231/231 - 30s - loss: 0.7918 - accuracy: 0.5298 - val_loss: 0.6824 - val_accuracy: 0.5456
Epoch 67/500
231/231 - 30s - loss: 0.7885 - accuracy: 0.5274 - val_loss: 0.6822 - val_accuracy: 0.5477
Epoch 68/500
231/231 - 30s - loss: 0.7763 - accuracy: 0.5327 - val_loss: 0.6820 - val_accuracy: 0.5456
Epoch 69/500
231/231 - 30s - loss: 0.7876 - accuracy: 0.5310 - val_loss: 0.6820 - val_accuracy: 0.5521
Epoch 70/500
231/231 - 30s - loss: 0.7852 - accuracy: 0.5251 - val_loss: 0.6812 - val_accuracy: 0.5499
Epoch 71/500
231/231 - 30s - loss: 0.7694 - accuracy: 0.5396 - val_loss: 0.6806 - val_accuracy: 0.5565
Epoch 72/500
231/231 - 30s - loss: 0.7750 - accuracy: 0.5388 - val_loss: 0.6804 - val_accuracy: 0.5598
Epoch 73/500
231/231 - 30s - loss: 0.7827 - accuracy: 0.5295 - val_loss: 0.6803 - val_accuracy: 0.5510
Epoch 74/500
231/231 - 30s - loss: 0.7931 - accuracy: 0.5261 - val_loss: 0.6793 - val_accuracy: 0.5598
Epoch 75/500
231/231 - 30s - loss: 0.7882 - accuracy: 0.5279 - val_loss: 0.6794 - val_accuracy: 0.5576
Epoch 76/500
231/231 - 30s - loss: 0.7827 - accuracy: 0.5375 - val_loss: 0.6793 - val_accuracy: 0.5631
Epoch 77/500
231/231 - 30s - loss: 0.7790 - accuracy: 0.5196 - val_loss: 0.6786 - val_accuracy: 0.5653
Epoch 78/500
231/231 - 30s - loss: 0.7745 - accuracy: 0.5327 - val_loss: 0.6789 - val_accuracy: 0.5587
Epoch 79/500
231/231 - 30s - loss: 0.7761 - accuracy: 0.5370 - val_loss: 0.6784 - val_accuracy: 0.5609
Epoch 80/500
231/231 - 30s - loss: 0.7694 - accuracy: 0.5428 - val_loss: 0.6778 - val_accuracy: 0.5598
Epoch 81/500
231/231 - 30s - loss: 0.7705 - accuracy: 0.5441 - val_loss: 0.6777 - val_accuracy: 0.5576
Epoch 82/500
231/231 - 30s - loss: 0.7777 - accuracy: 0.5303 - val_loss: 0.6770 - val_accuracy: 0.5620
Epoch 83/500
231/231 - 30s - loss: 0.7663 - accuracy: 0.5359 - val_loss: 0.6765 - val_accuracy: 0.5631
Epoch 84/500
231/231 - 30s - loss: 0.7698 - accuracy: 0.5408 - val_loss: 0.6758 - val_accuracy: 0.5664
Epoch 85/500
231/231 - 30s - loss: 0.7554 - accuracy: 0.5541 - val_loss: 0.6752 - val_accuracy: 0.5653
Epoch 86/500
231/231 - 30s - loss: 0.7652 - accuracy: 0.5432 - val_loss: 0.6749 - val_accuracy: 0.5686
Epoch 87/500
231/231 - 30s - loss: 0.7611 - accuracy: 0.5499 - val_loss: 0.6748 - val_accuracy: 0.5675
Epoch 88/500
231/231 - 30s - loss: 0.7731 - accuracy: 0.5404 - val_loss: 0.6742 - val_accuracy: 0.5741
Epoch 89/500
231/231 - 30s - loss: 0.7642 - accuracy: 0.5462 - val_loss: 0.6751 - val_accuracy: 0.5642
Epoch 90/500
231/231 - 30s - loss: 0.7611 - accuracy: 0.5529 - val_loss: 0.6744 - val_accuracy: 0.5664
Epoch 91/500
231/231 - 30s - loss: 0.7672 - accuracy: 0.5435 - val_loss: 0.6738 - val_accuracy: 0.5752
Epoch 92/500
231/231 - 30s - loss: 0.7693 - accuracy: 0.5423 - val_loss: 0.6730 - val_accuracy: 0.5818
Epoch 93/500
231/231 - 30s - loss: 0.7575 - accuracy: 0.5552 - val_loss: 0.6727 - val_accuracy: 0.5719
Epoch 94/500
231/231 - 30s - loss: 0.7616 - accuracy: 0.5464 - val_loss: 0.6716 - val_accuracy: 0.5829
Epoch 95/500
231/231 - 30s - loss: 0.7480 - accuracy: 0.5629 - val_loss: 0.6722 - val_accuracy: 0.5708
Epoch 96/500
231/231 - 30s - loss: 0.7516 - accuracy: 0.5500 - val_loss: 0.6724 - val_accuracy: 0.5664
Epoch 97/500
231/231 - 30s - loss: 0.7414 - accuracy: 0.5559 - val_loss: 0.6708 - val_accuracy: 0.5829
Epoch 98/500
231/231 - 30s - loss: 0.7448 - accuracy: 0.5585 - val_loss: 0.6714 - val_accuracy: 0.5807
Epoch 99/500
231/231 - 30s - loss: 0.7438 - accuracy: 0.5542 - val_loss: 0.6697 - val_accuracy: 0.5796
Epoch 100/500
231/231 - 30s - loss: 0.7405 - accuracy: 0.5643 - val_loss: 0.6695 - val_accuracy: 0.5895
Epoch 101/500
231/231 - 30s - loss: 0.7410 - accuracy: 0.5661 - val_loss: 0.6690 - val_accuracy: 0.5840
Epoch 102/500
231/231 - 30s - loss: 0.7468 - accuracy: 0.5545 - val_loss: 0.6688 - val_accuracy: 0.5807
Epoch 103/500
231/231 - 30s - loss: 0.7405 - accuracy: 0.5707 - val_loss: 0.6682 - val_accuracy: 0.5818
Epoch 104/500
231/231 - 30s - loss: 0.7300 - accuracy: 0.5715 - val_loss: 0.6679 - val_accuracy: 0.5840
Epoch 105/500
231/231 - 30s - loss: 0.7294 - accuracy: 0.5735 - val_loss: 0.6686 - val_accuracy: 0.5873
Epoch 106/500
231/231 - 30s - loss: 0.7330 - accuracy: 0.5692 - val_loss: 0.6665 - val_accuracy: 0.5917
Epoch 107/500
231/231 - 30s - loss: 0.7390 - accuracy: 0.5693 - val_loss: 0.6663 - val_accuracy: 0.5917
Epoch 108/500
231/231 - 30s - loss: 0.7216 - accuracy: 0.5754 - val_loss: 0.6664 - val_accuracy: 0.5895
Epoch 109/500
231/231 - 30s - loss: 0.7303 - accuracy: 0.5648 - val_loss: 0.6647 - val_accuracy: 0.5939
Epoch 110/500
231/231 - 30s - loss: 0.7249 - accuracy: 0.5781 - val_loss: 0.6650 - val_accuracy: 0.5928
Epoch 111/500
231/231 - 30s - loss: 0.7222 - accuracy: 0.5762 - val_loss: 0.6633 - val_accuracy: 0.5960
Epoch 112/500
231/231 - 30s - loss: 0.7327 - accuracy: 0.5673 - val_loss: 0.6622 - val_accuracy: 0.5993
Epoch 113/500
231/231 - 30s - loss: 0.7266 - accuracy: 0.5777 - val_loss: 0.6627 - val_accuracy: 0.5982
Epoch 114/500
231/231 - 30s - loss: 0.7096 - accuracy: 0.5882 - val_loss: 0.6631 - val_accuracy: 0.5906
Epoch 115/500
231/231 - 30s - loss: 0.7237 - accuracy: 0.5802 - val_loss: 0.6614 - val_accuracy: 0.5950
Epoch 116/500
231/231 - 30s - loss: 0.7214 - accuracy: 0.5750 - val_loss: 0.6599 - val_accuracy: 0.6059
Epoch 117/500
231/231 - 30s - loss: 0.7048 - accuracy: 0.5951 - val_loss: 0.6601 - val_accuracy: 0.5971
Epoch 118/500
231/231 - 30s - loss: 0.7120 - accuracy: 0.5905 - val_loss: 0.6600 - val_accuracy: 0.5939
Epoch 119/500
231/231 - 30s - loss: 0.7086 - accuracy: 0.6004 - val_loss: 0.6591 - val_accuracy: 0.5960
Epoch 120/500
231/231 - 30s - loss: 0.6998 - accuracy: 0.5970 - val_loss: 0.6590 - val_accuracy: 0.5971
Epoch 121/500
231/231 - 30s - loss: 0.6985 - accuracy: 0.6041 - val_loss: 0.6564 - val_accuracy: 0.6103
Epoch 122/500
231/231 - 30s - loss: 0.6947 - accuracy: 0.6043 - val_loss: 0.6579 - val_accuracy: 0.5971
Epoch 123/500
231/231 - 30s - loss: 0.7074 - accuracy: 0.5950 - val_loss: 0.6553 - val_accuracy: 0.6136
Epoch 124/500
231/231 - 30s - loss: 0.6982 - accuracy: 0.6052 - val_loss: 0.6541 - val_accuracy: 0.6191
Epoch 125/500
231/231 - 30s - loss: 0.6912 - accuracy: 0.6071 - val_loss: 0.6536 - val_accuracy: 0.6180
Epoch 126/500
231/231 - 30s - loss: 0.6871 - accuracy: 0.6086 - val_loss: 0.6541 - val_accuracy: 0.6125
Epoch 127/500
231/231 - 30s - loss: 0.6861 - accuracy: 0.6128 - val_loss: 0.6538 - val_accuracy: 0.6103
Epoch 128/500
231/231 - 30s - loss: 0.6791 - accuracy: 0.6166 - val_loss: 0.6518 - val_accuracy: 0.6147
Epoch 129/500
231/231 - 30s - loss: 0.6903 - accuracy: 0.6117 - val_loss: 0.6513 - val_accuracy: 0.6147
Epoch 130/500
231/231 - 30s - loss: 0.6708 - accuracy: 0.6274 - val_loss: 0.6489 - val_accuracy: 0.6246
Epoch 131/500
231/231 - 30s - loss: 0.6810 - accuracy: 0.6206 - val_loss: 0.6483 - val_accuracy: 0.6268
Epoch 132/500
231/231 - 30s - loss: 0.6703 - accuracy: 0.6293 - val_loss: 0.6477 - val_accuracy: 0.6235
Epoch 133/500
231/231 - 30s - loss: 0.6730 - accuracy: 0.6262 - val_loss: 0.6476 - val_accuracy: 0.6235
Epoch 134/500
231/231 - 30s - loss: 0.6742 - accuracy: 0.6299 - val_loss: 0.6483 - val_accuracy: 0.6191
Epoch 135/500
231/231 - 30s - loss: 0.6642 - accuracy: 0.6305 - val_loss: 0.6469 - val_accuracy: 0.6202
Epoch 136/500
231/231 - 30s - loss: 0.6607 - accuracy: 0.6367 - val_loss: 0.6448 - val_accuracy: 0.6268
Epoch 137/500
231/231 - 30s - loss: 0.6487 - accuracy: 0.6425 - val_loss: 0.6466 - val_accuracy: 0.6191
Epoch 138/500
231/231 - 30s - loss: 0.6497 - accuracy: 0.6429 - val_loss: 0.6448 - val_accuracy: 0.6180
Epoch 139/500
231/231 - 30s - loss: 0.6584 - accuracy: 0.6420 - val_loss: 0.6448 - val_accuracy: 0.6213
Epoch 140/500
231/231 - 30s - loss: 0.6425 - accuracy: 0.6509 - val_loss: 0.6429 - val_accuracy: 0.6279
Epoch 141/500
231/231 - 30s - loss: 0.6373 - accuracy: 0.6553 - val_loss: 0.6438 - val_accuracy: 0.6246
Epoch 142/500
231/231 - 30s - loss: 0.6434 - accuracy: 0.6553 - val_loss: 0.6418 - val_accuracy: 0.6257
Epoch 143/500
231/231 - 30s - loss: 0.6340 - accuracy: 0.6498 - val_loss: 0.6399 - val_accuracy: 0.6389
Epoch 144/500
231/231 - 30s - loss: 0.6367 - accuracy: 0.6589 - val_loss: 0.6399 - val_accuracy: 0.6334
Epoch 145/500
231/231 - 30s - loss: 0.6327 - accuracy: 0.6608 - val_loss: 0.6398 - val_accuracy: 0.6345
Epoch 146/500
231/231 - 30s - loss: 0.6286 - accuracy: 0.6657 - val_loss: 0.6402 - val_accuracy: 0.6301
Epoch 147/500
231/231 - 30s - loss: 0.6261 - accuracy: 0.6682 - val_loss: 0.6386 - val_accuracy: 0.6356
Epoch 148/500
231/231 - 30s - loss: 0.6220 - accuracy: 0.6648 - val_loss: 0.6411 - val_accuracy: 0.6279
Epoch 149/500
231/231 - 30s - loss: 0.6171 - accuracy: 0.6728 - val_loss: 0.6398 - val_accuracy: 0.6290
Epoch 150/500
231/231 - 30s - loss: 0.6086 - accuracy: 0.6836 - val_loss: 0.6383 - val_accuracy: 0.6345
Epoch 151/500
231/231 - 30s - loss: 0.6052 - accuracy: 0.6790 - val_loss: 0.6380 - val_accuracy: 0.6400
Epoch 152/500
231/231 - 30s - loss: 0.6029 - accuracy: 0.6839 - val_loss: 0.6375 - val_accuracy: 0.6378
Epoch 153/500
231/231 - 30s - loss: 0.5960 - accuracy: 0.6872 - val_loss: 0.6370 - val_accuracy: 0.6389
Epoch 154/500
231/231 - 30s - loss: 0.5928 - accuracy: 0.6873 - val_loss: 0.6360 - val_accuracy: 0.6411
Epoch 155/500
231/231 - 30s - loss: 0.5835 - accuracy: 0.6997 - val_loss: 0.6363 - val_accuracy: 0.6443
Epoch 156/500
231/231 - 30s - loss: 0.5908 - accuracy: 0.6931 - val_loss: 0.6382 - val_accuracy: 0.6400
Epoch 157/500
231/231 - 30s - loss: 0.5857 - accuracy: 0.7009 - val_loss: 0.6356 - val_accuracy: 0.6443
Epoch 158/500
231/231 - 30s - loss: 0.5773 - accuracy: 0.7020 - val_loss: 0.6355 - val_accuracy: 0.6422
Epoch 159/500
231/231 - 30s - loss: 0.5817 - accuracy: 0.7078 - val_loss: 0.6342 - val_accuracy: 0.6432
Epoch 160/500
231/231 - 30s - loss: 0.5677 - accuracy: 0.7103 - val_loss: 0.6371 - val_accuracy: 0.6422
Epoch 161/500
231/231 - 30s - loss: 0.5702 - accuracy: 0.7070 - val_loss: 0.6344 - val_accuracy: 0.6432
Epoch 162/500
231/231 - 30s - loss: 0.5560 - accuracy: 0.7134 - val_loss: 0.6341 - val_accuracy: 0.6432
Epoch 163/500
231/231 - 30s - loss: 0.5582 - accuracy: 0.7215 - val_loss: 0.6365 - val_accuracy: 0.6443
Epoch 164/500
231/231 - 30s - loss: 0.5597 - accuracy: 0.7195 - val_loss: 0.6345 - val_accuracy: 0.6400
Epoch 165/500
231/231 - 30s - loss: 0.5661 - accuracy: 0.7237 - val_loss: 0.6333 - val_accuracy: 0.6454
Epoch 166/500
231/231 - 30s - loss: 0.5438 - accuracy: 0.7248 - val_loss: 0.6322 - val_accuracy: 0.6520
Epoch 167/500
231/231 - 30s - loss: 0.5389 - accuracy: 0.7341 - val_loss: 0.6339 - val_accuracy: 0.6476
Epoch 168/500
231/231 - 30s - loss: 0.5418 - accuracy: 0.7276 - val_loss: 0.6334 - val_accuracy: 0.6498
Epoch 169/500
231/231 - 30s - loss: 0.5261 - accuracy: 0.7396 - val_loss: 0.6361 - val_accuracy: 0.6476
Epoch 170/500
231/231 - 30s - loss: 0.5316 - accuracy: 0.7343 - val_loss: 0.6352 - val_accuracy: 0.6476
Epoch 171/500
231/231 - 30s - loss: 0.5226 - accuracy: 0.7442 - val_loss: 0.6353 - val_accuracy: 0.6487
Epoch 172/500
231/231 - 30s - loss: 0.5232 - accuracy: 0.7499 - val_loss: 0.6369 - val_accuracy: 0.6476
Epoch 173/500
231/231 - 30s - loss: 0.5135 - accuracy: 0.7527 - val_loss: 0.6379 - val_accuracy: 0.6454
Epoch 174/500
231/231 - 30s - loss: 0.5086 - accuracy: 0.7508 - val_loss: 0.6384 - val_accuracy: 0.6454
Epoch 175/500
231/231 - 30s - loss: 0.4973 - accuracy: 0.7585 - val_loss: 0.6358 - val_accuracy: 0.6476
Epoch 176/500
231/231 - 30s - loss: 0.5084 - accuracy: 0.7468 - val_loss: 0.6375 - val_accuracy: 0.6487
Epoch 177/500
231/231 - 30s - loss: 0.4872 - accuracy: 0.7665 - val_loss: 0.6388 - val_accuracy: 0.6487
Epoch 178/500
231/231 - 30s - loss: 0.4926 - accuracy: 0.7654 - val_loss: 0.6393 - val_accuracy: 0.6509
Epoch 179/500
231/231 - 30s - loss: 0.4829 - accuracy: 0.7671 - val_loss: 0.6406 - val_accuracy: 0.6465
Epoch 180/500
231/231 - 30s - loss: 0.4872 - accuracy: 0.7631 - val_loss: 0.6409 - val_accuracy: 0.6476
Epoch 181/500
231/231 - 30s - loss: 0.4731 - accuracy: 0.7677 - val_loss: 0.6430 - val_accuracy: 0.6476
Epoch 182/500
231/231 - 30s - loss: 0.4643 - accuracy: 0.7754 - val_loss: 0.6468 - val_accuracy: 0.6432
Epoch 183/500
231/231 - 30s - loss: 0.4695 - accuracy: 0.7754 - val_loss: 0.6468 - val_accuracy: 0.6400
Epoch 184/500
231/231 - 30s - loss: 0.4595 - accuracy: 0.7859 - val_loss: 0.6461 - val_accuracy: 0.6432
Epoch 185/500
231/231 - 30s - loss: 0.4626 - accuracy: 0.7813 - val_loss: 0.6486 - val_accuracy: 0.6400
Epoch 186/500
231/231 - 30s - loss: 0.4438 - accuracy: 0.7901 - val_loss: 0.6497 - val_accuracy: 0.6400
========================================
save_weights
h5_weights/CM.po/embedding_dense.h5
========================================

end time >>> Sun Oct  3 02:01:16 2021

end time >>> Sun Oct  3 02:01:16 2021

end time >>> Sun Oct  3 02:01:16 2021

end time >>> Sun Oct  3 02:01:16 2021

end time >>> Sun Oct  3 02:01:16 2021












args.model = embedding_dense
time used = 5611.482242822647


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 02:01:18 2021

begin time >>> Sun Oct  3 02:01:18 2021

begin time >>> Sun Oct  3 02:01:18 2021

begin time >>> Sun Oct  3 02:01:18 2021

begin time >>> Sun Oct  3 02:01:18 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_cnn_one_branch
args.type = train
args.name = CM.po
args.length = 10001
===========================


-> h5_weights/CM.po folder already exist. pass.
-> result/CM.po/onehot_cnn_one_branch folder already exist. pass.
-> result/CM.po/onehot_cnn_two_branch folder already exist. pass.
-> result/CM.po/onehot_embedding_dense folder already exist. pass.
-> result/CM.po/onehot_dense folder already exist. pass.
-> result/CM.po/onehot_resnet18 folder already exist. pass.
-> result/CM.po/onehot_resnet34 folder already exist. pass.
-> result/CM.po/embedding_cnn_one_branch folder already exist. pass.
-> result/CM.po/embedding_cnn_two_branch folder already exist. pass.
-> result/CM.po/embedding_dense folder already exist. pass.
-> result/CM.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/CM.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
CM.po
########################################

########################################
model_name
embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
231/231 - 32s - loss: 0.8724 - accuracy: 0.5028 - val_loss: 0.7002 - val_accuracy: 0.4885
Epoch 2/500
231/231 - 32s - loss: 0.8588 - accuracy: 0.4960 - val_loss: 0.7054 - val_accuracy: 0.4852
Epoch 3/500
231/231 - 32s - loss: 0.8487 - accuracy: 0.5123 - val_loss: 0.7044 - val_accuracy: 0.4962
Epoch 4/500
231/231 - 32s - loss: 0.8448 - accuracy: 0.5141 - val_loss: 0.7017 - val_accuracy: 0.5071
Epoch 5/500
231/231 - 32s - loss: 0.8383 - accuracy: 0.5128 - val_loss: 0.7000 - val_accuracy: 0.5082
Epoch 6/500
231/231 - 32s - loss: 0.8240 - accuracy: 0.5246 - val_loss: 0.6974 - val_accuracy: 0.5247
Epoch 7/500
231/231 - 32s - loss: 0.8209 - accuracy: 0.5221 - val_loss: 0.6950 - val_accuracy: 0.5181
Epoch 8/500
231/231 - 32s - loss: 0.8103 - accuracy: 0.5222 - val_loss: 0.6936 - val_accuracy: 0.5269
Epoch 9/500
231/231 - 32s - loss: 0.8229 - accuracy: 0.5214 - val_loss: 0.6918 - val_accuracy: 0.5324
Epoch 10/500
231/231 - 32s - loss: 0.7976 - accuracy: 0.5377 - val_loss: 0.6911 - val_accuracy: 0.5368
Epoch 11/500
231/231 - 32s - loss: 0.8065 - accuracy: 0.5282 - val_loss: 0.6898 - val_accuracy: 0.5390
Epoch 12/500
231/231 - 32s - loss: 0.7975 - accuracy: 0.5320 - val_loss: 0.6875 - val_accuracy: 0.5412
Epoch 13/500
231/231 - 32s - loss: 0.7960 - accuracy: 0.5443 - val_loss: 0.6866 - val_accuracy: 0.5445
Epoch 14/500
231/231 - 32s - loss: 0.7842 - accuracy: 0.5485 - val_loss: 0.6847 - val_accuracy: 0.5423
Epoch 15/500
231/231 - 32s - loss: 0.7912 - accuracy: 0.5426 - val_loss: 0.6841 - val_accuracy: 0.5477
Epoch 16/500
231/231 - 32s - loss: 0.7797 - accuracy: 0.5513 - val_loss: 0.6823 - val_accuracy: 0.5488
Epoch 17/500
231/231 - 32s - loss: 0.7772 - accuracy: 0.5405 - val_loss: 0.6802 - val_accuracy: 0.5576
Epoch 18/500
231/231 - 32s - loss: 0.7602 - accuracy: 0.5559 - val_loss: 0.6800 - val_accuracy: 0.5477
Epoch 19/500
231/231 - 32s - loss: 0.7625 - accuracy: 0.5604 - val_loss: 0.6789 - val_accuracy: 0.5532
Epoch 20/500
231/231 - 32s - loss: 0.7720 - accuracy: 0.5466 - val_loss: 0.6772 - val_accuracy: 0.5653
Epoch 21/500
231/231 - 32s - loss: 0.7565 - accuracy: 0.5655 - val_loss: 0.6770 - val_accuracy: 0.5675
Epoch 22/500
231/231 - 32s - loss: 0.7593 - accuracy: 0.5598 - val_loss: 0.6756 - val_accuracy: 0.5675
Epoch 23/500
231/231 - 32s - loss: 0.7514 - accuracy: 0.5597 - val_loss: 0.6742 - val_accuracy: 0.5763
Epoch 24/500
231/231 - 32s - loss: 0.7552 - accuracy: 0.5659 - val_loss: 0.6726 - val_accuracy: 0.5807
Epoch 25/500
231/231 - 32s - loss: 0.7457 - accuracy: 0.5712 - val_loss: 0.6719 - val_accuracy: 0.5785
Epoch 26/500
231/231 - 32s - loss: 0.7386 - accuracy: 0.5705 - val_loss: 0.6700 - val_accuracy: 0.5884
Epoch 27/500
231/231 - 32s - loss: 0.7429 - accuracy: 0.5704 - val_loss: 0.6700 - val_accuracy: 0.5840
Epoch 28/500
231/231 - 32s - loss: 0.7367 - accuracy: 0.5739 - val_loss: 0.6700 - val_accuracy: 0.5796
Epoch 29/500
231/231 - 32s - loss: 0.7298 - accuracy: 0.5833 - val_loss: 0.6684 - val_accuracy: 0.5917
Epoch 30/500
231/231 - 32s - loss: 0.7333 - accuracy: 0.5837 - val_loss: 0.6671 - val_accuracy: 0.5928
Epoch 31/500
231/231 - 32s - loss: 0.7162 - accuracy: 0.5957 - val_loss: 0.6656 - val_accuracy: 0.5960
Epoch 32/500
231/231 - 32s - loss: 0.7166 - accuracy: 0.5974 - val_loss: 0.6643 - val_accuracy: 0.5939
Epoch 33/500
231/231 - 32s - loss: 0.7260 - accuracy: 0.5847 - val_loss: 0.6638 - val_accuracy: 0.5971
Epoch 34/500
231/231 - 32s - loss: 0.7040 - accuracy: 0.6000 - val_loss: 0.6632 - val_accuracy: 0.6015
Epoch 35/500
231/231 - 32s - loss: 0.7046 - accuracy: 0.5988 - val_loss: 0.6622 - val_accuracy: 0.6037
Epoch 36/500
231/231 - 32s - loss: 0.7065 - accuracy: 0.6062 - val_loss: 0.6615 - val_accuracy: 0.5982
Epoch 37/500
231/231 - 32s - loss: 0.6953 - accuracy: 0.6077 - val_loss: 0.6608 - val_accuracy: 0.6059
Epoch 38/500
231/231 - 32s - loss: 0.6890 - accuracy: 0.6069 - val_loss: 0.6594 - val_accuracy: 0.6037
Epoch 39/500
231/231 - 32s - loss: 0.6849 - accuracy: 0.6151 - val_loss: 0.6581 - val_accuracy: 0.6081
Epoch 40/500
231/231 - 32s - loss: 0.6801 - accuracy: 0.6145 - val_loss: 0.6570 - val_accuracy: 0.6070
Epoch 41/500
231/231 - 32s - loss: 0.6877 - accuracy: 0.6172 - val_loss: 0.6559 - val_accuracy: 0.6092
Epoch 42/500
231/231 - 32s - loss: 0.6772 - accuracy: 0.6223 - val_loss: 0.6554 - val_accuracy: 0.6147
Epoch 43/500
231/231 - 32s - loss: 0.6729 - accuracy: 0.6205 - val_loss: 0.6537 - val_accuracy: 0.6180
Epoch 44/500
231/231 - 32s - loss: 0.6607 - accuracy: 0.6333 - val_loss: 0.6529 - val_accuracy: 0.6125
Epoch 45/500
231/231 - 32s - loss: 0.6616 - accuracy: 0.6320 - val_loss: 0.6517 - val_accuracy: 0.6125
Epoch 46/500
231/231 - 32s - loss: 0.6629 - accuracy: 0.6346 - val_loss: 0.6520 - val_accuracy: 0.6158
Epoch 47/500
231/231 - 32s - loss: 0.6488 - accuracy: 0.6497 - val_loss: 0.6505 - val_accuracy: 0.6125
Epoch 48/500
231/231 - 32s - loss: 0.6570 - accuracy: 0.6433 - val_loss: 0.6491 - val_accuracy: 0.6180
Epoch 49/500
231/231 - 32s - loss: 0.6504 - accuracy: 0.6449 - val_loss: 0.6484 - val_accuracy: 0.6180
Epoch 50/500
231/231 - 32s - loss: 0.6441 - accuracy: 0.6439 - val_loss: 0.6473 - val_accuracy: 0.6213
Epoch 51/500
231/231 - 32s - loss: 0.6413 - accuracy: 0.6549 - val_loss: 0.6468 - val_accuracy: 0.6202
Epoch 52/500
231/231 - 32s - loss: 0.6289 - accuracy: 0.6595 - val_loss: 0.6467 - val_accuracy: 0.6290
Epoch 53/500
231/231 - 32s - loss: 0.6324 - accuracy: 0.6543 - val_loss: 0.6458 - val_accuracy: 0.6224
Epoch 54/500
231/231 - 32s - loss: 0.6161 - accuracy: 0.6675 - val_loss: 0.6452 - val_accuracy: 0.6257
Epoch 55/500
231/231 - 32s - loss: 0.6131 - accuracy: 0.6767 - val_loss: 0.6453 - val_accuracy: 0.6224
Epoch 56/500
231/231 - 32s - loss: 0.6103 - accuracy: 0.6741 - val_loss: 0.6440 - val_accuracy: 0.6312
Epoch 57/500
231/231 - 32s - loss: 0.6023 - accuracy: 0.6821 - val_loss: 0.6427 - val_accuracy: 0.6257
Epoch 58/500
231/231 - 32s - loss: 0.5907 - accuracy: 0.6907 - val_loss: 0.6425 - val_accuracy: 0.6268
Epoch 59/500
231/231 - 32s - loss: 0.5906 - accuracy: 0.6891 - val_loss: 0.6415 - val_accuracy: 0.6323
Epoch 60/500
231/231 - 32s - loss: 0.5827 - accuracy: 0.6936 - val_loss: 0.6418 - val_accuracy: 0.6334
Epoch 61/500
231/231 - 32s - loss: 0.5892 - accuracy: 0.6907 - val_loss: 0.6414 - val_accuracy: 0.6356
Epoch 62/500
231/231 - 32s - loss: 0.5776 - accuracy: 0.7031 - val_loss: 0.6411 - val_accuracy: 0.6257
Epoch 63/500
231/231 - 32s - loss: 0.5735 - accuracy: 0.7092 - val_loss: 0.6401 - val_accuracy: 0.6312
Epoch 64/500
231/231 - 32s - loss: 0.5730 - accuracy: 0.7020 - val_loss: 0.6392 - val_accuracy: 0.6345
Epoch 65/500
231/231 - 32s - loss: 0.5716 - accuracy: 0.6993 - val_loss: 0.6388 - val_accuracy: 0.6356
Epoch 66/500
231/231 - 32s - loss: 0.5612 - accuracy: 0.7147 - val_loss: 0.6379 - val_accuracy: 0.6389
Epoch 67/500
231/231 - 32s - loss: 0.5382 - accuracy: 0.7278 - val_loss: 0.6382 - val_accuracy: 0.6422
Epoch 68/500
231/231 - 32s - loss: 0.5509 - accuracy: 0.7203 - val_loss: 0.6374 - val_accuracy: 0.6411
Epoch 69/500
231/231 - 32s - loss: 0.5408 - accuracy: 0.7202 - val_loss: 0.6364 - val_accuracy: 0.6400
Epoch 70/500
231/231 - 32s - loss: 0.5332 - accuracy: 0.7320 - val_loss: 0.6361 - val_accuracy: 0.6389
Epoch 71/500
231/231 - 32s - loss: 0.5321 - accuracy: 0.7369 - val_loss: 0.6377 - val_accuracy: 0.6345
Epoch 72/500
231/231 - 32s - loss: 0.5334 - accuracy: 0.7329 - val_loss: 0.6370 - val_accuracy: 0.6422
Epoch 73/500
231/231 - 32s - loss: 0.5190 - accuracy: 0.7381 - val_loss: 0.6376 - val_accuracy: 0.6422
Epoch 74/500
231/231 - 32s - loss: 0.5218 - accuracy: 0.7408 - val_loss: 0.6369 - val_accuracy: 0.6367
Epoch 75/500
231/231 - 32s - loss: 0.5209 - accuracy: 0.7470 - val_loss: 0.6380 - val_accuracy: 0.6367
Epoch 76/500
231/231 - 32s - loss: 0.5106 - accuracy: 0.7420 - val_loss: 0.6365 - val_accuracy: 0.6400
Epoch 77/500
231/231 - 32s - loss: 0.5137 - accuracy: 0.7542 - val_loss: 0.6380 - val_accuracy: 0.6422
Epoch 78/500
231/231 - 32s - loss: 0.5053 - accuracy: 0.7477 - val_loss: 0.6379 - val_accuracy: 0.6400
Epoch 79/500
231/231 - 32s - loss: 0.4972 - accuracy: 0.7563 - val_loss: 0.6386 - val_accuracy: 0.6389
Epoch 80/500
231/231 - 32s - loss: 0.4807 - accuracy: 0.7670 - val_loss: 0.6366 - val_accuracy: 0.6389
Epoch 81/500
231/231 - 32s - loss: 0.4904 - accuracy: 0.7555 - val_loss: 0.6390 - val_accuracy: 0.6356
Epoch 82/500
231/231 - 32s - loss: 0.4607 - accuracy: 0.7771 - val_loss: 0.6407 - val_accuracy: 0.6400
Epoch 83/500
231/231 - 32s - loss: 0.4642 - accuracy: 0.7795 - val_loss: 0.6399 - val_accuracy: 0.6400
Epoch 84/500
231/231 - 32s - loss: 0.4676 - accuracy: 0.7773 - val_loss: 0.6404 - val_accuracy: 0.6389
Epoch 85/500
231/231 - 32s - loss: 0.4607 - accuracy: 0.7813 - val_loss: 0.6402 - val_accuracy: 0.6378
Epoch 86/500
231/231 - 32s - loss: 0.4666 - accuracy: 0.7737 - val_loss: 0.6415 - val_accuracy: 0.6378
Epoch 87/500
231/231 - 32s - loss: 0.4548 - accuracy: 0.7791 - val_loss: 0.6422 - val_accuracy: 0.6367
========================================
save_weights
h5_weights/CM.po/embedding_cnn_one_branch.h5
========================================

end time >>> Sun Oct  3 02:47:50 2021

end time >>> Sun Oct  3 02:47:50 2021

end time >>> Sun Oct  3 02:47:50 2021

end time >>> Sun Oct  3 02:47:50 2021

end time >>> Sun Oct  3 02:47:50 2021












args.model = embedding_cnn_one_branch
time used = 2792.602046728134


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 02:47:52 2021

begin time >>> Sun Oct  3 02:47:52 2021

begin time >>> Sun Oct  3 02:47:52 2021

begin time >>> Sun Oct  3 02:47:52 2021

begin time >>> Sun Oct  3 02:47:52 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_cnn_two_branch
args.type = train
args.name = CM.po
args.length = 10001
===========================


-> h5_weights/CM.po folder already exist. pass.
-> result/CM.po/onehot_cnn_one_branch folder already exist. pass.
-> result/CM.po/onehot_cnn_two_branch folder already exist. pass.
-> result/CM.po/onehot_embedding_dense folder already exist. pass.
-> result/CM.po/onehot_dense folder already exist. pass.
-> result/CM.po/onehot_resnet18 folder already exist. pass.
-> result/CM.po/onehot_resnet34 folder already exist. pass.
-> result/CM.po/embedding_cnn_one_branch folder already exist. pass.
-> result/CM.po/embedding_cnn_two_branch folder already exist. pass.
-> result/CM.po/embedding_dense folder already exist. pass.
-> result/CM.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/CM.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
CM.po
########################################

########################################
model_name
embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
231/231 - 32s - loss: 0.8631 - accuracy: 0.5043 - val_loss: 0.6996 - val_accuracy: 0.5016
Epoch 2/500
231/231 - 32s - loss: 0.8611 - accuracy: 0.5090 - val_loss: 0.7068 - val_accuracy: 0.4940
Epoch 3/500
231/231 - 32s - loss: 0.8547 - accuracy: 0.5059 - val_loss: 0.7066 - val_accuracy: 0.5005
Epoch 4/500
231/231 - 32s - loss: 0.8463 - accuracy: 0.5172 - val_loss: 0.7047 - val_accuracy: 0.5005
Epoch 5/500
231/231 - 32s - loss: 0.8212 - accuracy: 0.5306 - val_loss: 0.7027 - val_accuracy: 0.5071
Epoch 6/500
231/231 - 32s - loss: 0.8259 - accuracy: 0.5180 - val_loss: 0.7001 - val_accuracy: 0.5038
Epoch 7/500
231/231 - 32s - loss: 0.8124 - accuracy: 0.5303 - val_loss: 0.6985 - val_accuracy: 0.5115
Epoch 8/500
231/231 - 32s - loss: 0.8308 - accuracy: 0.5164 - val_loss: 0.6971 - val_accuracy: 0.5115
Epoch 9/500
231/231 - 32s - loss: 0.8187 - accuracy: 0.5260 - val_loss: 0.6949 - val_accuracy: 0.5247
Epoch 10/500
231/231 - 32s - loss: 0.8046 - accuracy: 0.5325 - val_loss: 0.6946 - val_accuracy: 0.5269
Epoch 11/500
231/231 - 32s - loss: 0.8158 - accuracy: 0.5302 - val_loss: 0.6926 - val_accuracy: 0.5291
Epoch 12/500
231/231 - 32s - loss: 0.7935 - accuracy: 0.5373 - val_loss: 0.6914 - val_accuracy: 0.5324
Epoch 13/500
231/231 - 32s - loss: 0.7930 - accuracy: 0.5388 - val_loss: 0.6900 - val_accuracy: 0.5412
Epoch 14/500
231/231 - 32s - loss: 0.7863 - accuracy: 0.5394 - val_loss: 0.6889 - val_accuracy: 0.5390
Epoch 15/500
231/231 - 32s - loss: 0.7800 - accuracy: 0.5503 - val_loss: 0.6879 - val_accuracy: 0.5390
Epoch 16/500
231/231 - 32s - loss: 0.7741 - accuracy: 0.5538 - val_loss: 0.6869 - val_accuracy: 0.5499
Epoch 17/500
231/231 - 32s - loss: 0.7677 - accuracy: 0.5513 - val_loss: 0.6859 - val_accuracy: 0.5456
Epoch 18/500
231/231 - 32s - loss: 0.7716 - accuracy: 0.5549 - val_loss: 0.6849 - val_accuracy: 0.5477
Epoch 19/500
231/231 - 32s - loss: 0.7542 - accuracy: 0.5617 - val_loss: 0.6834 - val_accuracy: 0.5456
Epoch 20/500
231/231 - 32s - loss: 0.7668 - accuracy: 0.5566 - val_loss: 0.6832 - val_accuracy: 0.5467
Epoch 21/500
231/231 - 32s - loss: 0.7575 - accuracy: 0.5614 - val_loss: 0.6819 - val_accuracy: 0.5510
Epoch 22/500
231/231 - 32s - loss: 0.7266 - accuracy: 0.5800 - val_loss: 0.6807 - val_accuracy: 0.5488
Epoch 23/500
231/231 - 32s - loss: 0.7423 - accuracy: 0.5682 - val_loss: 0.6797 - val_accuracy: 0.5521
Epoch 24/500
231/231 - 32s - loss: 0.7396 - accuracy: 0.5821 - val_loss: 0.6784 - val_accuracy: 0.5587
Epoch 25/500
231/231 - 32s - loss: 0.7474 - accuracy: 0.5648 - val_loss: 0.6778 - val_accuracy: 0.5664
Epoch 26/500
231/231 - 32s - loss: 0.7391 - accuracy: 0.5739 - val_loss: 0.6767 - val_accuracy: 0.5653
Epoch 27/500
231/231 - 32s - loss: 0.7395 - accuracy: 0.5757 - val_loss: 0.6760 - val_accuracy: 0.5664
Epoch 28/500
231/231 - 32s - loss: 0.7379 - accuracy: 0.5794 - val_loss: 0.6748 - val_accuracy: 0.5697
Epoch 29/500
231/231 - 32s - loss: 0.7244 - accuracy: 0.5859 - val_loss: 0.6737 - val_accuracy: 0.5686
Epoch 30/500
231/231 - 32s - loss: 0.7242 - accuracy: 0.5887 - val_loss: 0.6727 - val_accuracy: 0.5829
Epoch 31/500
231/231 - 32s - loss: 0.7079 - accuracy: 0.5957 - val_loss: 0.6718 - val_accuracy: 0.5807
Epoch 32/500
231/231 - 32s - loss: 0.7140 - accuracy: 0.5981 - val_loss: 0.6706 - val_accuracy: 0.5840
Epoch 33/500
231/231 - 32s - loss: 0.6978 - accuracy: 0.6048 - val_loss: 0.6698 - val_accuracy: 0.5807
Epoch 34/500
231/231 - 32s - loss: 0.6991 - accuracy: 0.6041 - val_loss: 0.6691 - val_accuracy: 0.5851
Epoch 35/500
231/231 - 32s - loss: 0.6959 - accuracy: 0.6061 - val_loss: 0.6681 - val_accuracy: 0.5829
Epoch 36/500
231/231 - 32s - loss: 0.6830 - accuracy: 0.6182 - val_loss: 0.6668 - val_accuracy: 0.5884
Epoch 37/500
231/231 - 32s - loss: 0.6940 - accuracy: 0.6100 - val_loss: 0.6659 - val_accuracy: 0.5895
Epoch 38/500
231/231 - 32s - loss: 0.6841 - accuracy: 0.6133 - val_loss: 0.6648 - val_accuracy: 0.5982
Epoch 39/500
231/231 - 32s - loss: 0.6789 - accuracy: 0.6236 - val_loss: 0.6634 - val_accuracy: 0.5982
Epoch 40/500
231/231 - 32s - loss: 0.6685 - accuracy: 0.6348 - val_loss: 0.6634 - val_accuracy: 0.6059
Epoch 41/500
231/231 - 32s - loss: 0.6720 - accuracy: 0.6240 - val_loss: 0.6614 - val_accuracy: 0.6103
Epoch 42/500
231/231 - 32s - loss: 0.6710 - accuracy: 0.6307 - val_loss: 0.6612 - val_accuracy: 0.6015
Epoch 43/500
231/231 - 32s - loss: 0.6557 - accuracy: 0.6368 - val_loss: 0.6601 - val_accuracy: 0.6081
Epoch 44/500
231/231 - 32s - loss: 0.6532 - accuracy: 0.6409 - val_loss: 0.6589 - val_accuracy: 0.6070
Epoch 45/500
231/231 - 32s - loss: 0.6456 - accuracy: 0.6517 - val_loss: 0.6578 - val_accuracy: 0.6092
Epoch 46/500
231/231 - 32s - loss: 0.6317 - accuracy: 0.6612 - val_loss: 0.6566 - val_accuracy: 0.6147
Epoch 47/500
231/231 - 32s - loss: 0.6243 - accuracy: 0.6633 - val_loss: 0.6566 - val_accuracy: 0.6180
Epoch 48/500
231/231 - 32s - loss: 0.6278 - accuracy: 0.6631 - val_loss: 0.6555 - val_accuracy: 0.6125
Epoch 49/500
231/231 - 32s - loss: 0.6242 - accuracy: 0.6625 - val_loss: 0.6546 - val_accuracy: 0.6191
Epoch 50/500
231/231 - 32s - loss: 0.6369 - accuracy: 0.6534 - val_loss: 0.6538 - val_accuracy: 0.6246
Epoch 51/500
231/231 - 32s - loss: 0.6113 - accuracy: 0.6754 - val_loss: 0.6526 - val_accuracy: 0.6235
Epoch 52/500
231/231 - 32s - loss: 0.6179 - accuracy: 0.6671 - val_loss: 0.6515 - val_accuracy: 0.6224
Epoch 53/500
231/231 - 32s - loss: 0.5929 - accuracy: 0.6873 - val_loss: 0.6515 - val_accuracy: 0.6290
Epoch 54/500
231/231 - 32s - loss: 0.6015 - accuracy: 0.6869 - val_loss: 0.6507 - val_accuracy: 0.6257
Epoch 55/500
231/231 - 31s - loss: 0.5972 - accuracy: 0.6859 - val_loss: 0.6497 - val_accuracy: 0.6279
Epoch 56/500
231/231 - 32s - loss: 0.5805 - accuracy: 0.6861 - val_loss: 0.6493 - val_accuracy: 0.6323
Epoch 57/500
231/231 - 31s - loss: 0.5747 - accuracy: 0.7033 - val_loss: 0.6490 - val_accuracy: 0.6345
Epoch 58/500
231/231 - 31s - loss: 0.5714 - accuracy: 0.7064 - val_loss: 0.6487 - val_accuracy: 0.6334
Epoch 59/500
231/231 - 32s - loss: 0.5609 - accuracy: 0.7107 - val_loss: 0.6476 - val_accuracy: 0.6312
Epoch 60/500
231/231 - 31s - loss: 0.5602 - accuracy: 0.7115 - val_loss: 0.6478 - val_accuracy: 0.6290
Epoch 61/500
231/231 - 32s - loss: 0.5615 - accuracy: 0.7103 - val_loss: 0.6474 - val_accuracy: 0.6268
Epoch 62/500
231/231 - 31s - loss: 0.5522 - accuracy: 0.7188 - val_loss: 0.6468 - val_accuracy: 0.6290
Epoch 63/500
231/231 - 32s - loss: 0.5489 - accuracy: 0.7199 - val_loss: 0.6459 - val_accuracy: 0.6356
Epoch 64/500
231/231 - 32s - loss: 0.5435 - accuracy: 0.7237 - val_loss: 0.6454 - val_accuracy: 0.6334
Epoch 65/500
231/231 - 31s - loss: 0.5407 - accuracy: 0.7230 - val_loss: 0.6451 - val_accuracy: 0.6356
Epoch 66/500
231/231 - 32s - loss: 0.5235 - accuracy: 0.7352 - val_loss: 0.6446 - val_accuracy: 0.6356
Epoch 67/500
231/231 - 31s - loss: 0.5250 - accuracy: 0.7426 - val_loss: 0.6451 - val_accuracy: 0.6356
Epoch 68/500
231/231 - 32s - loss: 0.5115 - accuracy: 0.7477 - val_loss: 0.6457 - val_accuracy: 0.6323
Epoch 69/500
231/231 - 32s - loss: 0.5092 - accuracy: 0.7480 - val_loss: 0.6445 - val_accuracy: 0.6345
Epoch 70/500
231/231 - 32s - loss: 0.5102 - accuracy: 0.7475 - val_loss: 0.6439 - val_accuracy: 0.6290
Epoch 71/500
231/231 - 32s - loss: 0.5001 - accuracy: 0.7594 - val_loss: 0.6441 - val_accuracy: 0.6334
Epoch 72/500
231/231 - 32s - loss: 0.4956 - accuracy: 0.7595 - val_loss: 0.6441 - val_accuracy: 0.6378
Epoch 73/500
231/231 - 32s - loss: 0.4796 - accuracy: 0.7720 - val_loss: 0.6446 - val_accuracy: 0.6367
Epoch 74/500
231/231 - 32s - loss: 0.4692 - accuracy: 0.7737 - val_loss: 0.6455 - val_accuracy: 0.6400
Epoch 75/500
231/231 - 32s - loss: 0.4717 - accuracy: 0.7742 - val_loss: 0.6458 - val_accuracy: 0.6411
Epoch 76/500
231/231 - 32s - loss: 0.4689 - accuracy: 0.7711 - val_loss: 0.6455 - val_accuracy: 0.6367
Epoch 77/500
231/231 - 31s - loss: 0.4561 - accuracy: 0.7815 - val_loss: 0.6457 - val_accuracy: 0.6411
Epoch 78/500
231/231 - 32s - loss: 0.4527 - accuracy: 0.7768 - val_loss: 0.6465 - val_accuracy: 0.6389
Epoch 79/500
231/231 - 31s - loss: 0.4608 - accuracy: 0.7780 - val_loss: 0.6462 - val_accuracy: 0.6443
Epoch 80/500
231/231 - 32s - loss: 0.4388 - accuracy: 0.7946 - val_loss: 0.6474 - val_accuracy: 0.6443
Epoch 81/500
231/231 - 31s - loss: 0.4410 - accuracy: 0.7944 - val_loss: 0.6479 - val_accuracy: 0.6432
Epoch 82/500
231/231 - 32s - loss: 0.4397 - accuracy: 0.7927 - val_loss: 0.6486 - val_accuracy: 0.6400
Epoch 83/500
231/231 - 31s - loss: 0.4311 - accuracy: 0.8041 - val_loss: 0.6486 - val_accuracy: 0.6422
Epoch 84/500
231/231 - 30s - loss: 0.4165 - accuracy: 0.8058 - val_loss: 0.6494 - val_accuracy: 0.6378
Epoch 85/500
231/231 - 30s - loss: 0.4302 - accuracy: 0.8007 - val_loss: 0.6507 - val_accuracy: 0.6367
Epoch 86/500
231/231 - 30s - loss: 0.3997 - accuracy: 0.8191 - val_loss: 0.6507 - val_accuracy: 0.6378
Epoch 87/500
231/231 - 31s - loss: 0.4032 - accuracy: 0.8110 - val_loss: 0.6519 - val_accuracy: 0.6356
Epoch 88/500
231/231 - 30s - loss: 0.3913 - accuracy: 0.8172 - val_loss: 0.6520 - val_accuracy: 0.6389
Epoch 89/500
231/231 - 31s - loss: 0.3964 - accuracy: 0.8253 - val_loss: 0.6535 - val_accuracy: 0.6400
Epoch 90/500
231/231 - 31s - loss: 0.3878 - accuracy: 0.8210 - val_loss: 0.6563 - val_accuracy: 0.6411
Epoch 91/500
231/231 - 31s - loss: 0.3827 - accuracy: 0.8269 - val_loss: 0.6567 - val_accuracy: 0.6422
Epoch 92/500
231/231 - 31s - loss: 0.3739 - accuracy: 0.8266 - val_loss: 0.6577 - val_accuracy: 0.6422
Epoch 93/500
231/231 - 31s - loss: 0.3741 - accuracy: 0.8339 - val_loss: 0.6603 - val_accuracy: 0.6411
Epoch 94/500
231/231 - 31s - loss: 0.3618 - accuracy: 0.8407 - val_loss: 0.6611 - val_accuracy: 0.6422
Epoch 95/500
231/231 - 31s - loss: 0.3661 - accuracy: 0.8379 - val_loss: 0.6630 - val_accuracy: 0.6389
Epoch 96/500
231/231 - 31s - loss: 0.3567 - accuracy: 0.8451 - val_loss: 0.6641 - val_accuracy: 0.6454
Epoch 97/500
231/231 - 31s - loss: 0.3507 - accuracy: 0.8436 - val_loss: 0.6654 - val_accuracy: 0.6422
Epoch 98/500
231/231 - 31s - loss: 0.3402 - accuracy: 0.8471 - val_loss: 0.6675 - val_accuracy: 0.6422
Epoch 99/500
231/231 - 31s - loss: 0.3294 - accuracy: 0.8546 - val_loss: 0.6694 - val_accuracy: 0.6454
Epoch 100/500
231/231 - 31s - loss: 0.3370 - accuracy: 0.8519 - val_loss: 0.6703 - val_accuracy: 0.6422
Epoch 101/500
231/231 - 31s - loss: 0.3292 - accuracy: 0.8561 - val_loss: 0.6724 - val_accuracy: 0.6443
Epoch 102/500
231/231 - 31s - loss: 0.3156 - accuracy: 0.8630 - val_loss: 0.6743 - val_accuracy: 0.6443
Epoch 103/500
231/231 - 31s - loss: 0.3103 - accuracy: 0.8656 - val_loss: 0.6754 - val_accuracy: 0.6454
Epoch 104/500
231/231 - 31s - loss: 0.3204 - accuracy: 0.8600 - val_loss: 0.6772 - val_accuracy: 0.6476
Epoch 105/500
231/231 - 31s - loss: 0.3074 - accuracy: 0.8695 - val_loss: 0.6809 - val_accuracy: 0.6432
Epoch 106/500
231/231 - 31s - loss: 0.3069 - accuracy: 0.8718 - val_loss: 0.6816 - val_accuracy: 0.6465
Epoch 107/500
231/231 - 31s - loss: 0.3008 - accuracy: 0.8676 - val_loss: 0.6842 - val_accuracy: 0.6498
Epoch 108/500
231/231 - 31s - loss: 0.2906 - accuracy: 0.8759 - val_loss: 0.6870 - val_accuracy: 0.6411
Epoch 109/500
231/231 - 31s - loss: 0.2928 - accuracy: 0.8755 - val_loss: 0.6892 - val_accuracy: 0.6432
Epoch 110/500
231/231 - 31s - loss: 0.2826 - accuracy: 0.8831 - val_loss: 0.6906 - val_accuracy: 0.6411
Epoch 111/500
231/231 - 31s - loss: 0.2764 - accuracy: 0.8842 - val_loss: 0.6942 - val_accuracy: 0.6411
Epoch 112/500
231/231 - 30s - loss: 0.2917 - accuracy: 0.8778 - val_loss: 0.6931 - val_accuracy: 0.6465
Epoch 113/500
231/231 - 30s - loss: 0.2746 - accuracy: 0.8881 - val_loss: 0.6975 - val_accuracy: 0.6422
Epoch 114/500
231/231 - 30s - loss: 0.2706 - accuracy: 0.8865 - val_loss: 0.6999 - val_accuracy: 0.6411
Epoch 115/500
231/231 - 30s - loss: 0.2685 - accuracy: 0.8899 - val_loss: 0.7033 - val_accuracy: 0.6400
Epoch 116/500
231/231 - 30s - loss: 0.2675 - accuracy: 0.8859 - val_loss: 0.7039 - val_accuracy: 0.6422
Epoch 117/500
231/231 - 30s - loss: 0.2593 - accuracy: 0.8910 - val_loss: 0.7068 - val_accuracy: 0.6454
Epoch 118/500
231/231 - 30s - loss: 0.2562 - accuracy: 0.8927 - val_loss: 0.7092 - val_accuracy: 0.6400
Epoch 119/500
231/231 - 30s - loss: 0.2533 - accuracy: 0.8949 - val_loss: 0.7120 - val_accuracy: 0.6422
Epoch 120/500
231/231 - 30s - loss: 0.2448 - accuracy: 0.8969 - val_loss: 0.7150 - val_accuracy: 0.6443
Epoch 121/500
231/231 - 31s - loss: 0.2428 - accuracy: 0.8980 - val_loss: 0.7181 - val_accuracy: 0.6465
Epoch 122/500
231/231 - 30s - loss: 0.2309 - accuracy: 0.9055 - val_loss: 0.7197 - val_accuracy: 0.6432
Epoch 123/500
231/231 - 30s - loss: 0.2443 - accuracy: 0.9022 - val_loss: 0.7244 - val_accuracy: 0.6454
Epoch 124/500
231/231 - 30s - loss: 0.2298 - accuracy: 0.9055 - val_loss: 0.7254 - val_accuracy: 0.6454
Epoch 125/500
231/231 - 30s - loss: 0.2322 - accuracy: 0.9054 - val_loss: 0.7278 - val_accuracy: 0.6465
Epoch 126/500
231/231 - 30s - loss: 0.2248 - accuracy: 0.9103 - val_loss: 0.7314 - val_accuracy: 0.6465
Epoch 127/500
231/231 - 30s - loss: 0.2193 - accuracy: 0.9109 - val_loss: 0.7349 - val_accuracy: 0.6465
========================================
save_weights
h5_weights/CM.po/embedding_cnn_two_branch.h5
========================================

end time >>> Sun Oct  3 03:54:32 2021

end time >>> Sun Oct  3 03:54:32 2021

end time >>> Sun Oct  3 03:54:32 2021

end time >>> Sun Oct  3 03:54:32 2021

end time >>> Sun Oct  3 03:54:32 2021












args.model = embedding_cnn_two_branch
time used = 4000.1380043029785


