************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 07:26:05 2021

begin time >>> Mon Oct  4 07:26:05 2021

begin time >>> Mon Oct  4 07:26:05 2021

begin time >>> Mon Oct  4 07:26:05 2021

begin time >>> Mon Oct  4 07:26:05 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_dense
args.type = train
args.name = X5628FC.pp
args.length = 10001
===========================


-> h5_weights/X5628FC.pp folder already exist. pass.
-> result/X5628FC.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/X5628FC.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/X5628FC.pp/onehot_embedding_dense folder already exist. pass.
-> result/X5628FC.pp/onehot_dense folder already exist. pass.
-> result/X5628FC.pp/onehot_resnet18 folder already exist. pass.
-> result/X5628FC.pp/onehot_resnet34 folder already exist. pass.
-> result/X5628FC.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/X5628FC.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/X5628FC.pp/embedding_dense folder already exist. pass.
-> result/X5628FC.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/X5628FC.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
X5628FC.pp
########################################

########################################
model_name
embedding_dense
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 626, 64)      409664      concatenate[0][0]                
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, 38, 64)       0           conv1d[0][0]                     
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 38, 64)       256         max_pooling1d[0][0]              
__________________________________________________________________________________________________
flatten (Flatten)               (None, 2432)         0           batch_normalization[0][0]        
__________________________________________________________________________________________________
dropout (Dropout)               (None, 2432)         0           flatten[0][0]                    
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          1245696     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation (Activation)         (None, 512)          0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation[0][0]                 
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 512)          0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_1[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 512)          2048        dense_2[0][0]                    
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 512)          0           activation_2[0][0]               
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            513         dropout_3[0][0]                  
==================================================================================================
Total params: 3,006,985
Trainable params: 3,003,785
Non-trainable params: 3,200
__________________________________________________________________________________________________
Epoch 1/500
226/226 - 30s - loss: 0.9189 - accuracy: 0.5089 - val_loss: 0.7009 - val_accuracy: 0.4882
Epoch 2/500
226/226 - 30s - loss: 0.9101 - accuracy: 0.4963 - val_loss: 0.7043 - val_accuracy: 0.4894
Epoch 3/500
226/226 - 30s - loss: 0.8981 - accuracy: 0.5023 - val_loss: 0.7037 - val_accuracy: 0.4871
Epoch 4/500
226/226 - 30s - loss: 0.8966 - accuracy: 0.4938 - val_loss: 0.7021 - val_accuracy: 0.4972
Epoch 5/500
226/226 - 30s - loss: 0.8616 - accuracy: 0.5027 - val_loss: 0.6993 - val_accuracy: 0.5095
Epoch 6/500
226/226 - 30s - loss: 0.8728 - accuracy: 0.5021 - val_loss: 0.6978 - val_accuracy: 0.5062
Epoch 7/500
226/226 - 30s - loss: 0.8655 - accuracy: 0.4965 - val_loss: 0.6969 - val_accuracy: 0.5062
Epoch 8/500
226/226 - 30s - loss: 0.8557 - accuracy: 0.5001 - val_loss: 0.6957 - val_accuracy: 0.5140
Epoch 9/500
226/226 - 30s - loss: 0.8495 - accuracy: 0.5039 - val_loss: 0.6943 - val_accuracy: 0.5252
Epoch 10/500
226/226 - 30s - loss: 0.8427 - accuracy: 0.5132 - val_loss: 0.6938 - val_accuracy: 0.5286
Epoch 11/500
226/226 - 30s - loss: 0.8463 - accuracy: 0.5139 - val_loss: 0.6932 - val_accuracy: 0.5308
Epoch 12/500
226/226 - 30s - loss: 0.8411 - accuracy: 0.5113 - val_loss: 0.6927 - val_accuracy: 0.5263
Epoch 13/500
226/226 - 30s - loss: 0.8429 - accuracy: 0.5110 - val_loss: 0.6917 - val_accuracy: 0.5319
Epoch 14/500
226/226 - 30s - loss: 0.8370 - accuracy: 0.5139 - val_loss: 0.6909 - val_accuracy: 0.5319
Epoch 15/500
226/226 - 30s - loss: 0.8397 - accuracy: 0.5071 - val_loss: 0.6906 - val_accuracy: 0.5252
Epoch 16/500
226/226 - 30s - loss: 0.8305 - accuracy: 0.5171 - val_loss: 0.6899 - val_accuracy: 0.5342
Epoch 17/500
226/226 - 30s - loss: 0.8354 - accuracy: 0.5185 - val_loss: 0.6898 - val_accuracy: 0.5319
Epoch 18/500
226/226 - 30s - loss: 0.8449 - accuracy: 0.5049 - val_loss: 0.6889 - val_accuracy: 0.5409
Epoch 19/500
226/226 - 30s - loss: 0.8361 - accuracy: 0.5084 - val_loss: 0.6879 - val_accuracy: 0.5398
Epoch 20/500
226/226 - 30s - loss: 0.8363 - accuracy: 0.5135 - val_loss: 0.6876 - val_accuracy: 0.5442
Epoch 21/500
226/226 - 30s - loss: 0.8331 - accuracy: 0.5116 - val_loss: 0.6873 - val_accuracy: 0.5487
Epoch 22/500
226/226 - 30s - loss: 0.8295 - accuracy: 0.5142 - val_loss: 0.6867 - val_accuracy: 0.5498
Epoch 23/500
226/226 - 30s - loss: 0.8368 - accuracy: 0.5075 - val_loss: 0.6863 - val_accuracy: 0.5498
Epoch 24/500
226/226 - 30s - loss: 0.8181 - accuracy: 0.5193 - val_loss: 0.6856 - val_accuracy: 0.5532
Epoch 25/500
226/226 - 30s - loss: 0.8335 - accuracy: 0.5064 - val_loss: 0.6855 - val_accuracy: 0.5521
Epoch 26/500
226/226 - 30s - loss: 0.8069 - accuracy: 0.5250 - val_loss: 0.6851 - val_accuracy: 0.5599
Epoch 27/500
226/226 - 30s - loss: 0.8219 - accuracy: 0.5217 - val_loss: 0.6851 - val_accuracy: 0.5566
Epoch 28/500
226/226 - 30s - loss: 0.8099 - accuracy: 0.5296 - val_loss: 0.6844 - val_accuracy: 0.5566
Epoch 29/500
226/226 - 30s - loss: 0.8180 - accuracy: 0.5164 - val_loss: 0.6840 - val_accuracy: 0.5521
Epoch 30/500
226/226 - 30s - loss: 0.8119 - accuracy: 0.5267 - val_loss: 0.6834 - val_accuracy: 0.5599
Epoch 31/500
226/226 - 30s - loss: 0.8063 - accuracy: 0.5265 - val_loss: 0.6832 - val_accuracy: 0.5566
Epoch 32/500
226/226 - 30s - loss: 0.8102 - accuracy: 0.5177 - val_loss: 0.6831 - val_accuracy: 0.5521
Epoch 33/500
226/226 - 30s - loss: 0.8043 - accuracy: 0.5244 - val_loss: 0.6828 - val_accuracy: 0.5588
Epoch 34/500
226/226 - 30s - loss: 0.8168 - accuracy: 0.5171 - val_loss: 0.6821 - val_accuracy: 0.5633
Epoch 35/500
226/226 - 30s - loss: 0.8101 - accuracy: 0.5231 - val_loss: 0.6815 - val_accuracy: 0.5666
Epoch 36/500
226/226 - 30s - loss: 0.7990 - accuracy: 0.5314 - val_loss: 0.6807 - val_accuracy: 0.5622
Epoch 37/500
226/226 - 30s - loss: 0.8106 - accuracy: 0.5186 - val_loss: 0.6804 - val_accuracy: 0.5644
Epoch 38/500
226/226 - 30s - loss: 0.8030 - accuracy: 0.5237 - val_loss: 0.6797 - val_accuracy: 0.5610
Epoch 39/500
226/226 - 30s - loss: 0.8297 - accuracy: 0.5023 - val_loss: 0.6794 - val_accuracy: 0.5644
Epoch 40/500
226/226 - 30s - loss: 0.7998 - accuracy: 0.5257 - val_loss: 0.6790 - val_accuracy: 0.5711
Epoch 41/500
226/226 - 30s - loss: 0.8095 - accuracy: 0.5156 - val_loss: 0.6786 - val_accuracy: 0.5711
Epoch 42/500
226/226 - 30s - loss: 0.8019 - accuracy: 0.5183 - val_loss: 0.6780 - val_accuracy: 0.5689
Epoch 43/500
226/226 - 30s - loss: 0.7903 - accuracy: 0.5304 - val_loss: 0.6775 - val_accuracy: 0.5722
Epoch 44/500
226/226 - 30s - loss: 0.7851 - accuracy: 0.5420 - val_loss: 0.6772 - val_accuracy: 0.5801
Epoch 45/500
226/226 - 30s - loss: 0.7961 - accuracy: 0.5254 - val_loss: 0.6768 - val_accuracy: 0.5789
Epoch 46/500
226/226 - 30s - loss: 0.7941 - accuracy: 0.5267 - val_loss: 0.6765 - val_accuracy: 0.5756
Epoch 47/500
226/226 - 30s - loss: 0.7962 - accuracy: 0.5369 - val_loss: 0.6758 - val_accuracy: 0.5767
Epoch 48/500
226/226 - 30s - loss: 0.8029 - accuracy: 0.5179 - val_loss: 0.6754 - val_accuracy: 0.5778
Epoch 49/500
226/226 - 30s - loss: 0.7824 - accuracy: 0.5402 - val_loss: 0.6749 - val_accuracy: 0.5767
Epoch 50/500
226/226 - 30s - loss: 0.7852 - accuracy: 0.5332 - val_loss: 0.6744 - val_accuracy: 0.5812
Epoch 51/500
226/226 - 30s - loss: 0.7892 - accuracy: 0.5258 - val_loss: 0.6741 - val_accuracy: 0.5834
Epoch 52/500
226/226 - 30s - loss: 0.7942 - accuracy: 0.5254 - val_loss: 0.6734 - val_accuracy: 0.5745
Epoch 53/500
226/226 - 30s - loss: 0.7862 - accuracy: 0.5333 - val_loss: 0.6732 - val_accuracy: 0.5722
Epoch 54/500
226/226 - 30s - loss: 0.8004 - accuracy: 0.5181 - val_loss: 0.6728 - val_accuracy: 0.5711
Epoch 55/500
226/226 - 30s - loss: 0.7757 - accuracy: 0.5453 - val_loss: 0.6718 - val_accuracy: 0.5801
Epoch 56/500
226/226 - 30s - loss: 0.7848 - accuracy: 0.5314 - val_loss: 0.6712 - val_accuracy: 0.5823
Epoch 57/500
226/226 - 30s - loss: 0.7781 - accuracy: 0.5365 - val_loss: 0.6708 - val_accuracy: 0.5845
Epoch 58/500
226/226 - 30s - loss: 0.7787 - accuracy: 0.5377 - val_loss: 0.6701 - val_accuracy: 0.5857
Epoch 59/500
226/226 - 30s - loss: 0.7808 - accuracy: 0.5435 - val_loss: 0.6696 - val_accuracy: 0.5823
Epoch 60/500
226/226 - 30s - loss: 0.7739 - accuracy: 0.5424 - val_loss: 0.6690 - val_accuracy: 0.5845
Epoch 61/500
226/226 - 30s - loss: 0.7749 - accuracy: 0.5380 - val_loss: 0.6685 - val_accuracy: 0.5879
Epoch 62/500
226/226 - 30s - loss: 0.7638 - accuracy: 0.5474 - val_loss: 0.6675 - val_accuracy: 0.5935
Epoch 63/500
226/226 - 30s - loss: 0.7697 - accuracy: 0.5469 - val_loss: 0.6669 - val_accuracy: 0.5890
Epoch 64/500
226/226 - 30s - loss: 0.7563 - accuracy: 0.5548 - val_loss: 0.6667 - val_accuracy: 0.5991
Epoch 65/500
226/226 - 30s - loss: 0.7700 - accuracy: 0.5477 - val_loss: 0.6660 - val_accuracy: 0.6002
Epoch 66/500
226/226 - 30s - loss: 0.7637 - accuracy: 0.5466 - val_loss: 0.6649 - val_accuracy: 0.6002
Epoch 67/500
226/226 - 30s - loss: 0.7553 - accuracy: 0.5610 - val_loss: 0.6638 - val_accuracy: 0.6013
Epoch 68/500
226/226 - 30s - loss: 0.7635 - accuracy: 0.5514 - val_loss: 0.6632 - val_accuracy: 0.6047
Epoch 69/500
226/226 - 30s - loss: 0.7545 - accuracy: 0.5610 - val_loss: 0.6628 - val_accuracy: 0.6092
Epoch 70/500
226/226 - 30s - loss: 0.7597 - accuracy: 0.5575 - val_loss: 0.6616 - val_accuracy: 0.6137
Epoch 71/500
226/226 - 30s - loss: 0.7580 - accuracy: 0.5599 - val_loss: 0.6609 - val_accuracy: 0.6103
Epoch 72/500
226/226 - 30s - loss: 0.7575 - accuracy: 0.5566 - val_loss: 0.6602 - val_accuracy: 0.6125
Epoch 73/500
226/226 - 30s - loss: 0.7494 - accuracy: 0.5615 - val_loss: 0.6592 - val_accuracy: 0.6159
Epoch 74/500
226/226 - 30s - loss: 0.7463 - accuracy: 0.5656 - val_loss: 0.6581 - val_accuracy: 0.6159
Epoch 75/500
226/226 - 30s - loss: 0.7625 - accuracy: 0.5590 - val_loss: 0.6572 - val_accuracy: 0.6125
Epoch 76/500
226/226 - 30s - loss: 0.7355 - accuracy: 0.5726 - val_loss: 0.6566 - val_accuracy: 0.6148
Epoch 77/500
226/226 - 30s - loss: 0.7450 - accuracy: 0.5697 - val_loss: 0.6564 - val_accuracy: 0.6170
Epoch 78/500
226/226 - 30s - loss: 0.7436 - accuracy: 0.5721 - val_loss: 0.6550 - val_accuracy: 0.6193
Epoch 79/500
226/226 - 30s - loss: 0.7345 - accuracy: 0.5795 - val_loss: 0.6539 - val_accuracy: 0.6181
Epoch 80/500
226/226 - 30s - loss: 0.7328 - accuracy: 0.5794 - val_loss: 0.6533 - val_accuracy: 0.6181
Epoch 81/500
226/226 - 30s - loss: 0.7581 - accuracy: 0.5650 - val_loss: 0.6519 - val_accuracy: 0.6181
Epoch 82/500
226/226 - 30s - loss: 0.7368 - accuracy: 0.5786 - val_loss: 0.6509 - val_accuracy: 0.6215
Epoch 83/500
226/226 - 30s - loss: 0.7260 - accuracy: 0.5755 - val_loss: 0.6499 - val_accuracy: 0.6249
Epoch 84/500
226/226 - 30s - loss: 0.7274 - accuracy: 0.5795 - val_loss: 0.6492 - val_accuracy: 0.6338
Epoch 85/500
226/226 - 30s - loss: 0.7212 - accuracy: 0.5896 - val_loss: 0.6476 - val_accuracy: 0.6305
Epoch 86/500
226/226 - 30s - loss: 0.7145 - accuracy: 0.5905 - val_loss: 0.6471 - val_accuracy: 0.6293
Epoch 87/500
226/226 - 30s - loss: 0.7215 - accuracy: 0.5892 - val_loss: 0.6463 - val_accuracy: 0.6293
Epoch 88/500
226/226 - 30s - loss: 0.7338 - accuracy: 0.5769 - val_loss: 0.6447 - val_accuracy: 0.6316
Epoch 89/500
226/226 - 30s - loss: 0.7090 - accuracy: 0.5998 - val_loss: 0.6440 - val_accuracy: 0.6271
Epoch 90/500
226/226 - 30s - loss: 0.7037 - accuracy: 0.5960 - val_loss: 0.6427 - val_accuracy: 0.6293
Epoch 91/500
226/226 - 30s - loss: 0.7044 - accuracy: 0.5995 - val_loss: 0.6414 - val_accuracy: 0.6372
Epoch 92/500
226/226 - 30s - loss: 0.7062 - accuracy: 0.6009 - val_loss: 0.6404 - val_accuracy: 0.6372
Epoch 93/500
226/226 - 30s - loss: 0.6989 - accuracy: 0.6090 - val_loss: 0.6394 - val_accuracy: 0.6450
Epoch 94/500
226/226 - 30s - loss: 0.7139 - accuracy: 0.5989 - val_loss: 0.6382 - val_accuracy: 0.6428
Epoch 95/500
226/226 - 30s - loss: 0.6943 - accuracy: 0.6035 - val_loss: 0.6376 - val_accuracy: 0.6450
Epoch 96/500
226/226 - 30s - loss: 0.7014 - accuracy: 0.6020 - val_loss: 0.6363 - val_accuracy: 0.6428
Epoch 97/500
226/226 - 30s - loss: 0.7010 - accuracy: 0.6014 - val_loss: 0.6358 - val_accuracy: 0.6439
Epoch 98/500
226/226 - 30s - loss: 0.6955 - accuracy: 0.6112 - val_loss: 0.6336 - val_accuracy: 0.6517
Epoch 99/500
226/226 - 30s - loss: 0.6962 - accuracy: 0.6085 - val_loss: 0.6329 - val_accuracy: 0.6473
Epoch 100/500
226/226 - 30s - loss: 0.6804 - accuracy: 0.6213 - val_loss: 0.6319 - val_accuracy: 0.6517
Epoch 101/500
226/226 - 30s - loss: 0.6820 - accuracy: 0.6209 - val_loss: 0.6316 - val_accuracy: 0.6517
Epoch 102/500
226/226 - 30s - loss: 0.6744 - accuracy: 0.6317 - val_loss: 0.6303 - val_accuracy: 0.6562
Epoch 103/500
226/226 - 30s - loss: 0.6700 - accuracy: 0.6388 - val_loss: 0.6296 - val_accuracy: 0.6551
Epoch 104/500
226/226 - 30s - loss: 0.6775 - accuracy: 0.6339 - val_loss: 0.6288 - val_accuracy: 0.6573
Epoch 105/500
226/226 - 30s - loss: 0.6667 - accuracy: 0.6375 - val_loss: 0.6275 - val_accuracy: 0.6596
Epoch 106/500
226/226 - 30s - loss: 0.6685 - accuracy: 0.6373 - val_loss: 0.6264 - val_accuracy: 0.6607
Epoch 107/500
226/226 - 30s - loss: 0.6719 - accuracy: 0.6355 - val_loss: 0.6255 - val_accuracy: 0.6629
Epoch 108/500
226/226 - 30s - loss: 0.6662 - accuracy: 0.6323 - val_loss: 0.6249 - val_accuracy: 0.6618
Epoch 109/500
226/226 - 30s - loss: 0.6525 - accuracy: 0.6443 - val_loss: 0.6236 - val_accuracy: 0.6618
Epoch 110/500
226/226 - 30s - loss: 0.6483 - accuracy: 0.6483 - val_loss: 0.6239 - val_accuracy: 0.6652
Epoch 111/500
226/226 - 30s - loss: 0.6411 - accuracy: 0.6585 - val_loss: 0.6225 - val_accuracy: 0.6641
Epoch 112/500
226/226 - 30s - loss: 0.6462 - accuracy: 0.6561 - val_loss: 0.6208 - val_accuracy: 0.6652
Epoch 113/500
226/226 - 30s - loss: 0.6532 - accuracy: 0.6472 - val_loss: 0.6204 - val_accuracy: 0.6685
Epoch 114/500
226/226 - 30s - loss: 0.6327 - accuracy: 0.6585 - val_loss: 0.6210 - val_accuracy: 0.6685
Epoch 115/500
226/226 - 30s - loss: 0.6463 - accuracy: 0.6531 - val_loss: 0.6200 - val_accuracy: 0.6663
Epoch 116/500
226/226 - 30s - loss: 0.6292 - accuracy: 0.6651 - val_loss: 0.6197 - val_accuracy: 0.6674
Epoch 117/500
226/226 - 30s - loss: 0.6138 - accuracy: 0.6745 - val_loss: 0.6176 - val_accuracy: 0.6697
Epoch 118/500
226/226 - 30s - loss: 0.6171 - accuracy: 0.6709 - val_loss: 0.6176 - val_accuracy: 0.6719
Epoch 119/500
226/226 - 30s - loss: 0.6118 - accuracy: 0.6773 - val_loss: 0.6176 - val_accuracy: 0.6730
Epoch 120/500
226/226 - 30s - loss: 0.6171 - accuracy: 0.6720 - val_loss: 0.6163 - val_accuracy: 0.6719
Epoch 121/500
226/226 - 30s - loss: 0.6093 - accuracy: 0.6888 - val_loss: 0.6168 - val_accuracy: 0.6741
Epoch 122/500
226/226 - 30s - loss: 0.6191 - accuracy: 0.6809 - val_loss: 0.6155 - val_accuracy: 0.6753
Epoch 123/500
226/226 - 30s - loss: 0.6075 - accuracy: 0.6864 - val_loss: 0.6162 - val_accuracy: 0.6741
Epoch 124/500
226/226 - 30s - loss: 0.6153 - accuracy: 0.6806 - val_loss: 0.6166 - val_accuracy: 0.6753
Epoch 125/500
226/226 - 30s - loss: 0.6015 - accuracy: 0.6920 - val_loss: 0.6161 - val_accuracy: 0.6764
Epoch 126/500
226/226 - 30s - loss: 0.5877 - accuracy: 0.6949 - val_loss: 0.6159 - val_accuracy: 0.6775
Epoch 127/500
226/226 - 30s - loss: 0.5793 - accuracy: 0.6964 - val_loss: 0.6140 - val_accuracy: 0.6775
Epoch 128/500
226/226 - 30s - loss: 0.5888 - accuracy: 0.6982 - val_loss: 0.6145 - val_accuracy: 0.6775
Epoch 129/500
226/226 - 30s - loss: 0.5780 - accuracy: 0.7131 - val_loss: 0.6137 - val_accuracy: 0.6775
Epoch 130/500
226/226 - 30s - loss: 0.5809 - accuracy: 0.7030 - val_loss: 0.6152 - val_accuracy: 0.6809
Epoch 131/500
226/226 - 30s - loss: 0.5704 - accuracy: 0.7162 - val_loss: 0.6152 - val_accuracy: 0.6797
Epoch 132/500
226/226 - 30s - loss: 0.5692 - accuracy: 0.7172 - val_loss: 0.6152 - val_accuracy: 0.6797
Epoch 133/500
226/226 - 30s - loss: 0.5676 - accuracy: 0.7129 - val_loss: 0.6151 - val_accuracy: 0.6786
Epoch 134/500
226/226 - 30s - loss: 0.5531 - accuracy: 0.7208 - val_loss: 0.6155 - val_accuracy: 0.6786
Epoch 135/500
226/226 - 30s - loss: 0.5587 - accuracy: 0.7167 - val_loss: 0.6162 - val_accuracy: 0.6775
Epoch 136/500
226/226 - 30s - loss: 0.5525 - accuracy: 0.7288 - val_loss: 0.6173 - val_accuracy: 0.6797
Epoch 137/500
226/226 - 30s - loss: 0.5520 - accuracy: 0.7299 - val_loss: 0.6173 - val_accuracy: 0.6775
Epoch 138/500
226/226 - 30s - loss: 0.5384 - accuracy: 0.7363 - val_loss: 0.6163 - val_accuracy: 0.6797
Epoch 139/500
226/226 - 30s - loss: 0.5415 - accuracy: 0.7361 - val_loss: 0.6174 - val_accuracy: 0.6831
Epoch 140/500
226/226 - 30s - loss: 0.5407 - accuracy: 0.7340 - val_loss: 0.6184 - val_accuracy: 0.6820
Epoch 141/500
226/226 - 30s - loss: 0.5298 - accuracy: 0.7442 - val_loss: 0.6177 - val_accuracy: 0.6831
Epoch 142/500
226/226 - 30s - loss: 0.5216 - accuracy: 0.7476 - val_loss: 0.6183 - val_accuracy: 0.6831
Epoch 143/500
226/226 - 30s - loss: 0.5127 - accuracy: 0.7518 - val_loss: 0.6188 - val_accuracy: 0.6809
Epoch 144/500
226/226 - 30s - loss: 0.5109 - accuracy: 0.7489 - val_loss: 0.6198 - val_accuracy: 0.6820
Epoch 145/500
226/226 - 30s - loss: 0.5155 - accuracy: 0.7533 - val_loss: 0.6224 - val_accuracy: 0.6831
Epoch 146/500
226/226 - 30s - loss: 0.5116 - accuracy: 0.7530 - val_loss: 0.6213 - val_accuracy: 0.6842
Epoch 147/500
226/226 - 30s - loss: 0.5094 - accuracy: 0.7534 - val_loss: 0.6229 - val_accuracy: 0.6876
Epoch 148/500
226/226 - 30s - loss: 0.4949 - accuracy: 0.7627 - val_loss: 0.6243 - val_accuracy: 0.6853
Epoch 149/500
226/226 - 30s - loss: 0.4926 - accuracy: 0.7638 - val_loss: 0.6251 - val_accuracy: 0.6876
Epoch 150/500
226/226 - 30s - loss: 0.4820 - accuracy: 0.7691 - val_loss: 0.6257 - val_accuracy: 0.6909
Epoch 151/500
226/226 - 30s - loss: 0.4999 - accuracy: 0.7599 - val_loss: 0.6269 - val_accuracy: 0.6865
Epoch 152/500
226/226 - 30s - loss: 0.4886 - accuracy: 0.7657 - val_loss: 0.6274 - val_accuracy: 0.6865
Epoch 153/500
226/226 - 30s - loss: 0.4807 - accuracy: 0.7754 - val_loss: 0.6292 - val_accuracy: 0.6865
Epoch 154/500
226/226 - 30s - loss: 0.4658 - accuracy: 0.7768 - val_loss: 0.6306 - val_accuracy: 0.6876
Epoch 155/500
226/226 - 30s - loss: 0.4637 - accuracy: 0.7824 - val_loss: 0.6309 - val_accuracy: 0.6853
Epoch 156/500
226/226 - 30s - loss: 0.4853 - accuracy: 0.7742 - val_loss: 0.6329 - val_accuracy: 0.6853
Epoch 157/500
226/226 - 30s - loss: 0.4673 - accuracy: 0.7860 - val_loss: 0.6330 - val_accuracy: 0.6876
Epoch 158/500
226/226 - 30s - loss: 0.4646 - accuracy: 0.7853 - val_loss: 0.6359 - val_accuracy: 0.6876
Epoch 159/500
226/226 - 30s - loss: 0.4620 - accuracy: 0.7811 - val_loss: 0.6372 - val_accuracy: 0.6876
Epoch 160/500
226/226 - 30s - loss: 0.4462 - accuracy: 0.7908 - val_loss: 0.6374 - val_accuracy: 0.6920
Epoch 161/500
226/226 - 30s - loss: 0.4506 - accuracy: 0.7900 - val_loss: 0.6386 - val_accuracy: 0.6898
Epoch 162/500
226/226 - 30s - loss: 0.4478 - accuracy: 0.7885 - val_loss: 0.6413 - val_accuracy: 0.6898
Epoch 163/500
226/226 - 30s - loss: 0.4380 - accuracy: 0.7991 - val_loss: 0.6423 - val_accuracy: 0.6887
Epoch 164/500
226/226 - 30s - loss: 0.4338 - accuracy: 0.8020 - val_loss: 0.6446 - val_accuracy: 0.6909
Epoch 165/500
226/226 - 30s - loss: 0.4318 - accuracy: 0.8033 - val_loss: 0.6453 - val_accuracy: 0.6876
Epoch 166/500
226/226 - 30s - loss: 0.4181 - accuracy: 0.8103 - val_loss: 0.6491 - val_accuracy: 0.6887
Epoch 167/500
226/226 - 30s - loss: 0.4268 - accuracy: 0.8038 - val_loss: 0.6495 - val_accuracy: 0.6887
Epoch 168/500
226/226 - 30s - loss: 0.4208 - accuracy: 0.8094 - val_loss: 0.6520 - val_accuracy: 0.6865
Epoch 169/500
226/226 - 30s - loss: 0.4142 - accuracy: 0.8119 - val_loss: 0.6548 - val_accuracy: 0.6853
Epoch 170/500
226/226 - 30s - loss: 0.4231 - accuracy: 0.8107 - val_loss: 0.6569 - val_accuracy: 0.6842
Epoch 171/500
226/226 - 30s - loss: 0.4141 - accuracy: 0.8146 - val_loss: 0.6591 - val_accuracy: 0.6853
Epoch 172/500
226/226 - 30s - loss: 0.4092 - accuracy: 0.8188 - val_loss: 0.6582 - val_accuracy: 0.6865
Epoch 173/500
226/226 - 30s - loss: 0.4044 - accuracy: 0.8157 - val_loss: 0.6621 - val_accuracy: 0.6853
Epoch 174/500
226/226 - 30s - loss: 0.4005 - accuracy: 0.8236 - val_loss: 0.6635 - val_accuracy: 0.6887
Epoch 175/500
226/226 - 30s - loss: 0.3964 - accuracy: 0.8217 - val_loss: 0.6668 - val_accuracy: 0.6853
Epoch 176/500
226/226 - 30s - loss: 0.3862 - accuracy: 0.8274 - val_loss: 0.6693 - val_accuracy: 0.6898
Epoch 177/500
226/226 - 30s - loss: 0.3909 - accuracy: 0.8232 - val_loss: 0.6717 - val_accuracy: 0.6887
Epoch 178/500
226/226 - 30s - loss: 0.3828 - accuracy: 0.8296 - val_loss: 0.6717 - val_accuracy: 0.6865
Epoch 179/500
226/226 - 30s - loss: 0.3853 - accuracy: 0.8258 - val_loss: 0.6766 - val_accuracy: 0.6920
Epoch 180/500
226/226 - 30s - loss: 0.3780 - accuracy: 0.8311 - val_loss: 0.6787 - val_accuracy: 0.6909
========================================
save_weights
h5_weights/X5628FC.pp/embedding_dense.h5
========================================

end time >>> Mon Oct  4 08:55:57 2021

end time >>> Mon Oct  4 08:55:57 2021

end time >>> Mon Oct  4 08:55:57 2021

end time >>> Mon Oct  4 08:55:57 2021

end time >>> Mon Oct  4 08:55:57 2021












args.model = embedding_dense
time used = 5391.854163885117


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 08:55:58 2021

begin time >>> Mon Oct  4 08:55:58 2021

begin time >>> Mon Oct  4 08:55:58 2021

begin time >>> Mon Oct  4 08:55:58 2021

begin time >>> Mon Oct  4 08:55:58 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_cnn_one_branch
args.type = train
args.name = X5628FC.pp
args.length = 10001
===========================


-> h5_weights/X5628FC.pp folder already exist. pass.
-> result/X5628FC.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/X5628FC.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/X5628FC.pp/onehot_embedding_dense folder already exist. pass.
-> result/X5628FC.pp/onehot_dense folder already exist. pass.
-> result/X5628FC.pp/onehot_resnet18 folder already exist. pass.
-> result/X5628FC.pp/onehot_resnet34 folder already exist. pass.
-> result/X5628FC.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/X5628FC.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/X5628FC.pp/embedding_dense folder already exist. pass.
-> result/X5628FC.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/X5628FC.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
X5628FC.pp
########################################

########################################
model_name
embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
226/226 - 32s - loss: 0.9025 - accuracy: 0.5012 - val_loss: 0.6934 - val_accuracy: 0.5062
Epoch 2/500
226/226 - 31s - loss: 0.8794 - accuracy: 0.5106 - val_loss: 0.7000 - val_accuracy: 0.5162
Epoch 3/500
226/226 - 31s - loss: 0.8821 - accuracy: 0.5021 - val_loss: 0.6996 - val_accuracy: 0.5308
Epoch 4/500
226/226 - 31s - loss: 0.8721 - accuracy: 0.5189 - val_loss: 0.6950 - val_accuracy: 0.5330
Epoch 5/500
226/226 - 31s - loss: 0.8592 - accuracy: 0.5123 - val_loss: 0.6913 - val_accuracy: 0.5375
Epoch 6/500
226/226 - 31s - loss: 0.8317 - accuracy: 0.5219 - val_loss: 0.6878 - val_accuracy: 0.5521
Epoch 7/500
226/226 - 31s - loss: 0.8314 - accuracy: 0.5197 - val_loss: 0.6852 - val_accuracy: 0.5566
Epoch 8/500
226/226 - 31s - loss: 0.8273 - accuracy: 0.5289 - val_loss: 0.6819 - val_accuracy: 0.5599
Epoch 9/500
226/226 - 31s - loss: 0.8199 - accuracy: 0.5372 - val_loss: 0.6778 - val_accuracy: 0.5711
Epoch 10/500
226/226 - 31s - loss: 0.8059 - accuracy: 0.5404 - val_loss: 0.6752 - val_accuracy: 0.5834
Epoch 11/500
226/226 - 31s - loss: 0.8032 - accuracy: 0.5430 - val_loss: 0.6724 - val_accuracy: 0.5879
Epoch 12/500
226/226 - 31s - loss: 0.8016 - accuracy: 0.5470 - val_loss: 0.6702 - val_accuracy: 0.5924
Epoch 13/500
226/226 - 31s - loss: 0.7967 - accuracy: 0.5488 - val_loss: 0.6678 - val_accuracy: 0.5946
Epoch 14/500
226/226 - 31s - loss: 0.7766 - accuracy: 0.5603 - val_loss: 0.6658 - val_accuracy: 0.6002
Epoch 15/500
226/226 - 31s - loss: 0.7769 - accuracy: 0.5640 - val_loss: 0.6636 - val_accuracy: 0.6047
Epoch 16/500
226/226 - 31s - loss: 0.7822 - accuracy: 0.5502 - val_loss: 0.6615 - val_accuracy: 0.6092
Epoch 17/500
226/226 - 31s - loss: 0.7679 - accuracy: 0.5642 - val_loss: 0.6596 - val_accuracy: 0.6069
Epoch 18/500
226/226 - 31s - loss: 0.7585 - accuracy: 0.5657 - val_loss: 0.6579 - val_accuracy: 0.6148
Epoch 19/500
226/226 - 31s - loss: 0.7532 - accuracy: 0.5786 - val_loss: 0.6554 - val_accuracy: 0.6148
Epoch 20/500
226/226 - 31s - loss: 0.7458 - accuracy: 0.5754 - val_loss: 0.6534 - val_accuracy: 0.6159
Epoch 21/500
226/226 - 31s - loss: 0.7590 - accuracy: 0.5718 - val_loss: 0.6521 - val_accuracy: 0.6249
Epoch 22/500
226/226 - 31s - loss: 0.7446 - accuracy: 0.5793 - val_loss: 0.6493 - val_accuracy: 0.6282
Epoch 23/500
226/226 - 31s - loss: 0.7408 - accuracy: 0.5923 - val_loss: 0.6478 - val_accuracy: 0.6316
Epoch 24/500
226/226 - 31s - loss: 0.7360 - accuracy: 0.5845 - val_loss: 0.6462 - val_accuracy: 0.6260
Epoch 25/500
226/226 - 31s - loss: 0.7244 - accuracy: 0.5986 - val_loss: 0.6443 - val_accuracy: 0.6305
Epoch 26/500
226/226 - 31s - loss: 0.7266 - accuracy: 0.5924 - val_loss: 0.6429 - val_accuracy: 0.6349
Epoch 27/500
226/226 - 31s - loss: 0.7211 - accuracy: 0.6002 - val_loss: 0.6405 - val_accuracy: 0.6383
Epoch 28/500
226/226 - 31s - loss: 0.7043 - accuracy: 0.6076 - val_loss: 0.6396 - val_accuracy: 0.6372
Epoch 29/500
226/226 - 31s - loss: 0.7088 - accuracy: 0.6082 - val_loss: 0.6386 - val_accuracy: 0.6349
Epoch 30/500
226/226 - 31s - loss: 0.6914 - accuracy: 0.6219 - val_loss: 0.6365 - val_accuracy: 0.6383
Epoch 31/500
226/226 - 31s - loss: 0.6963 - accuracy: 0.6162 - val_loss: 0.6348 - val_accuracy: 0.6461
Epoch 32/500
226/226 - 31s - loss: 0.6910 - accuracy: 0.6193 - val_loss: 0.6330 - val_accuracy: 0.6495
Epoch 33/500
226/226 - 31s - loss: 0.6770 - accuracy: 0.6226 - val_loss: 0.6316 - val_accuracy: 0.6405
Epoch 34/500
226/226 - 31s - loss: 0.6833 - accuracy: 0.6166 - val_loss: 0.6296 - val_accuracy: 0.6517
Epoch 35/500
226/226 - 31s - loss: 0.6730 - accuracy: 0.6363 - val_loss: 0.6275 - val_accuracy: 0.6562
Epoch 36/500
226/226 - 31s - loss: 0.6699 - accuracy: 0.6324 - val_loss: 0.6268 - val_accuracy: 0.6484
Epoch 37/500
226/226 - 31s - loss: 0.6712 - accuracy: 0.6373 - val_loss: 0.6261 - val_accuracy: 0.6495
Epoch 38/500
226/226 - 31s - loss: 0.6703 - accuracy: 0.6348 - val_loss: 0.6231 - val_accuracy: 0.6540
Epoch 39/500
226/226 - 31s - loss: 0.6647 - accuracy: 0.6331 - val_loss: 0.6226 - val_accuracy: 0.6540
Epoch 40/500
226/226 - 31s - loss: 0.6450 - accuracy: 0.6543 - val_loss: 0.6218 - val_accuracy: 0.6540
Epoch 41/500
226/226 - 31s - loss: 0.6455 - accuracy: 0.6500 - val_loss: 0.6208 - val_accuracy: 0.6517
Epoch 42/500
226/226 - 31s - loss: 0.6462 - accuracy: 0.6497 - val_loss: 0.6198 - val_accuracy: 0.6562
Epoch 43/500
226/226 - 31s - loss: 0.6262 - accuracy: 0.6615 - val_loss: 0.6184 - val_accuracy: 0.6585
Epoch 44/500
226/226 - 31s - loss: 0.6280 - accuracy: 0.6670 - val_loss: 0.6177 - val_accuracy: 0.6562
Epoch 45/500
226/226 - 31s - loss: 0.6347 - accuracy: 0.6641 - val_loss: 0.6160 - val_accuracy: 0.6641
Epoch 46/500
226/226 - 31s - loss: 0.6202 - accuracy: 0.6720 - val_loss: 0.6154 - val_accuracy: 0.6573
Epoch 47/500
226/226 - 31s - loss: 0.6133 - accuracy: 0.6835 - val_loss: 0.6137 - val_accuracy: 0.6663
Epoch 48/500
226/226 - 31s - loss: 0.6011 - accuracy: 0.6890 - val_loss: 0.6119 - val_accuracy: 0.6685
Epoch 49/500
226/226 - 31s - loss: 0.6069 - accuracy: 0.6783 - val_loss: 0.6108 - val_accuracy: 0.6674
Epoch 50/500
226/226 - 31s - loss: 0.5972 - accuracy: 0.6877 - val_loss: 0.6110 - val_accuracy: 0.6730
Epoch 51/500
226/226 - 31s - loss: 0.5878 - accuracy: 0.6867 - val_loss: 0.6103 - val_accuracy: 0.6741
Epoch 52/500
226/226 - 31s - loss: 0.5804 - accuracy: 0.7075 - val_loss: 0.6088 - val_accuracy: 0.6730
Epoch 53/500
226/226 - 31s - loss: 0.5767 - accuracy: 0.7010 - val_loss: 0.6096 - val_accuracy: 0.6730
Epoch 54/500
226/226 - 31s - loss: 0.5774 - accuracy: 0.7026 - val_loss: 0.6076 - val_accuracy: 0.6730
Epoch 55/500
226/226 - 31s - loss: 0.5704 - accuracy: 0.7058 - val_loss: 0.6068 - val_accuracy: 0.6741
Epoch 56/500
226/226 - 31s - loss: 0.5661 - accuracy: 0.7159 - val_loss: 0.6066 - val_accuracy: 0.6741
Epoch 57/500
226/226 - 31s - loss: 0.5676 - accuracy: 0.7129 - val_loss: 0.6067 - val_accuracy: 0.6764
Epoch 58/500
226/226 - 31s - loss: 0.5669 - accuracy: 0.7187 - val_loss: 0.6060 - val_accuracy: 0.6775
Epoch 59/500
226/226 - 31s - loss: 0.5445 - accuracy: 0.7248 - val_loss: 0.6052 - val_accuracy: 0.6775
Epoch 60/500
226/226 - 31s - loss: 0.5495 - accuracy: 0.7284 - val_loss: 0.6061 - val_accuracy: 0.6797
Epoch 61/500
226/226 - 31s - loss: 0.5363 - accuracy: 0.7311 - val_loss: 0.6042 - val_accuracy: 0.6786
Epoch 62/500
226/226 - 31s - loss: 0.5303 - accuracy: 0.7412 - val_loss: 0.6034 - val_accuracy: 0.6809
Epoch 63/500
226/226 - 31s - loss: 0.5396 - accuracy: 0.7314 - val_loss: 0.6044 - val_accuracy: 0.6797
Epoch 64/500
226/226 - 31s - loss: 0.5208 - accuracy: 0.7450 - val_loss: 0.6048 - val_accuracy: 0.6809
Epoch 65/500
226/226 - 31s - loss: 0.5157 - accuracy: 0.7461 - val_loss: 0.6055 - val_accuracy: 0.6876
Epoch 66/500
226/226 - 31s - loss: 0.5091 - accuracy: 0.7489 - val_loss: 0.6047 - val_accuracy: 0.6820
Epoch 67/500
226/226 - 31s - loss: 0.5116 - accuracy: 0.7501 - val_loss: 0.6055 - val_accuracy: 0.6797
Epoch 68/500
226/226 - 31s - loss: 0.4999 - accuracy: 0.7574 - val_loss: 0.6049 - val_accuracy: 0.6809
Epoch 69/500
226/226 - 31s - loss: 0.4983 - accuracy: 0.7608 - val_loss: 0.6053 - val_accuracy: 0.6831
Epoch 70/500
226/226 - 31s - loss: 0.4940 - accuracy: 0.7605 - val_loss: 0.6070 - val_accuracy: 0.6809
Epoch 71/500
226/226 - 31s - loss: 0.4903 - accuracy: 0.7667 - val_loss: 0.6067 - val_accuracy: 0.6809
Epoch 72/500
226/226 - 31s - loss: 0.4808 - accuracy: 0.7724 - val_loss: 0.6068 - val_accuracy: 0.6842
Epoch 73/500
226/226 - 31s - loss: 0.4758 - accuracy: 0.7749 - val_loss: 0.6079 - val_accuracy: 0.6853
Epoch 74/500
226/226 - 31s - loss: 0.4798 - accuracy: 0.7706 - val_loss: 0.6077 - val_accuracy: 0.6887
Epoch 75/500
226/226 - 31s - loss: 0.4656 - accuracy: 0.7754 - val_loss: 0.6080 - val_accuracy: 0.6842
Epoch 76/500
226/226 - 31s - loss: 0.4687 - accuracy: 0.7800 - val_loss: 0.6080 - val_accuracy: 0.6887
Epoch 77/500
226/226 - 31s - loss: 0.4540 - accuracy: 0.7869 - val_loss: 0.6093 - val_accuracy: 0.6887
Epoch 78/500
226/226 - 31s - loss: 0.4611 - accuracy: 0.7815 - val_loss: 0.6106 - val_accuracy: 0.6909
Epoch 79/500
226/226 - 31s - loss: 0.4670 - accuracy: 0.7770 - val_loss: 0.6105 - val_accuracy: 0.6909
Epoch 80/500
226/226 - 31s - loss: 0.4513 - accuracy: 0.7864 - val_loss: 0.6134 - val_accuracy: 0.6887
Epoch 81/500
226/226 - 31s - loss: 0.4477 - accuracy: 0.7898 - val_loss: 0.6149 - val_accuracy: 0.6842
Epoch 82/500
226/226 - 31s - loss: 0.4366 - accuracy: 0.7952 - val_loss: 0.6147 - val_accuracy: 0.6887
Epoch 83/500
226/226 - 31s - loss: 0.4421 - accuracy: 0.7950 - val_loss: 0.6143 - val_accuracy: 0.6920
Epoch 84/500
226/226 - 31s - loss: 0.4241 - accuracy: 0.8033 - val_loss: 0.6165 - val_accuracy: 0.6909
Epoch 85/500
226/226 - 31s - loss: 0.4271 - accuracy: 0.8042 - val_loss: 0.6171 - val_accuracy: 0.6909
Epoch 86/500
226/226 - 31s - loss: 0.4200 - accuracy: 0.8084 - val_loss: 0.6198 - val_accuracy: 0.6865
Epoch 87/500
226/226 - 31s - loss: 0.4095 - accuracy: 0.8166 - val_loss: 0.6195 - val_accuracy: 0.6876
Epoch 88/500
226/226 - 31s - loss: 0.4029 - accuracy: 0.8184 - val_loss: 0.6200 - val_accuracy: 0.6909
Epoch 89/500
226/226 - 31s - loss: 0.4144 - accuracy: 0.8098 - val_loss: 0.6235 - val_accuracy: 0.6853
Epoch 90/500
226/226 - 31s - loss: 0.4021 - accuracy: 0.8145 - val_loss: 0.6245 - val_accuracy: 0.6932
Epoch 91/500
226/226 - 31s - loss: 0.4073 - accuracy: 0.8178 - val_loss: 0.6252 - val_accuracy: 0.6909
Epoch 92/500
226/226 - 31s - loss: 0.3969 - accuracy: 0.8182 - val_loss: 0.6259 - val_accuracy: 0.6876
Epoch 93/500
226/226 - 31s - loss: 0.4057 - accuracy: 0.8121 - val_loss: 0.6275 - val_accuracy: 0.6887
Epoch 94/500
226/226 - 31s - loss: 0.3965 - accuracy: 0.8179 - val_loss: 0.6306 - val_accuracy: 0.6898
Epoch 95/500
226/226 - 31s - loss: 0.3895 - accuracy: 0.8253 - val_loss: 0.6327 - val_accuracy: 0.6842
Epoch 96/500
226/226 - 31s - loss: 0.3891 - accuracy: 0.8296 - val_loss: 0.6336 - val_accuracy: 0.6831
Epoch 97/500
226/226 - 31s - loss: 0.3840 - accuracy: 0.8298 - val_loss: 0.6331 - val_accuracy: 0.6954
Epoch 98/500
226/226 - 31s - loss: 0.3707 - accuracy: 0.8289 - val_loss: 0.6370 - val_accuracy: 0.6876
Epoch 99/500
226/226 - 31s - loss: 0.3660 - accuracy: 0.8373 - val_loss: 0.6402 - val_accuracy: 0.6865
Epoch 100/500
226/226 - 31s - loss: 0.3623 - accuracy: 0.8388 - val_loss: 0.6429 - val_accuracy: 0.6898
Epoch 101/500
226/226 - 31s - loss: 0.3724 - accuracy: 0.8348 - val_loss: 0.6435 - val_accuracy: 0.6887
Epoch 102/500
226/226 - 31s - loss: 0.3585 - accuracy: 0.8395 - val_loss: 0.6438 - val_accuracy: 0.6887
Epoch 103/500
226/226 - 31s - loss: 0.3688 - accuracy: 0.8346 - val_loss: 0.6465 - val_accuracy: 0.6865
Epoch 104/500
226/226 - 31s - loss: 0.3639 - accuracy: 0.8415 - val_loss: 0.6500 - val_accuracy: 0.6865
Epoch 105/500
226/226 - 31s - loss: 0.3682 - accuracy: 0.8373 - val_loss: 0.6486 - val_accuracy: 0.6898
Epoch 106/500
226/226 - 31s - loss: 0.3491 - accuracy: 0.8465 - val_loss: 0.6493 - val_accuracy: 0.6954
Epoch 107/500
226/226 - 31s - loss: 0.3458 - accuracy: 0.8476 - val_loss: 0.6529 - val_accuracy: 0.6853
Epoch 108/500
226/226 - 31s - loss: 0.3314 - accuracy: 0.8519 - val_loss: 0.6540 - val_accuracy: 0.6853
Epoch 109/500
226/226 - 31s - loss: 0.3402 - accuracy: 0.8506 - val_loss: 0.6573 - val_accuracy: 0.6853
Epoch 110/500
226/226 - 31s - loss: 0.3361 - accuracy: 0.8537 - val_loss: 0.6597 - val_accuracy: 0.6865
Epoch 111/500
226/226 - 31s - loss: 0.3273 - accuracy: 0.8603 - val_loss: 0.6608 - val_accuracy: 0.6865
Epoch 112/500
226/226 - 31s - loss: 0.3396 - accuracy: 0.8474 - val_loss: 0.6647 - val_accuracy: 0.6865
Epoch 113/500
226/226 - 31s - loss: 0.3371 - accuracy: 0.8517 - val_loss: 0.6657 - val_accuracy: 0.6842
Epoch 114/500
226/226 - 31s - loss: 0.3315 - accuracy: 0.8575 - val_loss: 0.6710 - val_accuracy: 0.6865
Epoch 115/500
226/226 - 31s - loss: 0.3214 - accuracy: 0.8634 - val_loss: 0.6706 - val_accuracy: 0.6853
Epoch 116/500
226/226 - 31s - loss: 0.3158 - accuracy: 0.8665 - val_loss: 0.6706 - val_accuracy: 0.6909
Epoch 117/500
226/226 - 31s - loss: 0.3161 - accuracy: 0.8635 - val_loss: 0.6761 - val_accuracy: 0.6820
========================================
save_weights
h5_weights/X5628FC.pp/embedding_cnn_one_branch.h5
========================================

end time >>> Mon Oct  4 09:56:18 2021

end time >>> Mon Oct  4 09:56:18 2021

end time >>> Mon Oct  4 09:56:18 2021

end time >>> Mon Oct  4 09:56:18 2021

end time >>> Mon Oct  4 09:56:18 2021












args.model = embedding_cnn_one_branch
time used = 3619.6789181232452


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 09:56:19 2021

begin time >>> Mon Oct  4 09:56:19 2021

begin time >>> Mon Oct  4 09:56:19 2021

begin time >>> Mon Oct  4 09:56:19 2021

begin time >>> Mon Oct  4 09:56:19 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_cnn_two_branch
args.type = train
args.name = X5628FC.pp
args.length = 10001
===========================


-> h5_weights/X5628FC.pp folder already exist. pass.
-> result/X5628FC.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/X5628FC.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/X5628FC.pp/onehot_embedding_dense folder already exist. pass.
-> result/X5628FC.pp/onehot_dense folder already exist. pass.
-> result/X5628FC.pp/onehot_resnet18 folder already exist. pass.
-> result/X5628FC.pp/onehot_resnet34 folder already exist. pass.
-> result/X5628FC.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/X5628FC.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/X5628FC.pp/embedding_dense folder already exist. pass.
-> result/X5628FC.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/X5628FC.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
X5628FC.pp
########################################

########################################
model_name
embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
226/226 - 31s - loss: 0.9073 - accuracy: 0.5020 - val_loss: 0.7160 - val_accuracy: 0.4714
Epoch 2/500
226/226 - 31s - loss: 0.8850 - accuracy: 0.5091 - val_loss: 0.7186 - val_accuracy: 0.4815
Epoch 3/500
226/226 - 31s - loss: 0.8657 - accuracy: 0.5138 - val_loss: 0.7161 - val_accuracy: 0.4994
Epoch 4/500
226/226 - 31s - loss: 0.8552 - accuracy: 0.5183 - val_loss: 0.7095 - val_accuracy: 0.5050
Epoch 5/500
226/226 - 31s - loss: 0.8554 - accuracy: 0.5181 - val_loss: 0.7019 - val_accuracy: 0.5084
Epoch 6/500
226/226 - 31s - loss: 0.8298 - accuracy: 0.5333 - val_loss: 0.6982 - val_accuracy: 0.5185
Epoch 7/500
226/226 - 31s - loss: 0.8430 - accuracy: 0.5240 - val_loss: 0.6935 - val_accuracy: 0.5319
Epoch 8/500
226/226 - 31s - loss: 0.8340 - accuracy: 0.5206 - val_loss: 0.6901 - val_accuracy: 0.5465
Epoch 9/500
226/226 - 31s - loss: 0.8218 - accuracy: 0.5297 - val_loss: 0.6865 - val_accuracy: 0.5510
Epoch 10/500
226/226 - 31s - loss: 0.8116 - accuracy: 0.5351 - val_loss: 0.6836 - val_accuracy: 0.5610
Epoch 11/500
226/226 - 31s - loss: 0.8014 - accuracy: 0.5480 - val_loss: 0.6804 - val_accuracy: 0.5733
Epoch 12/500
226/226 - 31s - loss: 0.7888 - accuracy: 0.5489 - val_loss: 0.6774 - val_accuracy: 0.5745
Epoch 13/500
226/226 - 31s - loss: 0.7817 - accuracy: 0.5586 - val_loss: 0.6749 - val_accuracy: 0.5834
Epoch 14/500
226/226 - 31s - loss: 0.7744 - accuracy: 0.5589 - val_loss: 0.6735 - val_accuracy: 0.5969
Epoch 15/500
226/226 - 31s - loss: 0.7749 - accuracy: 0.5559 - val_loss: 0.6722 - val_accuracy: 0.5957
Epoch 16/500
226/226 - 31s - loss: 0.7774 - accuracy: 0.5553 - val_loss: 0.6689 - val_accuracy: 0.6002
Epoch 17/500
226/226 - 31s - loss: 0.7593 - accuracy: 0.5664 - val_loss: 0.6678 - val_accuracy: 0.6025
Epoch 18/500
226/226 - 31s - loss: 0.7607 - accuracy: 0.5654 - val_loss: 0.6643 - val_accuracy: 0.6148
Epoch 19/500
226/226 - 31s - loss: 0.7464 - accuracy: 0.5783 - val_loss: 0.6628 - val_accuracy: 0.6081
Epoch 20/500
226/226 - 31s - loss: 0.7474 - accuracy: 0.5800 - val_loss: 0.6605 - val_accuracy: 0.6215
Epoch 21/500
226/226 - 31s - loss: 0.7356 - accuracy: 0.5788 - val_loss: 0.6603 - val_accuracy: 0.6204
Epoch 22/500
226/226 - 31s - loss: 0.7316 - accuracy: 0.5866 - val_loss: 0.6578 - val_accuracy: 0.6204
Epoch 23/500
226/226 - 31s - loss: 0.7232 - accuracy: 0.5877 - val_loss: 0.6557 - val_accuracy: 0.6204
Epoch 24/500
226/226 - 31s - loss: 0.7330 - accuracy: 0.5919 - val_loss: 0.6528 - val_accuracy: 0.6305
Epoch 25/500
226/226 - 31s - loss: 0.7154 - accuracy: 0.6000 - val_loss: 0.6524 - val_accuracy: 0.6305
Epoch 26/500
226/226 - 31s - loss: 0.7175 - accuracy: 0.5981 - val_loss: 0.6512 - val_accuracy: 0.6316
Epoch 27/500
226/226 - 31s - loss: 0.6954 - accuracy: 0.6172 - val_loss: 0.6488 - val_accuracy: 0.6361
Epoch 28/500
226/226 - 31s - loss: 0.7028 - accuracy: 0.6114 - val_loss: 0.6458 - val_accuracy: 0.6372
Epoch 29/500
226/226 - 31s - loss: 0.6952 - accuracy: 0.6191 - val_loss: 0.6442 - val_accuracy: 0.6405
Epoch 30/500
226/226 - 31s - loss: 0.6842 - accuracy: 0.6240 - val_loss: 0.6435 - val_accuracy: 0.6439
Epoch 31/500
226/226 - 31s - loss: 0.6812 - accuracy: 0.6277 - val_loss: 0.6410 - val_accuracy: 0.6495
Epoch 32/500
226/226 - 31s - loss: 0.6836 - accuracy: 0.6223 - val_loss: 0.6404 - val_accuracy: 0.6428
Epoch 33/500
226/226 - 31s - loss: 0.6890 - accuracy: 0.6212 - val_loss: 0.6392 - val_accuracy: 0.6439
Epoch 34/500
226/226 - 31s - loss: 0.6680 - accuracy: 0.6331 - val_loss: 0.6370 - val_accuracy: 0.6573
Epoch 35/500
226/226 - 31s - loss: 0.6589 - accuracy: 0.6427 - val_loss: 0.6354 - val_accuracy: 0.6641
Epoch 36/500
226/226 - 31s - loss: 0.6530 - accuracy: 0.6446 - val_loss: 0.6345 - val_accuracy: 0.6585
Epoch 37/500
226/226 - 31s - loss: 0.6500 - accuracy: 0.6452 - val_loss: 0.6339 - val_accuracy: 0.6573
Epoch 38/500
226/226 - 31s - loss: 0.6385 - accuracy: 0.6596 - val_loss: 0.6328 - val_accuracy: 0.6585
Epoch 39/500
226/226 - 31s - loss: 0.6273 - accuracy: 0.6655 - val_loss: 0.6316 - val_accuracy: 0.6596
Epoch 40/500
226/226 - 31s - loss: 0.6262 - accuracy: 0.6645 - val_loss: 0.6296 - val_accuracy: 0.6708
Epoch 41/500
226/226 - 31s - loss: 0.6371 - accuracy: 0.6597 - val_loss: 0.6285 - val_accuracy: 0.6674
Epoch 42/500
226/226 - 31s - loss: 0.6214 - accuracy: 0.6684 - val_loss: 0.6273 - val_accuracy: 0.6697
Epoch 43/500
226/226 - 31s - loss: 0.6087 - accuracy: 0.6796 - val_loss: 0.6274 - val_accuracy: 0.6697
Epoch 44/500
226/226 - 31s - loss: 0.6250 - accuracy: 0.6722 - val_loss: 0.6256 - val_accuracy: 0.6753
Epoch 45/500
226/226 - 30s - loss: 0.6177 - accuracy: 0.6762 - val_loss: 0.6245 - val_accuracy: 0.6730
Epoch 46/500
226/226 - 31s - loss: 0.6113 - accuracy: 0.6806 - val_loss: 0.6235 - val_accuracy: 0.6741
Epoch 47/500
226/226 - 30s - loss: 0.5897 - accuracy: 0.6906 - val_loss: 0.6224 - val_accuracy: 0.6741
Epoch 48/500
226/226 - 31s - loss: 0.5851 - accuracy: 0.6992 - val_loss: 0.6220 - val_accuracy: 0.6753
Epoch 49/500
226/226 - 31s - loss: 0.5877 - accuracy: 0.6982 - val_loss: 0.6204 - val_accuracy: 0.6753
Epoch 50/500
226/226 - 30s - loss: 0.5687 - accuracy: 0.7094 - val_loss: 0.6206 - val_accuracy: 0.6741
Epoch 51/500
226/226 - 31s - loss: 0.5697 - accuracy: 0.7098 - val_loss: 0.6195 - val_accuracy: 0.6797
Epoch 52/500
226/226 - 31s - loss: 0.5574 - accuracy: 0.7228 - val_loss: 0.6193 - val_accuracy: 0.6853
Epoch 53/500
226/226 - 31s - loss: 0.5644 - accuracy: 0.7134 - val_loss: 0.6193 - val_accuracy: 0.6764
Epoch 54/500
226/226 - 30s - loss: 0.5540 - accuracy: 0.7160 - val_loss: 0.6176 - val_accuracy: 0.6797
Epoch 55/500
226/226 - 31s - loss: 0.5531 - accuracy: 0.7231 - val_loss: 0.6171 - val_accuracy: 0.6809
Epoch 56/500
226/226 - 30s - loss: 0.5421 - accuracy: 0.7277 - val_loss: 0.6167 - val_accuracy: 0.6797
Epoch 57/500
226/226 - 31s - loss: 0.5344 - accuracy: 0.7295 - val_loss: 0.6163 - val_accuracy: 0.6786
Epoch 58/500
226/226 - 30s - loss: 0.5290 - accuracy: 0.7357 - val_loss: 0.6183 - val_accuracy: 0.6820
Epoch 59/500
226/226 - 30s - loss: 0.5235 - accuracy: 0.7418 - val_loss: 0.6179 - val_accuracy: 0.6820
Epoch 60/500
226/226 - 31s - loss: 0.5176 - accuracy: 0.7461 - val_loss: 0.6180 - val_accuracy: 0.6775
Epoch 61/500
226/226 - 31s - loss: 0.5146 - accuracy: 0.7454 - val_loss: 0.6176 - val_accuracy: 0.6809
Epoch 62/500
226/226 - 31s - loss: 0.5076 - accuracy: 0.7520 - val_loss: 0.6167 - val_accuracy: 0.6876
Epoch 63/500
226/226 - 30s - loss: 0.5014 - accuracy: 0.7541 - val_loss: 0.6178 - val_accuracy: 0.6853
Epoch 64/500
226/226 - 31s - loss: 0.4878 - accuracy: 0.7592 - val_loss: 0.6180 - val_accuracy: 0.6842
Epoch 65/500
226/226 - 30s - loss: 0.4897 - accuracy: 0.7700 - val_loss: 0.6181 - val_accuracy: 0.6842
Epoch 66/500
226/226 - 31s - loss: 0.4857 - accuracy: 0.7677 - val_loss: 0.6187 - val_accuracy: 0.6887
Epoch 67/500
226/226 - 30s - loss: 0.4812 - accuracy: 0.7720 - val_loss: 0.6192 - val_accuracy: 0.6853
Epoch 68/500
226/226 - 31s - loss: 0.4773 - accuracy: 0.7732 - val_loss: 0.6207 - val_accuracy: 0.6865
Epoch 69/500
226/226 - 31s - loss: 0.4666 - accuracy: 0.7819 - val_loss: 0.6191 - val_accuracy: 0.6865
Epoch 70/500
226/226 - 31s - loss: 0.4732 - accuracy: 0.7753 - val_loss: 0.6217 - val_accuracy: 0.6853
Epoch 71/500
226/226 - 31s - loss: 0.4506 - accuracy: 0.7965 - val_loss: 0.6211 - val_accuracy: 0.6865
Epoch 72/500
226/226 - 31s - loss: 0.4495 - accuracy: 0.7893 - val_loss: 0.6223 - val_accuracy: 0.6853
Epoch 73/500
226/226 - 31s - loss: 0.4567 - accuracy: 0.7807 - val_loss: 0.6216 - val_accuracy: 0.6876
Epoch 74/500
226/226 - 30s - loss: 0.4422 - accuracy: 0.7951 - val_loss: 0.6243 - val_accuracy: 0.6876
Epoch 75/500
226/226 - 31s - loss: 0.4319 - accuracy: 0.8015 - val_loss: 0.6251 - val_accuracy: 0.6887
Epoch 76/500
226/226 - 31s - loss: 0.4302 - accuracy: 0.8034 - val_loss: 0.6236 - val_accuracy: 0.6909
Epoch 77/500
226/226 - 30s - loss: 0.4259 - accuracy: 0.8035 - val_loss: 0.6259 - val_accuracy: 0.6898
Epoch 78/500
226/226 - 31s - loss: 0.4180 - accuracy: 0.8091 - val_loss: 0.6281 - val_accuracy: 0.6909
Epoch 79/500
226/226 - 31s - loss: 0.4171 - accuracy: 0.8019 - val_loss: 0.6296 - val_accuracy: 0.6887
Epoch 80/500
226/226 - 31s - loss: 0.4033 - accuracy: 0.8179 - val_loss: 0.6302 - val_accuracy: 0.6853
Epoch 81/500
226/226 - 31s - loss: 0.4114 - accuracy: 0.8107 - val_loss: 0.6302 - val_accuracy: 0.6909
Epoch 82/500
226/226 - 31s - loss: 0.4097 - accuracy: 0.8146 - val_loss: 0.6335 - val_accuracy: 0.6920
Epoch 83/500
226/226 - 31s - loss: 0.4064 - accuracy: 0.8130 - val_loss: 0.6356 - val_accuracy: 0.6898
Epoch 84/500
226/226 - 31s - loss: 0.3919 - accuracy: 0.8257 - val_loss: 0.6354 - val_accuracy: 0.6920
Epoch 85/500
226/226 - 31s - loss: 0.3913 - accuracy: 0.8275 - val_loss: 0.6383 - val_accuracy: 0.6876
Epoch 86/500
226/226 - 31s - loss: 0.3911 - accuracy: 0.8247 - val_loss: 0.6414 - val_accuracy: 0.6898
Epoch 87/500
226/226 - 31s - loss: 0.3828 - accuracy: 0.8242 - val_loss: 0.6399 - val_accuracy: 0.6865
Epoch 88/500
226/226 - 31s - loss: 0.3704 - accuracy: 0.8366 - val_loss: 0.6422 - val_accuracy: 0.6898
Epoch 89/500
226/226 - 31s - loss: 0.3657 - accuracy: 0.8375 - val_loss: 0.6448 - val_accuracy: 0.6898
Epoch 90/500
226/226 - 31s - loss: 0.3672 - accuracy: 0.8408 - val_loss: 0.6454 - val_accuracy: 0.6865
Epoch 91/500
226/226 - 31s - loss: 0.3689 - accuracy: 0.8398 - val_loss: 0.6489 - val_accuracy: 0.6876
Epoch 92/500
226/226 - 31s - loss: 0.3596 - accuracy: 0.8379 - val_loss: 0.6515 - val_accuracy: 0.6898
Epoch 93/500
226/226 - 31s - loss: 0.3446 - accuracy: 0.8487 - val_loss: 0.6537 - val_accuracy: 0.6865
Epoch 94/500
226/226 - 31s - loss: 0.3620 - accuracy: 0.8372 - val_loss: 0.6544 - val_accuracy: 0.6909
Epoch 95/500
226/226 - 31s - loss: 0.3411 - accuracy: 0.8549 - val_loss: 0.6562 - val_accuracy: 0.6887
Epoch 96/500
226/226 - 31s - loss: 0.3368 - accuracy: 0.8539 - val_loss: 0.6587 - val_accuracy: 0.6876
Epoch 97/500
226/226 - 31s - loss: 0.3303 - accuracy: 0.8598 - val_loss: 0.6632 - val_accuracy: 0.6831
Epoch 98/500
226/226 - 31s - loss: 0.3247 - accuracy: 0.8602 - val_loss: 0.6615 - val_accuracy: 0.6853
Epoch 99/500
226/226 - 31s - loss: 0.3353 - accuracy: 0.8546 - val_loss: 0.6644 - val_accuracy: 0.6887
Epoch 100/500
226/226 - 31s - loss: 0.3391 - accuracy: 0.8514 - val_loss: 0.6678 - val_accuracy: 0.6797
Epoch 101/500
226/226 - 31s - loss: 0.3331 - accuracy: 0.8563 - val_loss: 0.6702 - val_accuracy: 0.6786
Epoch 102/500
226/226 - 31s - loss: 0.3255 - accuracy: 0.8642 - val_loss: 0.6703 - val_accuracy: 0.6809
========================================
save_weights
h5_weights/X5628FC.pp/embedding_cnn_two_branch.h5
========================================

end time >>> Mon Oct  4 10:48:48 2021

end time >>> Mon Oct  4 10:48:48 2021

end time >>> Mon Oct  4 10:48:48 2021

end time >>> Mon Oct  4 10:48:48 2021

end time >>> Mon Oct  4 10:48:48 2021












args.model = embedding_cnn_two_branch
time used = 3149.2982380390167


