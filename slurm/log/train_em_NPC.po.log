************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 01:38:41 2021

begin time >>> Mon Oct  4 01:38:41 2021

begin time >>> Mon Oct  4 01:38:41 2021

begin time >>> Mon Oct  4 01:38:41 2021

begin time >>> Mon Oct  4 01:38:41 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_dense
args.type = train
args.name = NPC.po
args.length = 10001
===========================


-> h5_weights/NPC.po folder already exist. pass.
-> result/NPC.po/onehot_cnn_one_branch folder already exist. pass.
-> result/NPC.po/onehot_cnn_two_branch folder already exist. pass.
-> result/NPC.po/onehot_embedding_dense folder already exist. pass.
-> result/NPC.po/onehot_dense folder already exist. pass.
-> result/NPC.po/onehot_resnet18 folder already exist. pass.
-> result/NPC.po/onehot_resnet34 folder already exist. pass.
-> result/NPC.po/embedding_cnn_one_branch folder already exist. pass.
-> result/NPC.po/embedding_cnn_two_branch folder already exist. pass.
-> result/NPC.po/embedding_dense folder already exist. pass.
-> result/NPC.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/NPC.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
NPC.po
########################################

########################################
model_name
embedding_dense
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 626, 64)      409664      concatenate[0][0]                
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, 38, 64)       0           conv1d[0][0]                     
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 38, 64)       256         max_pooling1d[0][0]              
__________________________________________________________________________________________________
flatten (Flatten)               (None, 2432)         0           batch_normalization[0][0]        
__________________________________________________________________________________________________
dropout (Dropout)               (None, 2432)         0           flatten[0][0]                    
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          1245696     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation (Activation)         (None, 512)          0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation[0][0]                 
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 512)          0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_1[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 512)          2048        dense_2[0][0]                    
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 512)          0           activation_2[0][0]               
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            513         dropout_3[0][0]                  
==================================================================================================
Total params: 3,006,985
Trainable params: 3,003,785
Non-trainable params: 3,200
__________________________________________________________________________________________________
Epoch 1/500
101/101 - 14s - loss: 0.8716 - accuracy: 0.4978 - val_loss: 0.6971 - val_accuracy: 0.4824
Epoch 2/500
101/101 - 13s - loss: 0.8671 - accuracy: 0.4991 - val_loss: 0.6987 - val_accuracy: 0.5000
Epoch 3/500
101/101 - 13s - loss: 0.8696 - accuracy: 0.5059 - val_loss: 0.7009 - val_accuracy: 0.4874
Epoch 4/500
101/101 - 13s - loss: 0.8452 - accuracy: 0.5115 - val_loss: 0.7033 - val_accuracy: 0.4849
Epoch 5/500
101/101 - 13s - loss: 0.8737 - accuracy: 0.4988 - val_loss: 0.7039 - val_accuracy: 0.4975
Epoch 6/500
101/101 - 13s - loss: 0.8634 - accuracy: 0.4947 - val_loss: 0.7038 - val_accuracy: 0.4849
Epoch 7/500
101/101 - 13s - loss: 0.8729 - accuracy: 0.4941 - val_loss: 0.7037 - val_accuracy: 0.4874
Epoch 8/500
101/101 - 13s - loss: 0.8728 - accuracy: 0.4898 - val_loss: 0.7031 - val_accuracy: 0.4899
Epoch 9/500
101/101 - 13s - loss: 0.8363 - accuracy: 0.5202 - val_loss: 0.7034 - val_accuracy: 0.4925
Epoch 10/500
101/101 - 14s - loss: 0.8436 - accuracy: 0.5096 - val_loss: 0.7031 - val_accuracy: 0.5025
Epoch 11/500
101/101 - 13s - loss: 0.8288 - accuracy: 0.5165 - val_loss: 0.7026 - val_accuracy: 0.5126
Epoch 12/500
101/101 - 13s - loss: 0.8293 - accuracy: 0.5224 - val_loss: 0.7022 - val_accuracy: 0.5101
Epoch 13/500
101/101 - 13s - loss: 0.8692 - accuracy: 0.4941 - val_loss: 0.7022 - val_accuracy: 0.4975
Epoch 14/500
101/101 - 13s - loss: 0.8382 - accuracy: 0.5174 - val_loss: 0.7023 - val_accuracy: 0.5025
Epoch 15/500
101/101 - 13s - loss: 0.8496 - accuracy: 0.5096 - val_loss: 0.7021 - val_accuracy: 0.5000
Epoch 16/500
101/101 - 13s - loss: 0.8471 - accuracy: 0.5102 - val_loss: 0.7013 - val_accuracy: 0.5050
Epoch 17/500
101/101 - 13s - loss: 0.8003 - accuracy: 0.5286 - val_loss: 0.7014 - val_accuracy: 0.5050
Epoch 18/500
101/101 - 13s - loss: 0.8305 - accuracy: 0.5084 - val_loss: 0.7013 - val_accuracy: 0.5025
Epoch 19/500
101/101 - 13s - loss: 0.8505 - accuracy: 0.5047 - val_loss: 0.7010 - val_accuracy: 0.5126
Epoch 20/500
101/101 - 13s - loss: 0.8293 - accuracy: 0.5217 - val_loss: 0.7014 - val_accuracy: 0.5101
Epoch 21/500
101/101 - 14s - loss: 0.8258 - accuracy: 0.5267 - val_loss: 0.7015 - val_accuracy: 0.5101
Epoch 22/500
101/101 - 13s - loss: 0.8216 - accuracy: 0.5236 - val_loss: 0.7009 - val_accuracy: 0.5101
Epoch 23/500
101/101 - 13s - loss: 0.8253 - accuracy: 0.5075 - val_loss: 0.7004 - val_accuracy: 0.5101
Epoch 24/500
101/101 - 13s - loss: 0.8286 - accuracy: 0.5127 - val_loss: 0.7001 - val_accuracy: 0.5151
Epoch 25/500
101/101 - 13s - loss: 0.8193 - accuracy: 0.5270 - val_loss: 0.7004 - val_accuracy: 0.5151
Epoch 26/500
101/101 - 13s - loss: 0.8281 - accuracy: 0.5208 - val_loss: 0.7004 - val_accuracy: 0.5126
Epoch 27/500
101/101 - 13s - loss: 0.8364 - accuracy: 0.5186 - val_loss: 0.6999 - val_accuracy: 0.5151
Epoch 28/500
101/101 - 13s - loss: 0.8294 - accuracy: 0.5193 - val_loss: 0.7003 - val_accuracy: 0.5050
Epoch 29/500
101/101 - 13s - loss: 0.7992 - accuracy: 0.5286 - val_loss: 0.6998 - val_accuracy: 0.5176
Epoch 30/500
101/101 - 13s - loss: 0.8007 - accuracy: 0.5311 - val_loss: 0.7002 - val_accuracy: 0.5201
Epoch 31/500
101/101 - 13s - loss: 0.8144 - accuracy: 0.5158 - val_loss: 0.7000 - val_accuracy: 0.5126
Epoch 32/500
101/101 - 14s - loss: 0.8171 - accuracy: 0.5149 - val_loss: 0.7012 - val_accuracy: 0.5101
Epoch 33/500
101/101 - 13s - loss: 0.8096 - accuracy: 0.5224 - val_loss: 0.7013 - val_accuracy: 0.5126
Epoch 34/500
101/101 - 13s - loss: 0.8322 - accuracy: 0.5158 - val_loss: 0.7004 - val_accuracy: 0.5126
Epoch 35/500
101/101 - 13s - loss: 0.8150 - accuracy: 0.5255 - val_loss: 0.6995 - val_accuracy: 0.5201
Epoch 36/500
101/101 - 13s - loss: 0.8148 - accuracy: 0.5286 - val_loss: 0.6991 - val_accuracy: 0.5201
Epoch 37/500
101/101 - 13s - loss: 0.8083 - accuracy: 0.5208 - val_loss: 0.6996 - val_accuracy: 0.5101
Epoch 38/500
101/101 - 13s - loss: 0.8187 - accuracy: 0.5165 - val_loss: 0.6992 - val_accuracy: 0.5151
Epoch 39/500
101/101 - 13s - loss: 0.8059 - accuracy: 0.5270 - val_loss: 0.7001 - val_accuracy: 0.5126
Epoch 40/500
101/101 - 13s - loss: 0.7964 - accuracy: 0.5304 - val_loss: 0.6996 - val_accuracy: 0.5226
Epoch 41/500
101/101 - 13s - loss: 0.8084 - accuracy: 0.5267 - val_loss: 0.7002 - val_accuracy: 0.5151
Epoch 42/500
101/101 - 13s - loss: 0.8095 - accuracy: 0.5245 - val_loss: 0.6998 - val_accuracy: 0.5126
Epoch 43/500
101/101 - 13s - loss: 0.7991 - accuracy: 0.5351 - val_loss: 0.7006 - val_accuracy: 0.5151
Epoch 44/500
101/101 - 13s - loss: 0.8035 - accuracy: 0.5307 - val_loss: 0.6992 - val_accuracy: 0.5151
Epoch 45/500
101/101 - 13s - loss: 0.7946 - accuracy: 0.5500 - val_loss: 0.6994 - val_accuracy: 0.5276
Epoch 46/500
101/101 - 13s - loss: 0.8155 - accuracy: 0.5276 - val_loss: 0.6992 - val_accuracy: 0.5327
Epoch 47/500
101/101 - 13s - loss: 0.7928 - accuracy: 0.5314 - val_loss: 0.6988 - val_accuracy: 0.5226
Epoch 48/500
101/101 - 13s - loss: 0.8048 - accuracy: 0.5280 - val_loss: 0.6991 - val_accuracy: 0.5201
Epoch 49/500
101/101 - 13s - loss: 0.7925 - accuracy: 0.5354 - val_loss: 0.6985 - val_accuracy: 0.5226
Epoch 50/500
101/101 - 13s - loss: 0.7974 - accuracy: 0.5332 - val_loss: 0.6989 - val_accuracy: 0.5176
Epoch 51/500
101/101 - 13s - loss: 0.7960 - accuracy: 0.5292 - val_loss: 0.6985 - val_accuracy: 0.5201
Epoch 52/500
101/101 - 13s - loss: 0.7950 - accuracy: 0.5292 - val_loss: 0.6980 - val_accuracy: 0.5151
Epoch 53/500
101/101 - 13s - loss: 0.7959 - accuracy: 0.5363 - val_loss: 0.6978 - val_accuracy: 0.5201
Epoch 54/500
101/101 - 13s - loss: 0.8051 - accuracy: 0.5292 - val_loss: 0.6979 - val_accuracy: 0.5151
Epoch 55/500
101/101 - 13s - loss: 0.7926 - accuracy: 0.5360 - val_loss: 0.6978 - val_accuracy: 0.5126
Epoch 56/500
101/101 - 13s - loss: 0.7829 - accuracy: 0.5472 - val_loss: 0.6979 - val_accuracy: 0.5201
Epoch 57/500
101/101 - 13s - loss: 0.7946 - accuracy: 0.5410 - val_loss: 0.6981 - val_accuracy: 0.5251
Epoch 58/500
101/101 - 13s - loss: 0.7795 - accuracy: 0.5547 - val_loss: 0.6981 - val_accuracy: 0.5176
Epoch 59/500
101/101 - 13s - loss: 0.8000 - accuracy: 0.5366 - val_loss: 0.6984 - val_accuracy: 0.5226
Epoch 60/500
101/101 - 13s - loss: 0.7794 - accuracy: 0.5453 - val_loss: 0.6982 - val_accuracy: 0.5126
Epoch 61/500
101/101 - 13s - loss: 0.7915 - accuracy: 0.5348 - val_loss: 0.6978 - val_accuracy: 0.5176
Epoch 62/500
101/101 - 13s - loss: 0.7895 - accuracy: 0.5317 - val_loss: 0.6982 - val_accuracy: 0.5201
Epoch 63/500
101/101 - 13s - loss: 0.7743 - accuracy: 0.5503 - val_loss: 0.6968 - val_accuracy: 0.5276
Epoch 64/500
101/101 - 13s - loss: 0.7737 - accuracy: 0.5528 - val_loss: 0.6978 - val_accuracy: 0.5201
Epoch 65/500
101/101 - 13s - loss: 0.8201 - accuracy: 0.5155 - val_loss: 0.6979 - val_accuracy: 0.5151
Epoch 66/500
101/101 - 13s - loss: 0.7593 - accuracy: 0.5612 - val_loss: 0.6978 - val_accuracy: 0.5176
========================================
save_weights
h5_weights/NPC.po/embedding_dense.h5
========================================

end time >>> Mon Oct  4 01:53:41 2021

end time >>> Mon Oct  4 01:53:41 2021

end time >>> Mon Oct  4 01:53:41 2021

end time >>> Mon Oct  4 01:53:41 2021

end time >>> Mon Oct  4 01:53:41 2021












args.model = embedding_dense
time used = 900.04727435112


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 01:53:43 2021

begin time >>> Mon Oct  4 01:53:43 2021

begin time >>> Mon Oct  4 01:53:43 2021

begin time >>> Mon Oct  4 01:53:43 2021

begin time >>> Mon Oct  4 01:53:43 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_cnn_one_branch
args.type = train
args.name = NPC.po
args.length = 10001
===========================


-> h5_weights/NPC.po folder already exist. pass.
-> result/NPC.po/onehot_cnn_one_branch folder already exist. pass.
-> result/NPC.po/onehot_cnn_two_branch folder already exist. pass.
-> result/NPC.po/onehot_embedding_dense folder already exist. pass.
-> result/NPC.po/onehot_dense folder already exist. pass.
-> result/NPC.po/onehot_resnet18 folder already exist. pass.
-> result/NPC.po/onehot_resnet34 folder already exist. pass.
-> result/NPC.po/embedding_cnn_one_branch folder already exist. pass.
-> result/NPC.po/embedding_cnn_two_branch folder already exist. pass.
-> result/NPC.po/embedding_dense folder already exist. pass.
-> result/NPC.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/NPC.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
NPC.po
########################################

########################################
model_name
embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
101/101 - 14s - loss: 0.8918 - accuracy: 0.4866 - val_loss: 0.6914 - val_accuracy: 0.5201
Epoch 2/500
101/101 - 14s - loss: 0.8361 - accuracy: 0.5124 - val_loss: 0.6897 - val_accuracy: 0.5050
Epoch 3/500
101/101 - 14s - loss: 0.8413 - accuracy: 0.5140 - val_loss: 0.6910 - val_accuracy: 0.5251
Epoch 4/500
101/101 - 14s - loss: 0.8559 - accuracy: 0.5093 - val_loss: 0.6920 - val_accuracy: 0.5201
Epoch 5/500
101/101 - 14s - loss: 0.8435 - accuracy: 0.5084 - val_loss: 0.6928 - val_accuracy: 0.5151
Epoch 6/500
101/101 - 14s - loss: 0.8459 - accuracy: 0.5134 - val_loss: 0.6927 - val_accuracy: 0.5126
Epoch 7/500
101/101 - 14s - loss: 0.8195 - accuracy: 0.5258 - val_loss: 0.6920 - val_accuracy: 0.5176
Epoch 8/500
101/101 - 14s - loss: 0.8287 - accuracy: 0.5168 - val_loss: 0.6916 - val_accuracy: 0.5176
Epoch 9/500
101/101 - 14s - loss: 0.8233 - accuracy: 0.5298 - val_loss: 0.6902 - val_accuracy: 0.5201
Epoch 10/500
101/101 - 14s - loss: 0.8130 - accuracy: 0.5301 - val_loss: 0.6890 - val_accuracy: 0.5201
Epoch 11/500
101/101 - 14s - loss: 0.8193 - accuracy: 0.5317 - val_loss: 0.6884 - val_accuracy: 0.5201
Epoch 12/500
101/101 - 14s - loss: 0.8018 - accuracy: 0.5357 - val_loss: 0.6872 - val_accuracy: 0.5251
Epoch 13/500
101/101 - 14s - loss: 0.8088 - accuracy: 0.5342 - val_loss: 0.6868 - val_accuracy: 0.5352
Epoch 14/500
101/101 - 14s - loss: 0.8016 - accuracy: 0.5301 - val_loss: 0.6857 - val_accuracy: 0.5327
Epoch 15/500
101/101 - 14s - loss: 0.7735 - accuracy: 0.5525 - val_loss: 0.6851 - val_accuracy: 0.5427
Epoch 16/500
101/101 - 14s - loss: 0.7801 - accuracy: 0.5385 - val_loss: 0.6844 - val_accuracy: 0.5427
Epoch 17/500
101/101 - 14s - loss: 0.7655 - accuracy: 0.5671 - val_loss: 0.6831 - val_accuracy: 0.5377
Epoch 18/500
101/101 - 14s - loss: 0.7762 - accuracy: 0.5506 - val_loss: 0.6823 - val_accuracy: 0.5402
Epoch 19/500
101/101 - 14s - loss: 0.7741 - accuracy: 0.5519 - val_loss: 0.6812 - val_accuracy: 0.5377
Epoch 20/500
101/101 - 14s - loss: 0.7687 - accuracy: 0.5500 - val_loss: 0.6804 - val_accuracy: 0.5352
Epoch 21/500
101/101 - 14s - loss: 0.7751 - accuracy: 0.5602 - val_loss: 0.6799 - val_accuracy: 0.5377
Epoch 22/500
101/101 - 14s - loss: 0.7849 - accuracy: 0.5509 - val_loss: 0.6794 - val_accuracy: 0.5352
Epoch 23/500
101/101 - 14s - loss: 0.7692 - accuracy: 0.5575 - val_loss: 0.6786 - val_accuracy: 0.5477
Epoch 24/500
101/101 - 14s - loss: 0.7520 - accuracy: 0.5668 - val_loss: 0.6775 - val_accuracy: 0.5402
Epoch 25/500
101/101 - 14s - loss: 0.7451 - accuracy: 0.5773 - val_loss: 0.6770 - val_accuracy: 0.5452
Epoch 26/500
101/101 - 14s - loss: 0.7365 - accuracy: 0.5866 - val_loss: 0.6760 - val_accuracy: 0.5427
Epoch 27/500
101/101 - 14s - loss: 0.7254 - accuracy: 0.5885 - val_loss: 0.6761 - val_accuracy: 0.5503
Epoch 28/500
101/101 - 14s - loss: 0.7356 - accuracy: 0.5910 - val_loss: 0.6754 - val_accuracy: 0.5402
Epoch 29/500
101/101 - 14s - loss: 0.7337 - accuracy: 0.5804 - val_loss: 0.6749 - val_accuracy: 0.5452
Epoch 30/500
101/101 - 14s - loss: 0.7253 - accuracy: 0.5857 - val_loss: 0.6745 - val_accuracy: 0.5503
Epoch 31/500
101/101 - 14s - loss: 0.7286 - accuracy: 0.5978 - val_loss: 0.6738 - val_accuracy: 0.5402
Epoch 32/500
101/101 - 14s - loss: 0.7192 - accuracy: 0.5913 - val_loss: 0.6738 - val_accuracy: 0.5452
Epoch 33/500
101/101 - 14s - loss: 0.7092 - accuracy: 0.6053 - val_loss: 0.6735 - val_accuracy: 0.5427
Epoch 34/500
101/101 - 14s - loss: 0.7095 - accuracy: 0.6109 - val_loss: 0.6725 - val_accuracy: 0.5402
Epoch 35/500
101/101 - 14s - loss: 0.7288 - accuracy: 0.5984 - val_loss: 0.6723 - val_accuracy: 0.5427
Epoch 36/500
101/101 - 14s - loss: 0.6858 - accuracy: 0.6102 - val_loss: 0.6717 - val_accuracy: 0.5452
Epoch 37/500
101/101 - 14s - loss: 0.6947 - accuracy: 0.6006 - val_loss: 0.6707 - val_accuracy: 0.5553
Epoch 38/500
101/101 - 14s - loss: 0.6889 - accuracy: 0.6043 - val_loss: 0.6702 - val_accuracy: 0.5553
Epoch 39/500
101/101 - 14s - loss: 0.7012 - accuracy: 0.6171 - val_loss: 0.6700 - val_accuracy: 0.5503
Epoch 40/500
101/101 - 14s - loss: 0.6958 - accuracy: 0.6118 - val_loss: 0.6695 - val_accuracy: 0.5553
Epoch 41/500
101/101 - 14s - loss: 0.6730 - accuracy: 0.6323 - val_loss: 0.6693 - val_accuracy: 0.5452
Epoch 42/500
101/101 - 14s - loss: 0.6764 - accuracy: 0.6248 - val_loss: 0.6689 - val_accuracy: 0.5452
Epoch 43/500
101/101 - 14s - loss: 0.6566 - accuracy: 0.6379 - val_loss: 0.6683 - val_accuracy: 0.5553
Epoch 44/500
101/101 - 14s - loss: 0.6700 - accuracy: 0.6255 - val_loss: 0.6675 - val_accuracy: 0.5578
Epoch 45/500
101/101 - 14s - loss: 0.6597 - accuracy: 0.6351 - val_loss: 0.6677 - val_accuracy: 0.5503
Epoch 46/500
101/101 - 14s - loss: 0.6556 - accuracy: 0.6314 - val_loss: 0.6675 - val_accuracy: 0.5528
Epoch 47/500
101/101 - 14s - loss: 0.6590 - accuracy: 0.6419 - val_loss: 0.6663 - val_accuracy: 0.5603
Epoch 48/500
101/101 - 14s - loss: 0.6333 - accuracy: 0.6599 - val_loss: 0.6660 - val_accuracy: 0.5603
Epoch 49/500
101/101 - 14s - loss: 0.6447 - accuracy: 0.6540 - val_loss: 0.6662 - val_accuracy: 0.5553
Epoch 50/500
101/101 - 14s - loss: 0.6304 - accuracy: 0.6602 - val_loss: 0.6655 - val_accuracy: 0.5603
Epoch 51/500
101/101 - 14s - loss: 0.6280 - accuracy: 0.6615 - val_loss: 0.6657 - val_accuracy: 0.5578
Epoch 52/500
101/101 - 14s - loss: 0.6539 - accuracy: 0.6382 - val_loss: 0.6645 - val_accuracy: 0.5678
Epoch 53/500
101/101 - 14s - loss: 0.6382 - accuracy: 0.6553 - val_loss: 0.6638 - val_accuracy: 0.5729
Epoch 54/500
101/101 - 14s - loss: 0.6417 - accuracy: 0.6540 - val_loss: 0.6636 - val_accuracy: 0.5704
Epoch 55/500
101/101 - 14s - loss: 0.6261 - accuracy: 0.6627 - val_loss: 0.6635 - val_accuracy: 0.5678
Epoch 56/500
101/101 - 14s - loss: 0.6192 - accuracy: 0.6674 - val_loss: 0.6636 - val_accuracy: 0.5628
Epoch 57/500
101/101 - 14s - loss: 0.6249 - accuracy: 0.6602 - val_loss: 0.6641 - val_accuracy: 0.5678
Epoch 58/500
101/101 - 14s - loss: 0.6165 - accuracy: 0.6733 - val_loss: 0.6638 - val_accuracy: 0.5754
Epoch 59/500
101/101 - 14s - loss: 0.5925 - accuracy: 0.6866 - val_loss: 0.6636 - val_accuracy: 0.5779
Epoch 60/500
101/101 - 14s - loss: 0.6001 - accuracy: 0.6839 - val_loss: 0.6633 - val_accuracy: 0.5754
Epoch 61/500
101/101 - 14s - loss: 0.6015 - accuracy: 0.6866 - val_loss: 0.6631 - val_accuracy: 0.5653
Epoch 62/500
101/101 - 14s - loss: 0.6095 - accuracy: 0.6733 - val_loss: 0.6636 - val_accuracy: 0.5729
Epoch 63/500
101/101 - 14s - loss: 0.5767 - accuracy: 0.6988 - val_loss: 0.6629 - val_accuracy: 0.5729
Epoch 64/500
101/101 - 14s - loss: 0.5763 - accuracy: 0.6981 - val_loss: 0.6630 - val_accuracy: 0.5678
Epoch 65/500
101/101 - 14s - loss: 0.5714 - accuracy: 0.7090 - val_loss: 0.6623 - val_accuracy: 0.5704
Epoch 66/500
101/101 - 14s - loss: 0.5533 - accuracy: 0.7152 - val_loss: 0.6623 - val_accuracy: 0.5754
Epoch 67/500
101/101 - 14s - loss: 0.5594 - accuracy: 0.7084 - val_loss: 0.6623 - val_accuracy: 0.5678
Epoch 68/500
101/101 - 14s - loss: 0.5666 - accuracy: 0.7155 - val_loss: 0.6615 - val_accuracy: 0.5729
Epoch 69/500
101/101 - 14s - loss: 0.5571 - accuracy: 0.7180 - val_loss: 0.6618 - val_accuracy: 0.5704
Epoch 70/500
101/101 - 14s - loss: 0.5444 - accuracy: 0.7208 - val_loss: 0.6628 - val_accuracy: 0.5704
Epoch 71/500
101/101 - 14s - loss: 0.5373 - accuracy: 0.7292 - val_loss: 0.6632 - val_accuracy: 0.5704
Epoch 72/500
101/101 - 14s - loss: 0.5352 - accuracy: 0.7298 - val_loss: 0.6628 - val_accuracy: 0.5628
Epoch 73/500
101/101 - 14s - loss: 0.5331 - accuracy: 0.7379 - val_loss: 0.6626 - val_accuracy: 0.5653
Epoch 74/500
101/101 - 14s - loss: 0.5473 - accuracy: 0.7180 - val_loss: 0.6627 - val_accuracy: 0.5628
Epoch 75/500
101/101 - 14s - loss: 0.5341 - accuracy: 0.7360 - val_loss: 0.6624 - val_accuracy: 0.5704
Epoch 76/500
101/101 - 14s - loss: 0.5301 - accuracy: 0.7270 - val_loss: 0.6625 - val_accuracy: 0.5628
Epoch 77/500
101/101 - 14s - loss: 0.5240 - accuracy: 0.7419 - val_loss: 0.6630 - val_accuracy: 0.5603
Epoch 78/500
101/101 - 14s - loss: 0.5225 - accuracy: 0.7314 - val_loss: 0.6635 - val_accuracy: 0.5653
Epoch 79/500
101/101 - 14s - loss: 0.5116 - accuracy: 0.7509 - val_loss: 0.6637 - val_accuracy: 0.5678
========================================
save_weights
h5_weights/NPC.po/embedding_cnn_one_branch.h5
========================================

end time >>> Mon Oct  4 02:12:34 2021

end time >>> Mon Oct  4 02:12:34 2021

end time >>> Mon Oct  4 02:12:34 2021

end time >>> Mon Oct  4 02:12:34 2021

end time >>> Mon Oct  4 02:12:34 2021












args.model = embedding_cnn_one_branch
time used = 1131.0119941234589


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 02:12:35 2021

begin time >>> Mon Oct  4 02:12:35 2021

begin time >>> Mon Oct  4 02:12:35 2021

begin time >>> Mon Oct  4 02:12:35 2021

begin time >>> Mon Oct  4 02:12:35 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_cnn_two_branch
args.type = train
args.name = NPC.po
args.length = 10001
===========================


-> h5_weights/NPC.po folder already exist. pass.
-> result/NPC.po/onehot_cnn_one_branch folder already exist. pass.
-> result/NPC.po/onehot_cnn_two_branch folder already exist. pass.
-> result/NPC.po/onehot_embedding_dense folder already exist. pass.
-> result/NPC.po/onehot_dense folder already exist. pass.
-> result/NPC.po/onehot_resnet18 folder already exist. pass.
-> result/NPC.po/onehot_resnet34 folder already exist. pass.
-> result/NPC.po/embedding_cnn_one_branch folder already exist. pass.
-> result/NPC.po/embedding_cnn_two_branch folder already exist. pass.
-> result/NPC.po/embedding_dense folder already exist. pass.
-> result/NPC.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/NPC.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
NPC.po
########################################

########################################
model_name
embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
101/101 - 14s - loss: 0.9014 - accuracy: 0.5037 - val_loss: 0.6901 - val_accuracy: 0.5452
Epoch 2/500
101/101 - 14s - loss: 0.9062 - accuracy: 0.5016 - val_loss: 0.6953 - val_accuracy: 0.4975
Epoch 3/500
101/101 - 14s - loss: 0.8867 - accuracy: 0.5186 - val_loss: 0.6994 - val_accuracy: 0.5101
Epoch 4/500
101/101 - 14s - loss: 0.8806 - accuracy: 0.5062 - val_loss: 0.7013 - val_accuracy: 0.5201
Epoch 5/500
101/101 - 14s - loss: 0.8595 - accuracy: 0.5205 - val_loss: 0.7025 - val_accuracy: 0.5302
Epoch 6/500
101/101 - 14s - loss: 0.8621 - accuracy: 0.5149 - val_loss: 0.7018 - val_accuracy: 0.5276
Epoch 7/500
101/101 - 14s - loss: 0.8511 - accuracy: 0.5242 - val_loss: 0.7004 - val_accuracy: 0.5251
Epoch 8/500
101/101 - 14s - loss: 0.8125 - accuracy: 0.5394 - val_loss: 0.6994 - val_accuracy: 0.5276
Epoch 9/500
101/101 - 14s - loss: 0.8433 - accuracy: 0.5214 - val_loss: 0.6984 - val_accuracy: 0.5226
Epoch 10/500
101/101 - 14s - loss: 0.8290 - accuracy: 0.5230 - val_loss: 0.6973 - val_accuracy: 0.5327
Epoch 11/500
101/101 - 14s - loss: 0.8445 - accuracy: 0.5158 - val_loss: 0.6963 - val_accuracy: 0.5101
Epoch 12/500
101/101 - 14s - loss: 0.8173 - accuracy: 0.5385 - val_loss: 0.6952 - val_accuracy: 0.5151
Epoch 13/500
101/101 - 14s - loss: 0.8023 - accuracy: 0.5488 - val_loss: 0.6935 - val_accuracy: 0.5176
Epoch 14/500
101/101 - 14s - loss: 0.8054 - accuracy: 0.5435 - val_loss: 0.6927 - val_accuracy: 0.5101
Epoch 15/500
101/101 - 14s - loss: 0.7761 - accuracy: 0.5543 - val_loss: 0.6913 - val_accuracy: 0.5151
Epoch 16/500
101/101 - 14s - loss: 0.7677 - accuracy: 0.5671 - val_loss: 0.6901 - val_accuracy: 0.5151
Epoch 17/500
101/101 - 14s - loss: 0.7885 - accuracy: 0.5522 - val_loss: 0.6896 - val_accuracy: 0.5201
Epoch 18/500
101/101 - 14s - loss: 0.7569 - accuracy: 0.5652 - val_loss: 0.6888 - val_accuracy: 0.5201
Epoch 19/500
101/101 - 14s - loss: 0.7920 - accuracy: 0.5435 - val_loss: 0.6886 - val_accuracy: 0.5176
Epoch 20/500
101/101 - 14s - loss: 0.7787 - accuracy: 0.5683 - val_loss: 0.6876 - val_accuracy: 0.5201
Epoch 21/500
101/101 - 14s - loss: 0.7635 - accuracy: 0.5736 - val_loss: 0.6865 - val_accuracy: 0.5151
========================================
save_weights
h5_weights/NPC.po/embedding_cnn_two_branch.h5
========================================

end time >>> Mon Oct  4 02:17:44 2021

end time >>> Mon Oct  4 02:17:44 2021

end time >>> Mon Oct  4 02:17:44 2021

end time >>> Mon Oct  4 02:17:44 2021

end time >>> Mon Oct  4 02:17:44 2021












args.model = embedding_cnn_two_branch
time used = 308.7123701572418


