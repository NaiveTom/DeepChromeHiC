************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sat Oct  2 22:51:22 2021

begin time >>> Sat Oct  2 22:51:22 2021

begin time >>> Sat Oct  2 22:51:22 2021

begin time >>> Sat Oct  2 22:51:22 2021

begin time >>> Sat Oct  2 22:51:22 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_cnn_one_branch
args.type = train
args.name = BL1.po
args.length = 10001
===========================


-> make new folder: h5_weights/BL1.po
-> make new folder: result/BL1.po/onehot_cnn_one_branch
-> make new folder: result/BL1.po/onehot_cnn_two_branch
-> make new folder: result/BL1.po/onehot_embedding_dense
-> make new folder: result/BL1.po/onehot_dense
-> make new folder: result/BL1.po/onehot_resnet18
-> make new folder: result/BL1.po/onehot_resnet34
-> make new folder: result/BL1.po/embedding_cnn_one_branch
-> make new folder: result/BL1.po/embedding_cnn_two_branch
-> make new folder: result/BL1.po/embedding_dense
-> make new folder: result/BL1.po/onehot_embedding_cnn_one_branch
-> make new folder: result/BL1.po/onehot_embedding_cnn_two_branch
########################################
gen_name
BL1.po
########################################

########################################
model_name
onehot_cnn_one_branch
########################################

Found 4942 images belonging to 2 classes.
Found 610 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 5001, 5, 64)       1600      
_________________________________________________________________
batch_normalization (BatchNo (None, 5001, 5, 64)       256       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1251, 5, 64)       98368     
_________________________________________________________________
batch_normalization_1 (Batch (None, 1251, 5, 64)       256       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 313, 5, 64)        98368     
_________________________________________________________________
batch_normalization_2 (Batch (None, 313, 5, 64)        256       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 156, 5, 64)        0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 156, 5, 64)        256       
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 39, 5, 128)        196736    
_________________________________________________________________
batch_normalization_4 (Batch (None, 39, 5, 128)        512       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 10, 5, 128)        393344    
_________________________________________________________________
batch_normalization_5 (Batch (None, 10, 5, 128)        512       
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 3, 5, 128)         393344    
_________________________________________________________________
batch_normalization_6 (Batch (None, 3, 5, 128)         512       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 1, 5, 128)         0         
_________________________________________________________________
flatten (Flatten)            (None, 640)               0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 640)               2560      
_________________________________________________________________
dense (Dense)                (None, 512)               328192    
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
dense_3 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 2,041,410
Trainable params: 2,038,850
Non-trainable params: 2,560
_________________________________________________________________
Epoch 1/500
154/154 - 131s - loss: 0.7951 - accuracy: 0.5020 - val_loss: 0.6922 - val_accuracy: 0.5049
Epoch 2/500
154/154 - 19s - loss: 0.7099 - accuracy: 0.5648 - val_loss: 0.6962 - val_accuracy: 0.5345
Epoch 3/500
154/154 - 19s - loss: 0.6738 - accuracy: 0.6037 - val_loss: 0.7017 - val_accuracy: 0.5477
Epoch 4/500
154/154 - 19s - loss: 0.6364 - accuracy: 0.6424 - val_loss: 0.7811 - val_accuracy: 0.5230
Epoch 5/500
154/154 - 19s - loss: 0.5697 - accuracy: 0.6969 - val_loss: 0.8670 - val_accuracy: 0.5164
Epoch 6/500
154/154 - 19s - loss: 0.4701 - accuracy: 0.7792 - val_loss: 2.4497 - val_accuracy: 0.5016
Epoch 7/500
154/154 - 19s - loss: 0.3385 - accuracy: 0.8629 - val_loss: 1.6085 - val_accuracy: 0.5214
Epoch 8/500
154/154 - 19s - loss: 0.2088 - accuracy: 0.9234 - val_loss: 1.2389 - val_accuracy: 0.5724
Epoch 9/500
154/154 - 19s - loss: 0.1180 - accuracy: 0.9611 - val_loss: 2.3416 - val_accuracy: 0.5164
Epoch 10/500
154/154 - 19s - loss: 0.0658 - accuracy: 0.9792 - val_loss: 1.6156 - val_accuracy: 0.5987
Epoch 11/500
154/154 - 19s - loss: 0.0449 - accuracy: 0.9870 - val_loss: 1.9191 - val_accuracy: 0.5707
Epoch 12/500
154/154 - 19s - loss: 0.0280 - accuracy: 0.9914 - val_loss: 1.7284 - val_accuracy: 0.5954
Epoch 13/500
154/154 - 19s - loss: 0.0260 - accuracy: 0.9931 - val_loss: 2.7103 - val_accuracy: 0.5658
Epoch 14/500
154/154 - 19s - loss: 0.0185 - accuracy: 0.9947 - val_loss: 2.3944 - val_accuracy: 0.5938
Epoch 15/500
154/154 - 19s - loss: 0.0169 - accuracy: 0.9941 - val_loss: 1.8517 - val_accuracy: 0.5724
Epoch 16/500
154/154 - 19s - loss: 0.0138 - accuracy: 0.9953 - val_loss: 3.2249 - val_accuracy: 0.5345
Epoch 17/500
154/154 - 19s - loss: 0.0145 - accuracy: 0.9959 - val_loss: 3.5967 - val_accuracy: 0.5312
Epoch 18/500
154/154 - 19s - loss: 0.0129 - accuracy: 0.9953 - val_loss: 2.4201 - val_accuracy: 0.5888
Epoch 19/500
154/154 - 19s - loss: 0.0167 - accuracy: 0.9933 - val_loss: 9.7154 - val_accuracy: 0.5049
Epoch 20/500
154/154 - 19s - loss: 0.0328 - accuracy: 0.9890 - val_loss: 2.7254 - val_accuracy: 0.5707
========================================
save_weights
h5_weights/BL1.po/onehot_cnn_one_branch.h5
========================================

end time >>> Sat Oct  2 22:59:48 2021

end time >>> Sat Oct  2 22:59:48 2021

end time >>> Sat Oct  2 22:59:48 2021

end time >>> Sat Oct  2 22:59:48 2021

end time >>> Sat Oct  2 22:59:48 2021












args.model = onehot_cnn_one_branch
time used = 506.20557737350464


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sat Oct  2 22:59:49 2021

begin time >>> Sat Oct  2 22:59:49 2021

begin time >>> Sat Oct  2 22:59:49 2021

begin time >>> Sat Oct  2 22:59:49 2021

begin time >>> Sat Oct  2 22:59:49 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_cnn_two_branch
args.type = train
args.name = BL1.po
args.length = 10001
===========================


-> h5_weights/BL1.po folder already exist. pass.
-> result/BL1.po/onehot_cnn_one_branch folder already exist. pass.
-> result/BL1.po/onehot_cnn_two_branch folder already exist. pass.
-> result/BL1.po/onehot_embedding_dense folder already exist. pass.
-> result/BL1.po/onehot_dense folder already exist. pass.
-> result/BL1.po/onehot_resnet18 folder already exist. pass.
-> result/BL1.po/onehot_resnet34 folder already exist. pass.
-> result/BL1.po/embedding_cnn_one_branch folder already exist. pass.
-> result/BL1.po/embedding_cnn_two_branch folder already exist. pass.
-> result/BL1.po/embedding_dense folder already exist. pass.
-> result/BL1.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/BL1.po/onehot_embedding_cnn_two_branch folder already exist. pass.
Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001, 5, 1) 0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001, 5, 1) 0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 2501, 5, 64)  1600        input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 2501, 5, 64)  1600        input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 2501, 5, 64)  256         conv2d[0][0]                     
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 2501, 5, 64)  256         conv2d_6[0][0]                   
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 626, 5, 64)   98368       batch_normalization[0][0]        
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 626, 5, 64)   98368       batch_normalization_8[0][0]      
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 626, 5, 64)   256         conv2d_1[0][0]                   
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 626, 5, 64)   256         conv2d_7[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 157, 5, 64)   98368       batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 157, 5, 64)   98368       batch_normalization_9[0][0]      
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 157, 5, 64)   256         conv2d_2[0][0]                   
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 157, 5, 64)   256         conv2d_8[0][0]                   
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 78, 5, 64)    0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 78, 5, 64)    0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 78, 5, 64)    256         max_pooling2d[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 78, 5, 64)    256         max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 20, 5, 128)   196736      batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 20, 5, 128)   196736      batch_normalization_11[0][0]     
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 20, 5, 128)   512         conv2d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 20, 5, 128)   512         conv2d_9[0][0]                   
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 5, 5, 128)    393344      batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 5, 5, 128)    393344      batch_normalization_12[0][0]     
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 5, 5, 128)    512         conv2d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 5, 5, 128)    512         conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 2, 5, 128)    393344      batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 2, 5, 128)    393344      batch_normalization_13[0][0]     
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 2, 5, 128)    512         conv2d_5[0][0]                   
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 2, 5, 128)    512         conv2d_11[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 1, 5, 128)    0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 1, 5, 128)    0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
flatten (Flatten)               (None, 640)          0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 640)          0           max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 640)          2560        flatten[0][0]                    
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 640)          2560        flatten_1[0][0]                  
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          328192      batch_normalization_7[0][0]      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          328192      batch_normalization_15[0][0]     
__________________________________________________________________________________________________
dropout (Dropout)               (None, 512)          0           dense[0][0]                      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 1024)         0           dropout[0][0]                    
                                                                 dropout_1[0][0]                  
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          524800      concatenate[0][0]                
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 512)          262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 2)            1026        dense_3[0][0]                    
==================================================================================================
Total params: 3,818,626
Trainable params: 3,813,506
Non-trainable params: 5,120
__________________________________________________________________________________________________
Found 4942 images belonging to 2 classes.
Found 4942 images belonging to 2 classes.
Epoch 1/500
Found 610 images belonging to 2 classes.
Found 610 images belonging to 2 classes.
1535/1535 - 421s - loss: 0.4250 - accuracy: 0.7743 - val_loss: 1.5247 - val_accuracy: 0.5236
Epoch 2/500
1535/1535 - 223s - loss: 0.0295 - accuracy: 0.9903 - val_loss: 5.3024 - val_accuracy: 0.5165
Epoch 3/500
1535/1535 - 223s - loss: 0.0233 - accuracy: 0.9927 - val_loss: 2.7124 - val_accuracy: 0.5805
Epoch 4/500
1535/1535 - 222s - loss: 0.0220 - accuracy: 0.9926 - val_loss: 3.9586 - val_accuracy: 0.5659
Epoch 5/500
1535/1535 - 221s - loss: 0.0168 - accuracy: 0.9951 - val_loss: 3.9935 - val_accuracy: 0.5772
Epoch 6/500
1535/1535 - 221s - loss: 0.0129 - accuracy: 0.9955 - val_loss: 3.4083 - val_accuracy: 0.5618
Epoch 7/500
1535/1535 - 221s - loss: 0.0134 - accuracy: 0.9960 - val_loss: 2.9877 - val_accuracy: 0.5936
Epoch 8/500
1535/1535 - 221s - loss: 0.0077 - accuracy: 0.9974 - val_loss: 3.9356 - val_accuracy: 0.5852
Epoch 9/500
1535/1535 - 221s - loss: 0.0124 - accuracy: 0.9962 - val_loss: 2.7961 - val_accuracy: 0.6060
Epoch 10/500
1535/1535 - 220s - loss: 0.0071 - accuracy: 0.9979 - val_loss: 3.6591 - val_accuracy: 0.6145
Epoch 11/500
1535/1535 - 222s - loss: 0.0097 - accuracy: 0.9970 - val_loss: 2.8202 - val_accuracy: 0.5782
Epoch 12/500
1535/1535 - 222s - loss: 0.0063 - accuracy: 0.9982 - val_loss: 3.5162 - val_accuracy: 0.5747
Epoch 13/500
1535/1535 - 222s - loss: 0.0091 - accuracy: 0.9971 - val_loss: 6.1861 - val_accuracy: 0.4955
Epoch 14/500
1535/1535 - 223s - loss: 0.0067 - accuracy: 0.9976 - val_loss: 3.0722 - val_accuracy: 0.5996
Epoch 15/500
1535/1535 - 221s - loss: 0.0041 - accuracy: 0.9987 - val_loss: 3.8451 - val_accuracy: 0.5943
Epoch 16/500
1535/1535 - 222s - loss: 0.0080 - accuracy: 0.9976 - val_loss: 2.9767 - val_accuracy: 0.5947
Epoch 17/500
1535/1535 - 220s - loss: 0.0034 - accuracy: 0.9989 - val_loss: 3.1211 - val_accuracy: 0.5955
Epoch 18/500
1535/1535 - 222s - loss: 0.0064 - accuracy: 0.9979 - val_loss: 4.2891 - val_accuracy: 0.5527
Epoch 19/500
1535/1535 - 220s - loss: 0.0050 - accuracy: 0.9985 - val_loss: 3.3605 - val_accuracy: 0.5953
Epoch 20/500
1535/1535 - 221s - loss: 0.0057 - accuracy: 0.9984 - val_loss: 2.7592 - val_accuracy: 0.5996
========================================
save_weights
h5_weights/BL1.po/onehot_cnn_two_branch.h5
========================================

end time >>> Sun Oct  3 00:17:13 2021

end time >>> Sun Oct  3 00:17:13 2021

end time >>> Sun Oct  3 00:17:13 2021

end time >>> Sun Oct  3 00:17:13 2021

end time >>> Sun Oct  3 00:17:13 2021












args.model = onehot_cnn_two_branch
time used = 4643.444332122803


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 00:17:14 2021

begin time >>> Sun Oct  3 00:17:14 2021

begin time >>> Sun Oct  3 00:17:14 2021

begin time >>> Sun Oct  3 00:17:14 2021

begin time >>> Sun Oct  3 00:17:14 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_dense
args.type = train
args.name = BL1.po
args.length = 10001
===========================


-> h5_weights/BL1.po folder already exist. pass.
-> result/BL1.po/onehot_cnn_one_branch folder already exist. pass.
-> result/BL1.po/onehot_cnn_two_branch folder already exist. pass.
-> result/BL1.po/onehot_embedding_dense folder already exist. pass.
-> result/BL1.po/onehot_dense folder already exist. pass.
-> result/BL1.po/onehot_resnet18 folder already exist. pass.
-> result/BL1.po/onehot_resnet34 folder already exist. pass.
-> result/BL1.po/embedding_cnn_one_branch folder already exist. pass.
-> result/BL1.po/embedding_cnn_two_branch folder already exist. pass.
-> result/BL1.po/embedding_dense folder already exist. pass.
-> result/BL1.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/BL1.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
BL1.po
########################################

########################################
model_name
onehot_dense
########################################

Found 4942 images belonging to 2 classes.
Found 610 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 100010)            0         
_________________________________________________________________
batch_normalization (BatchNo (None, 100010)            400040    
_________________________________________________________________
dense (Dense)                (None, 512)               51205632  
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_3 (Batch (None, 512)               2048      
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_3 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 52,402,858
Trainable params: 52,198,742
Non-trainable params: 204,116
_________________________________________________________________
Epoch 1/500
154/154 - 19s - loss: 0.8205 - accuracy: 0.5051 - val_loss: 0.6786 - val_accuracy: 0.5329
Epoch 2/500
154/154 - 14s - loss: 0.7066 - accuracy: 0.5772 - val_loss: 0.6682 - val_accuracy: 0.5806
Epoch 3/500
154/154 - 14s - loss: 0.6397 - accuracy: 0.6436 - val_loss: 0.6765 - val_accuracy: 0.5938
Epoch 4/500
154/154 - 14s - loss: 0.5503 - accuracy: 0.7220 - val_loss: 0.7182 - val_accuracy: 0.5740
Epoch 5/500
154/154 - 14s - loss: 0.4366 - accuracy: 0.8043 - val_loss: 0.8058 - val_accuracy: 0.5674
Epoch 6/500
154/154 - 14s - loss: 0.3185 - accuracy: 0.8699 - val_loss: 1.0026 - val_accuracy: 0.5674
Epoch 7/500
154/154 - 14s - loss: 0.2222 - accuracy: 0.9096 - val_loss: 1.1599 - val_accuracy: 0.5592
Epoch 8/500
154/154 - 14s - loss: 0.1552 - accuracy: 0.9450 - val_loss: 1.3331 - val_accuracy: 0.5543
Epoch 9/500
154/154 - 14s - loss: 0.1190 - accuracy: 0.9578 - val_loss: 1.4693 - val_accuracy: 0.5609
Epoch 10/500
154/154 - 14s - loss: 0.0919 - accuracy: 0.9682 - val_loss: 1.5711 - val_accuracy: 0.5576
Epoch 11/500
154/154 - 14s - loss: 0.0825 - accuracy: 0.9717 - val_loss: 1.6820 - val_accuracy: 0.5592
Epoch 12/500
154/154 - 14s - loss: 0.0607 - accuracy: 0.9796 - val_loss: 1.7652 - val_accuracy: 0.5674
Epoch 13/500
154/154 - 14s - loss: 0.0579 - accuracy: 0.9819 - val_loss: 1.7740 - val_accuracy: 0.5658
========================================
save_weights
h5_weights/BL1.po/onehot_dense.h5
========================================

end time >>> Sun Oct  3 00:20:32 2021

end time >>> Sun Oct  3 00:20:32 2021

end time >>> Sun Oct  3 00:20:32 2021

end time >>> Sun Oct  3 00:20:32 2021

end time >>> Sun Oct  3 00:20:32 2021












args.model = onehot_dense
time used = 198.22048783302307


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 00:20:33 2021

begin time >>> Sun Oct  3 00:20:33 2021

begin time >>> Sun Oct  3 00:20:33 2021

begin time >>> Sun Oct  3 00:20:33 2021

begin time >>> Sun Oct  3 00:20:33 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_resnet18
args.type = train
args.name = BL1.po
args.length = 10001
===========================


-> h5_weights/BL1.po folder already exist. pass.
-> result/BL1.po/onehot_cnn_one_branch folder already exist. pass.
-> result/BL1.po/onehot_cnn_two_branch folder already exist. pass.
-> result/BL1.po/onehot_embedding_dense folder already exist. pass.
-> result/BL1.po/onehot_dense folder already exist. pass.
-> result/BL1.po/onehot_resnet18 folder already exist. pass.
-> result/BL1.po/onehot_resnet34 folder already exist. pass.
-> result/BL1.po/embedding_cnn_one_branch folder already exist. pass.
-> result/BL1.po/embedding_cnn_two_branch folder already exist. pass.
-> result/BL1.po/embedding_dense folder already exist. pass.
-> result/BL1.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/BL1.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
BL1.po
########################################

########################################
model_name
onehot_resnet18
########################################

Found 4942 images belonging to 2 classes.
Found 610 images belonging to 2 classes.
Model: "res_net_type_i"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              multiple                  1600      
_________________________________________________________________
batch_normalization (BatchNo multiple                  256       
_________________________________________________________________
max_pooling2d (MaxPooling2D) multiple                  0         
_________________________________________________________________
sequential (Sequential)      (None, 313, 1, 64)        398912    
_________________________________________________________________
sequential_2 (Sequential)    (None, 20, 1, 64)         398912    
_________________________________________________________________
sequential_4 (Sequential)    (None, 2, 1, 64)          398912    
_________________________________________________________________
sequential_6 (Sequential)    (None, 1, 1, 64)          398912    
_________________________________________________________________
global_average_pooling2d (Gl multiple                  0         
_________________________________________________________________
dense (Dense)                multiple                  130       
=================================================================
Total params: 1,597,634
Trainable params: 1,594,946
Non-trainable params: 2,688
_________________________________________________________________
Epoch 1/500
154/154 - 19s - loss: 0.7859 - accuracy: 0.5029 - val_loss: 0.6964 - val_accuracy: 0.5000
Epoch 2/500
154/154 - 18s - loss: 0.6031 - accuracy: 0.6703 - val_loss: 0.7124 - val_accuracy: 0.5049
Epoch 3/500
154/154 - 18s - loss: 0.5107 - accuracy: 0.7580 - val_loss: 0.7596 - val_accuracy: 0.4852
Epoch 4/500
154/154 - 18s - loss: 0.4248 - accuracy: 0.8246 - val_loss: 0.8260 - val_accuracy: 0.4967
Epoch 5/500
154/154 - 18s - loss: 0.3374 - accuracy: 0.8725 - val_loss: 0.8717 - val_accuracy: 0.5148
Epoch 6/500
154/154 - 19s - loss: 0.2487 - accuracy: 0.9136 - val_loss: 0.9537 - val_accuracy: 0.5033
Epoch 7/500
154/154 - 18s - loss: 0.1799 - accuracy: 0.9446 - val_loss: 1.0537 - val_accuracy: 0.5312
Epoch 8/500
154/154 - 18s - loss: 0.1354 - accuracy: 0.9640 - val_loss: 1.1101 - val_accuracy: 0.5247
Epoch 9/500
154/154 - 18s - loss: 0.1094 - accuracy: 0.9692 - val_loss: 1.2069 - val_accuracy: 0.5115
Epoch 10/500
154/154 - 18s - loss: 0.1020 - accuracy: 0.9705 - val_loss: 1.2767 - val_accuracy: 0.5362
Epoch 11/500
154/154 - 18s - loss: 0.0952 - accuracy: 0.9680 - val_loss: 1.2618 - val_accuracy: 0.5214
Epoch 12/500
154/154 - 18s - loss: 0.0881 - accuracy: 0.9721 - val_loss: 1.3710 - val_accuracy: 0.5345
Epoch 13/500
154/154 - 18s - loss: 0.1078 - accuracy: 0.9603 - val_loss: 1.4349 - val_accuracy: 0.5049
Epoch 14/500
154/154 - 18s - loss: 0.1262 - accuracy: 0.9536 - val_loss: 1.3742 - val_accuracy: 0.5345
Epoch 15/500
154/154 - 18s - loss: 0.1278 - accuracy: 0.9519 - val_loss: 1.4011 - val_accuracy: 0.5280
Epoch 16/500
154/154 - 20s - loss: 0.1381 - accuracy: 0.9489 - val_loss: 1.4879 - val_accuracy: 0.5214
Epoch 17/500
154/154 - 19s - loss: 0.1208 - accuracy: 0.9574 - val_loss: 1.5071 - val_accuracy: 0.5312
Epoch 18/500
154/154 - 18s - loss: 0.0990 - accuracy: 0.9644 - val_loss: 1.4330 - val_accuracy: 0.5543
Epoch 19/500
154/154 - 18s - loss: 0.0695 - accuracy: 0.9776 - val_loss: 1.5066 - val_accuracy: 0.5378
Epoch 20/500
154/154 - 18s - loss: 0.0671 - accuracy: 0.9774 - val_loss: 1.4938 - val_accuracy: 0.5345
Epoch 21/500
154/154 - 18s - loss: 0.0463 - accuracy: 0.9845 - val_loss: 1.5985 - val_accuracy: 0.5197
Epoch 22/500
154/154 - 18s - loss: 0.0390 - accuracy: 0.9886 - val_loss: 1.5647 - val_accuracy: 0.5362
Epoch 23/500
154/154 - 18s - loss: 0.0339 - accuracy: 0.9910 - val_loss: 1.5562 - val_accuracy: 0.5477
Epoch 24/500
154/154 - 18s - loss: 0.0365 - accuracy: 0.9896 - val_loss: 1.5556 - val_accuracy: 0.5411
Epoch 25/500
154/154 - 18s - loss: 0.0446 - accuracy: 0.9843 - val_loss: 1.5005 - val_accuracy: 0.5576
Epoch 26/500
154/154 - 18s - loss: 0.0545 - accuracy: 0.9792 - val_loss: 1.5850 - val_accuracy: 0.5230
Epoch 27/500
154/154 - 19s - loss: 0.0575 - accuracy: 0.9796 - val_loss: 1.6435 - val_accuracy: 0.5477
Epoch 28/500
154/154 - 19s - loss: 0.0570 - accuracy: 0.9813 - val_loss: 1.6617 - val_accuracy: 0.5543
Epoch 29/500
154/154 - 18s - loss: 0.0521 - accuracy: 0.9827 - val_loss: 1.6126 - val_accuracy: 0.5444
Epoch 30/500
154/154 - 18s - loss: 0.0559 - accuracy: 0.9778 - val_loss: 1.6379 - val_accuracy: 0.5378
Epoch 31/500
154/154 - 19s - loss: 0.0479 - accuracy: 0.9845 - val_loss: 1.6239 - val_accuracy: 0.5115
Epoch 32/500
154/154 - 18s - loss: 0.0445 - accuracy: 0.9843 - val_loss: 1.5879 - val_accuracy: 0.5411
Epoch 33/500
154/154 - 18s - loss: 0.0565 - accuracy: 0.9807 - val_loss: 1.5905 - val_accuracy: 0.5493
Epoch 34/500
154/154 - 18s - loss: 0.0576 - accuracy: 0.9800 - val_loss: 1.6695 - val_accuracy: 0.5362
Epoch 35/500
154/154 - 18s - loss: 0.0591 - accuracy: 0.9796 - val_loss: 1.6245 - val_accuracy: 0.5592
Epoch 36/500
154/154 - 18s - loss: 0.0565 - accuracy: 0.9817 - val_loss: 1.5229 - val_accuracy: 0.5493
Epoch 37/500
154/154 - 18s - loss: 0.0566 - accuracy: 0.9807 - val_loss: 1.5673 - val_accuracy: 0.5691
Epoch 38/500
154/154 - 18s - loss: 0.0340 - accuracy: 0.9890 - val_loss: 1.5195 - val_accuracy: 0.5609
Epoch 39/500
154/154 - 19s - loss: 0.0238 - accuracy: 0.9931 - val_loss: 1.4826 - val_accuracy: 0.5707
Epoch 40/500
154/154 - 18s - loss: 0.0352 - accuracy: 0.9859 - val_loss: 1.5387 - val_accuracy: 0.5658
Epoch 41/500
154/154 - 18s - loss: 0.0358 - accuracy: 0.9870 - val_loss: 1.5429 - val_accuracy: 0.5559
Epoch 42/500
154/154 - 18s - loss: 0.0330 - accuracy: 0.9884 - val_loss: 1.6289 - val_accuracy: 0.5773
Epoch 43/500
154/154 - 18s - loss: 0.0321 - accuracy: 0.9886 - val_loss: 1.4817 - val_accuracy: 0.5970
Epoch 44/500
154/154 - 18s - loss: 0.0331 - accuracy: 0.9892 - val_loss: 1.6049 - val_accuracy: 0.5641
Epoch 45/500
154/154 - 18s - loss: 0.0393 - accuracy: 0.9874 - val_loss: 1.6102 - val_accuracy: 0.5674
Epoch 46/500
154/154 - 18s - loss: 0.0315 - accuracy: 0.9898 - val_loss: 1.6675 - val_accuracy: 0.5625
Epoch 47/500
154/154 - 18s - loss: 0.0290 - accuracy: 0.9916 - val_loss: 1.5728 - val_accuracy: 0.5707
Epoch 48/500
154/154 - 19s - loss: 0.0238 - accuracy: 0.9923 - val_loss: 1.6640 - val_accuracy: 0.5559
Epoch 49/500
154/154 - 18s - loss: 0.0367 - accuracy: 0.9878 - val_loss: 1.7671 - val_accuracy: 0.5674
Epoch 50/500
154/154 - 18s - loss: 0.0375 - accuracy: 0.9880 - val_loss: 1.6957 - val_accuracy: 0.5707
Epoch 51/500
154/154 - 18s - loss: 0.0365 - accuracy: 0.9882 - val_loss: 1.7687 - val_accuracy: 0.5411
Epoch 52/500
154/154 - 18s - loss: 0.0334 - accuracy: 0.9896 - val_loss: 1.7739 - val_accuracy: 0.5428
Epoch 53/500
154/154 - 18s - loss: 0.0232 - accuracy: 0.9929 - val_loss: 1.7160 - val_accuracy: 0.5296
========================================
save_weights
h5_weights/BL1.po/onehot_resnet18.h5
========================================

end time >>> Sun Oct  3 00:37:10 2021

end time >>> Sun Oct  3 00:37:10 2021

end time >>> Sun Oct  3 00:37:10 2021

end time >>> Sun Oct  3 00:37:10 2021

end time >>> Sun Oct  3 00:37:10 2021












args.model = onehot_resnet18
time used = 996.7052755355835


