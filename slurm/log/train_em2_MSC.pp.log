************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 00:31:14 2021

begin time >>> Mon Oct  4 00:31:14 2021

begin time >>> Mon Oct  4 00:31:14 2021

begin time >>> Mon Oct  4 00:31:14 2021

begin time >>> Mon Oct  4 00:31:14 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_dense
args.type = train
args.name = MSC.pp
args.length = 10001
===========================


-> h5_weights/MSC.pp folder already exist. pass.
-> result/MSC.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/MSC.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/MSC.pp/onehot_embedding_dense folder already exist. pass.
-> result/MSC.pp/onehot_dense folder already exist. pass.
-> result/MSC.pp/onehot_resnet18 folder already exist. pass.
-> result/MSC.pp/onehot_resnet34 folder already exist. pass.
-> result/MSC.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/MSC.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/MSC.pp/embedding_dense folder already exist. pass.
-> result/MSC.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/MSC.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
MSC.pp
########################################

########################################
model_name
onehot_embedding_dense
########################################

Found 3950 images belonging to 2 classes.
Found 488 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 20002, 5, 1, 8)    32776     
_________________________________________________________________
flatten (Flatten)            (None, 800080)            0         
_________________________________________________________________
batch_normalization (BatchNo (None, 800080)            3200320   
_________________________________________________________________
dense (Dense)                (None, 512)               409641472 
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_3 (Batch (None, 512)               2048      
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_3 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 413,671,754
Trainable params: 412,067,498
Non-trainable params: 1,604,256
_________________________________________________________________
Epoch 1/500
123/123 - 38s - loss: 0.7845 - accuracy: 0.5332 - val_loss: 0.6909 - val_accuracy: 0.5188
Epoch 2/500
123/123 - 23s - loss: 0.6700 - accuracy: 0.6274 - val_loss: 0.7381 - val_accuracy: 0.5000
Epoch 3/500
123/123 - 23s - loss: 0.5978 - accuracy: 0.6930 - val_loss: 0.8202 - val_accuracy: 0.5000
Epoch 4/500
123/123 - 23s - loss: 0.4986 - accuracy: 0.7637 - val_loss: 0.9681 - val_accuracy: 0.4938
Epoch 5/500
123/123 - 24s - loss: 0.4184 - accuracy: 0.8091 - val_loss: 1.0477 - val_accuracy: 0.5292
Epoch 6/500
123/123 - 24s - loss: 0.3340 - accuracy: 0.8583 - val_loss: 1.1221 - val_accuracy: 0.5688
Epoch 7/500
123/123 - 24s - loss: 0.2779 - accuracy: 0.8862 - val_loss: 1.2381 - val_accuracy: 0.5792
Epoch 8/500
123/123 - 23s - loss: 0.2163 - accuracy: 0.9211 - val_loss: 1.3631 - val_accuracy: 0.5792
Epoch 9/500
123/123 - 24s - loss: 0.1885 - accuracy: 0.9255 - val_loss: 1.4330 - val_accuracy: 0.5875
Epoch 10/500
123/123 - 24s - loss: 0.1580 - accuracy: 0.9441 - val_loss: 1.4885 - val_accuracy: 0.6042
Epoch 11/500
123/123 - 23s - loss: 0.1337 - accuracy: 0.9497 - val_loss: 1.5610 - val_accuracy: 0.5979
Epoch 12/500
123/123 - 23s - loss: 0.1142 - accuracy: 0.9576 - val_loss: 1.6016 - val_accuracy: 0.5917
Epoch 13/500
123/123 - 23s - loss: 0.1028 - accuracy: 0.9640 - val_loss: 1.6673 - val_accuracy: 0.6021
Epoch 14/500
123/123 - 24s - loss: 0.0901 - accuracy: 0.9681 - val_loss: 1.6336 - val_accuracy: 0.6208
Epoch 15/500
123/123 - 23s - loss: 0.0852 - accuracy: 0.9699 - val_loss: 1.7237 - val_accuracy: 0.6062
Epoch 16/500
123/123 - 23s - loss: 0.0846 - accuracy: 0.9699 - val_loss: 1.8232 - val_accuracy: 0.6146
Epoch 17/500
123/123 - 24s - loss: 0.0655 - accuracy: 0.9758 - val_loss: 1.8497 - val_accuracy: 0.6229
Epoch 18/500
123/123 - 23s - loss: 0.0640 - accuracy: 0.9768 - val_loss: 1.8523 - val_accuracy: 0.6229
Epoch 19/500
123/123 - 23s - loss: 0.0546 - accuracy: 0.9816 - val_loss: 1.9206 - val_accuracy: 0.6125
Epoch 20/500
123/123 - 23s - loss: 0.0509 - accuracy: 0.9824 - val_loss: 1.9609 - val_accuracy: 0.6083
Epoch 21/500
123/123 - 25s - loss: 0.0495 - accuracy: 0.9826 - val_loss: 1.9420 - val_accuracy: 0.6250
Epoch 22/500
123/123 - 23s - loss: 0.0446 - accuracy: 0.9849 - val_loss: 2.0074 - val_accuracy: 0.6250
Epoch 23/500
123/123 - 24s - loss: 0.0365 - accuracy: 0.9893 - val_loss: 1.9884 - val_accuracy: 0.6292
Epoch 24/500
123/123 - 24s - loss: 0.0391 - accuracy: 0.9865 - val_loss: 2.0214 - val_accuracy: 0.6354
Epoch 25/500
123/123 - 23s - loss: 0.0517 - accuracy: 0.9811 - val_loss: 2.1233 - val_accuracy: 0.6229
Epoch 26/500
123/123 - 23s - loss: 0.0450 - accuracy: 0.9837 - val_loss: 2.1184 - val_accuracy: 0.6333
Epoch 27/500
123/123 - 23s - loss: 0.0397 - accuracy: 0.9862 - val_loss: 2.1416 - val_accuracy: 0.6271
Epoch 28/500
123/123 - 23s - loss: 0.0372 - accuracy: 0.9913 - val_loss: 2.0712 - val_accuracy: 0.6271
Epoch 29/500
123/123 - 23s - loss: 0.0433 - accuracy: 0.9849 - val_loss: 2.1785 - val_accuracy: 0.6229
Epoch 30/500
123/123 - 23s - loss: 0.0407 - accuracy: 0.9877 - val_loss: 2.1979 - val_accuracy: 0.6187
Epoch 31/500
123/123 - 23s - loss: 0.0382 - accuracy: 0.9875 - val_loss: 2.1867 - val_accuracy: 0.6229
Epoch 32/500
123/123 - 23s - loss: 0.0363 - accuracy: 0.9875 - val_loss: 2.2610 - val_accuracy: 0.6292
Epoch 33/500
123/123 - 23s - loss: 0.0309 - accuracy: 0.9895 - val_loss: 2.2811 - val_accuracy: 0.6354
Epoch 34/500
123/123 - 24s - loss: 0.0415 - accuracy: 0.9880 - val_loss: 2.2549 - val_accuracy: 0.6292
========================================
save_weights
h5_weights/MSC.pp/onehot_embedding_dense.h5
========================================

end time >>> Mon Oct  4 00:45:10 2021

end time >>> Mon Oct  4 00:45:10 2021

end time >>> Mon Oct  4 00:45:10 2021

end time >>> Mon Oct  4 00:45:10 2021

end time >>> Mon Oct  4 00:45:10 2021












args.model = onehot_embedding_dense
time used = 836.3842189311981


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 00:45:11 2021

begin time >>> Mon Oct  4 00:45:11 2021

begin time >>> Mon Oct  4 00:45:11 2021

begin time >>> Mon Oct  4 00:45:11 2021

begin time >>> Mon Oct  4 00:45:11 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_one_branch
args.type = train
args.name = MSC.pp
args.length = 10001
===========================


-> h5_weights/MSC.pp folder already exist. pass.
-> result/MSC.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/MSC.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/MSC.pp/onehot_embedding_dense folder already exist. pass.
-> result/MSC.pp/onehot_dense folder already exist. pass.
-> result/MSC.pp/onehot_resnet18 folder already exist. pass.
-> result/MSC.pp/onehot_resnet34 folder already exist. pass.
-> result/MSC.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/MSC.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/MSC.pp/embedding_dense folder already exist. pass.
-> result/MSC.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/MSC.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
MSC.pp
########################################

########################################
model_name
onehot_embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
124/124 - 18s - loss: 0.9081 - accuracy: 0.4968 - val_loss: 0.6955 - val_accuracy: 0.5460
Epoch 2/500
124/124 - 17s - loss: 0.8825 - accuracy: 0.5047 - val_loss: 0.7018 - val_accuracy: 0.5460
Epoch 3/500
124/124 - 17s - loss: 0.9044 - accuracy: 0.4865 - val_loss: 0.7019 - val_accuracy: 0.5460
Epoch 4/500
124/124 - 17s - loss: 0.8593 - accuracy: 0.5166 - val_loss: 0.6964 - val_accuracy: 0.5460
Epoch 5/500
124/124 - 17s - loss: 0.8581 - accuracy: 0.5135 - val_loss: 0.6928 - val_accuracy: 0.5337
Epoch 6/500
124/124 - 17s - loss: 0.8459 - accuracy: 0.5224 - val_loss: 0.6919 - val_accuracy: 0.5256
Epoch 7/500
124/124 - 17s - loss: 0.8494 - accuracy: 0.5184 - val_loss: 0.6910 - val_accuracy: 0.5215
Epoch 8/500
124/124 - 17s - loss: 0.8250 - accuracy: 0.5237 - val_loss: 0.6905 - val_accuracy: 0.5378
Epoch 9/500
124/124 - 17s - loss: 0.8052 - accuracy: 0.5480 - val_loss: 0.6894 - val_accuracy: 0.5276
Epoch 10/500
124/124 - 17s - loss: 0.8190 - accuracy: 0.5374 - val_loss: 0.6884 - val_accuracy: 0.5337
Epoch 11/500
124/124 - 17s - loss: 0.7928 - accuracy: 0.5449 - val_loss: 0.6873 - val_accuracy: 0.5440
Epoch 12/500
124/124 - 17s - loss: 0.7988 - accuracy: 0.5419 - val_loss: 0.6863 - val_accuracy: 0.5399
Epoch 13/500
124/124 - 17s - loss: 0.7879 - accuracy: 0.5551 - val_loss: 0.6857 - val_accuracy: 0.5358
Epoch 14/500
124/124 - 17s - loss: 0.7951 - accuracy: 0.5376 - val_loss: 0.6841 - val_accuracy: 0.5501
Epoch 15/500
124/124 - 17s - loss: 0.7826 - accuracy: 0.5487 - val_loss: 0.6835 - val_accuracy: 0.5399
Epoch 16/500
124/124 - 17s - loss: 0.7767 - accuracy: 0.5604 - val_loss: 0.6823 - val_accuracy: 0.5460
Epoch 17/500
124/124 - 17s - loss: 0.7727 - accuracy: 0.5617 - val_loss: 0.6811 - val_accuracy: 0.5501
Epoch 18/500
124/124 - 17s - loss: 0.7698 - accuracy: 0.5619 - val_loss: 0.6810 - val_accuracy: 0.5378
Epoch 19/500
124/124 - 17s - loss: 0.7578 - accuracy: 0.5695 - val_loss: 0.6801 - val_accuracy: 0.5399
Epoch 20/500
124/124 - 17s - loss: 0.7556 - accuracy: 0.5720 - val_loss: 0.6791 - val_accuracy: 0.5378
Epoch 21/500
124/124 - 17s - loss: 0.7468 - accuracy: 0.5845 - val_loss: 0.6793 - val_accuracy: 0.5378
Epoch 22/500
124/124 - 17s - loss: 0.7361 - accuracy: 0.5751 - val_loss: 0.6782 - val_accuracy: 0.5440
Epoch 23/500
124/124 - 17s - loss: 0.7562 - accuracy: 0.5713 - val_loss: 0.6781 - val_accuracy: 0.5399
Epoch 24/500
124/124 - 17s - loss: 0.7321 - accuracy: 0.5817 - val_loss: 0.6772 - val_accuracy: 0.5501
Epoch 25/500
124/124 - 17s - loss: 0.7336 - accuracy: 0.5814 - val_loss: 0.6767 - val_accuracy: 0.5501
Epoch 26/500
124/124 - 17s - loss: 0.7295 - accuracy: 0.5822 - val_loss: 0.6758 - val_accuracy: 0.5542
Epoch 27/500
124/124 - 17s - loss: 0.7146 - accuracy: 0.5996 - val_loss: 0.6750 - val_accuracy: 0.5521
Epoch 28/500
124/124 - 17s - loss: 0.7023 - accuracy: 0.6040 - val_loss: 0.6741 - val_accuracy: 0.5603
Epoch 29/500
124/124 - 17s - loss: 0.6924 - accuracy: 0.6141 - val_loss: 0.6740 - val_accuracy: 0.5521
Epoch 30/500
124/124 - 17s - loss: 0.6986 - accuracy: 0.6098 - val_loss: 0.6727 - val_accuracy: 0.5583
Epoch 31/500
124/124 - 17s - loss: 0.6912 - accuracy: 0.6161 - val_loss: 0.6734 - val_accuracy: 0.5583
Epoch 32/500
124/124 - 17s - loss: 0.6901 - accuracy: 0.6136 - val_loss: 0.6717 - val_accuracy: 0.5481
Epoch 33/500
124/124 - 17s - loss: 0.6710 - accuracy: 0.6234 - val_loss: 0.6712 - val_accuracy: 0.5521
Epoch 34/500
124/124 - 17s - loss: 0.6732 - accuracy: 0.6270 - val_loss: 0.6701 - val_accuracy: 0.5542
Epoch 35/500
124/124 - 17s - loss: 0.6669 - accuracy: 0.6252 - val_loss: 0.6700 - val_accuracy: 0.5644
Epoch 36/500
124/124 - 17s - loss: 0.6571 - accuracy: 0.6442 - val_loss: 0.6689 - val_accuracy: 0.5521
Epoch 37/500
124/124 - 17s - loss: 0.6546 - accuracy: 0.6361 - val_loss: 0.6686 - val_accuracy: 0.5562
Epoch 38/500
124/124 - 17s - loss: 0.6521 - accuracy: 0.6437 - val_loss: 0.6681 - val_accuracy: 0.5562
Epoch 39/500
124/124 - 17s - loss: 0.6438 - accuracy: 0.6561 - val_loss: 0.6674 - val_accuracy: 0.5583
Epoch 40/500
124/124 - 17s - loss: 0.6494 - accuracy: 0.6483 - val_loss: 0.6660 - val_accuracy: 0.5562
Epoch 41/500
124/124 - 17s - loss: 0.6440 - accuracy: 0.6511 - val_loss: 0.6670 - val_accuracy: 0.5624
Epoch 42/500
124/124 - 17s - loss: 0.6446 - accuracy: 0.6551 - val_loss: 0.6655 - val_accuracy: 0.5665
Epoch 43/500
124/124 - 17s - loss: 0.6372 - accuracy: 0.6554 - val_loss: 0.6649 - val_accuracy: 0.5685
Epoch 44/500
124/124 - 17s - loss: 0.6217 - accuracy: 0.6655 - val_loss: 0.6642 - val_accuracy: 0.5665
Epoch 45/500
124/124 - 17s - loss: 0.6046 - accuracy: 0.6807 - val_loss: 0.6635 - val_accuracy: 0.5685
Epoch 46/500
124/124 - 17s - loss: 0.6123 - accuracy: 0.6728 - val_loss: 0.6641 - val_accuracy: 0.5808
Epoch 47/500
124/124 - 17s - loss: 0.6029 - accuracy: 0.6804 - val_loss: 0.6645 - val_accuracy: 0.5828
Epoch 48/500
124/124 - 17s - loss: 0.6120 - accuracy: 0.6787 - val_loss: 0.6635 - val_accuracy: 0.5849
Epoch 49/500
124/124 - 17s - loss: 0.5903 - accuracy: 0.6873 - val_loss: 0.6630 - val_accuracy: 0.5828
Epoch 50/500
124/124 - 17s - loss: 0.5618 - accuracy: 0.7101 - val_loss: 0.6631 - val_accuracy: 0.5828
Epoch 51/500
124/124 - 17s - loss: 0.5697 - accuracy: 0.7047 - val_loss: 0.6614 - val_accuracy: 0.5910
Epoch 52/500
124/124 - 17s - loss: 0.5552 - accuracy: 0.7166 - val_loss: 0.6632 - val_accuracy: 0.5849
Epoch 53/500
124/124 - 17s - loss: 0.5568 - accuracy: 0.7159 - val_loss: 0.6630 - val_accuracy: 0.5910
Epoch 54/500
124/124 - 17s - loss: 0.5668 - accuracy: 0.7065 - val_loss: 0.6621 - val_accuracy: 0.5849
Epoch 55/500
124/124 - 17s - loss: 0.5445 - accuracy: 0.7247 - val_loss: 0.6621 - val_accuracy: 0.5849
Epoch 56/500
124/124 - 17s - loss: 0.5560 - accuracy: 0.7146 - val_loss: 0.6631 - val_accuracy: 0.5910
Epoch 57/500
124/124 - 17s - loss: 0.5322 - accuracy: 0.7318 - val_loss: 0.6635 - val_accuracy: 0.5910
Epoch 58/500
124/124 - 17s - loss: 0.5337 - accuracy: 0.7283 - val_loss: 0.6651 - val_accuracy: 0.5930
Epoch 59/500
124/124 - 17s - loss: 0.5052 - accuracy: 0.7458 - val_loss: 0.6647 - val_accuracy: 0.5910
Epoch 60/500
124/124 - 17s - loss: 0.5134 - accuracy: 0.7442 - val_loss: 0.6640 - val_accuracy: 0.5910
Epoch 61/500
124/124 - 17s - loss: 0.5050 - accuracy: 0.7566 - val_loss: 0.6648 - val_accuracy: 0.5869
Epoch 62/500
124/124 - 17s - loss: 0.4910 - accuracy: 0.7539 - val_loss: 0.6645 - val_accuracy: 0.5869
Epoch 63/500
124/124 - 17s - loss: 0.4990 - accuracy: 0.7511 - val_loss: 0.6647 - val_accuracy: 0.5828
Epoch 64/500
124/124 - 17s - loss: 0.4925 - accuracy: 0.7627 - val_loss: 0.6674 - val_accuracy: 0.5869
Epoch 65/500
124/124 - 17s - loss: 0.4839 - accuracy: 0.7703 - val_loss: 0.6682 - val_accuracy: 0.5849
Epoch 66/500
124/124 - 17s - loss: 0.4577 - accuracy: 0.7911 - val_loss: 0.6660 - val_accuracy: 0.5930
Epoch 67/500
124/124 - 17s - loss: 0.4640 - accuracy: 0.7787 - val_loss: 0.6688 - val_accuracy: 0.5890
Epoch 68/500
124/124 - 17s - loss: 0.4582 - accuracy: 0.7845 - val_loss: 0.6680 - val_accuracy: 0.5869
Epoch 69/500
124/124 - 17s - loss: 0.4427 - accuracy: 0.7918 - val_loss: 0.6700 - val_accuracy: 0.5890
Epoch 70/500
124/124 - 17s - loss: 0.4426 - accuracy: 0.7888 - val_loss: 0.6705 - val_accuracy: 0.5910
Epoch 71/500
124/124 - 17s - loss: 0.4329 - accuracy: 0.7987 - val_loss: 0.6739 - val_accuracy: 0.5890
Epoch 72/500
124/124 - 17s - loss: 0.4242 - accuracy: 0.8058 - val_loss: 0.6738 - val_accuracy: 0.5869
Epoch 73/500
124/124 - 17s - loss: 0.4334 - accuracy: 0.7994 - val_loss: 0.6742 - val_accuracy: 0.5910
Epoch 74/500
124/124 - 17s - loss: 0.4138 - accuracy: 0.8070 - val_loss: 0.6773 - val_accuracy: 0.5869
Epoch 75/500
124/124 - 17s - loss: 0.4101 - accuracy: 0.8151 - val_loss: 0.6785 - val_accuracy: 0.5849
Epoch 76/500
124/124 - 17s - loss: 0.3950 - accuracy: 0.8255 - val_loss: 0.6780 - val_accuracy: 0.5951
Epoch 77/500
124/124 - 17s - loss: 0.3911 - accuracy: 0.8215 - val_loss: 0.6812 - val_accuracy: 0.5910
Epoch 78/500
124/124 - 17s - loss: 0.3830 - accuracy: 0.8255 - val_loss: 0.6834 - val_accuracy: 0.5890
Epoch 79/500
124/124 - 17s - loss: 0.3740 - accuracy: 0.8362 - val_loss: 0.6848 - val_accuracy: 0.5910
Epoch 80/500
124/124 - 17s - loss: 0.3776 - accuracy: 0.8286 - val_loss: 0.6875 - val_accuracy: 0.5910
Epoch 81/500
124/124 - 17s - loss: 0.3642 - accuracy: 0.8422 - val_loss: 0.6900 - val_accuracy: 0.5930
Epoch 82/500
124/124 - 17s - loss: 0.3740 - accuracy: 0.8301 - val_loss: 0.6911 - val_accuracy: 0.5849
Epoch 83/500
124/124 - 17s - loss: 0.3545 - accuracy: 0.8453 - val_loss: 0.6945 - val_accuracy: 0.5849
Epoch 84/500
124/124 - 17s - loss: 0.3574 - accuracy: 0.8427 - val_loss: 0.6948 - val_accuracy: 0.5890
Epoch 85/500
124/124 - 17s - loss: 0.3314 - accuracy: 0.8587 - val_loss: 0.6974 - val_accuracy: 0.5869
Epoch 86/500
124/124 - 17s - loss: 0.3289 - accuracy: 0.8622 - val_loss: 0.7014 - val_accuracy: 0.5910
Epoch 87/500
124/124 - 17s - loss: 0.3407 - accuracy: 0.8511 - val_loss: 0.7010 - val_accuracy: 0.5910
Epoch 88/500
124/124 - 17s - loss: 0.3186 - accuracy: 0.8681 - val_loss: 0.7025 - val_accuracy: 0.5971
Epoch 89/500
124/124 - 17s - loss: 0.3203 - accuracy: 0.8605 - val_loss: 0.7056 - val_accuracy: 0.5971
Epoch 90/500
124/124 - 17s - loss: 0.3164 - accuracy: 0.8610 - val_loss: 0.7087 - val_accuracy: 0.5992
Epoch 91/500
124/124 - 17s - loss: 0.3110 - accuracy: 0.8739 - val_loss: 0.7109 - val_accuracy: 0.5951
Epoch 92/500
124/124 - 17s - loss: 0.2872 - accuracy: 0.8797 - val_loss: 0.7141 - val_accuracy: 0.5992
Epoch 93/500
124/124 - 17s - loss: 0.3009 - accuracy: 0.8749 - val_loss: 0.7175 - val_accuracy: 0.6012
Epoch 94/500
124/124 - 17s - loss: 0.2886 - accuracy: 0.8779 - val_loss: 0.7196 - val_accuracy: 0.5951
Epoch 95/500
124/124 - 17s - loss: 0.2757 - accuracy: 0.8904 - val_loss: 0.7234 - val_accuracy: 0.5971
Epoch 96/500
124/124 - 17s - loss: 0.2852 - accuracy: 0.8843 - val_loss: 0.7265 - val_accuracy: 0.5971
Epoch 97/500
124/124 - 17s - loss: 0.2781 - accuracy: 0.8845 - val_loss: 0.7297 - val_accuracy: 0.5992
Epoch 98/500
124/124 - 17s - loss: 0.2665 - accuracy: 0.8909 - val_loss: 0.7357 - val_accuracy: 0.6012
Epoch 99/500
124/124 - 17s - loss: 0.2678 - accuracy: 0.8921 - val_loss: 0.7366 - val_accuracy: 0.5992
Epoch 100/500
124/124 - 17s - loss: 0.2628 - accuracy: 0.8992 - val_loss: 0.7367 - val_accuracy: 0.5992
Epoch 101/500
124/124 - 17s - loss: 0.2593 - accuracy: 0.8893 - val_loss: 0.7437 - val_accuracy: 0.6033
Epoch 102/500
124/124 - 17s - loss: 0.2490 - accuracy: 0.9028 - val_loss: 0.7476 - val_accuracy: 0.5992
Epoch 103/500
124/124 - 17s - loss: 0.2438 - accuracy: 0.9007 - val_loss: 0.7518 - val_accuracy: 0.6012
Epoch 104/500
124/124 - 17s - loss: 0.2534 - accuracy: 0.8967 - val_loss: 0.7526 - val_accuracy: 0.6053
Epoch 105/500
124/124 - 17s - loss: 0.2410 - accuracy: 0.9030 - val_loss: 0.7574 - val_accuracy: 0.5992
Epoch 106/500
124/124 - 17s - loss: 0.2260 - accuracy: 0.9101 - val_loss: 0.7631 - val_accuracy: 0.6033
Epoch 107/500
124/124 - 17s - loss: 0.2224 - accuracy: 0.9111 - val_loss: 0.7683 - val_accuracy: 0.6012
Epoch 108/500
124/124 - 17s - loss: 0.2068 - accuracy: 0.9238 - val_loss: 0.7673 - val_accuracy: 0.5992
Epoch 109/500
124/124 - 17s - loss: 0.2189 - accuracy: 0.9126 - val_loss: 0.7710 - val_accuracy: 0.5992
Epoch 110/500
124/124 - 17s - loss: 0.2195 - accuracy: 0.9134 - val_loss: 0.7759 - val_accuracy: 0.6012
Epoch 111/500
124/124 - 17s - loss: 0.2058 - accuracy: 0.9172 - val_loss: 0.7809 - val_accuracy: 0.5992
Epoch 112/500
124/124 - 17s - loss: 0.2053 - accuracy: 0.9220 - val_loss: 0.7874 - val_accuracy: 0.6033
Epoch 113/500
124/124 - 17s - loss: 0.1993 - accuracy: 0.9200 - val_loss: 0.7905 - val_accuracy: 0.5971
Epoch 114/500
124/124 - 17s - loss: 0.2004 - accuracy: 0.9212 - val_loss: 0.7951 - val_accuracy: 0.5971
Epoch 115/500
124/124 - 17s - loss: 0.2001 - accuracy: 0.9200 - val_loss: 0.7977 - val_accuracy: 0.5992
Epoch 116/500
124/124 - 17s - loss: 0.1917 - accuracy: 0.9309 - val_loss: 0.8002 - val_accuracy: 0.6012
Epoch 117/500
124/124 - 17s - loss: 0.1837 - accuracy: 0.9263 - val_loss: 0.8070 - val_accuracy: 0.6033
Epoch 118/500
124/124 - 17s - loss: 0.1869 - accuracy: 0.9273 - val_loss: 0.8079 - val_accuracy: 0.5992
Epoch 119/500
124/124 - 17s - loss: 0.1722 - accuracy: 0.9347 - val_loss: 0.8139 - val_accuracy: 0.5992
Epoch 120/500
124/124 - 17s - loss: 0.1748 - accuracy: 0.9375 - val_loss: 0.8147 - val_accuracy: 0.6012
Epoch 121/500
124/124 - 17s - loss: 0.1815 - accuracy: 0.9311 - val_loss: 0.8159 - val_accuracy: 0.6033
Epoch 122/500
124/124 - 17s - loss: 0.1771 - accuracy: 0.9331 - val_loss: 0.8231 - val_accuracy: 0.6033
Epoch 123/500
124/124 - 17s - loss: 0.1662 - accuracy: 0.9405 - val_loss: 0.8291 - val_accuracy: 0.5992
Epoch 124/500
124/124 - 17s - loss: 0.1698 - accuracy: 0.9347 - val_loss: 0.8300 - val_accuracy: 0.6012
========================================
save_weights
h5_weights/MSC.pp/onehot_embedding_cnn_one_branch.h5
========================================

end time >>> Mon Oct  4 01:21:06 2021

end time >>> Mon Oct  4 01:21:06 2021

end time >>> Mon Oct  4 01:21:06 2021

end time >>> Mon Oct  4 01:21:06 2021

end time >>> Mon Oct  4 01:21:06 2021












args.model = onehot_embedding_cnn_one_branch
time used = 2154.9008951187134


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 01:21:07 2021

begin time >>> Mon Oct  4 01:21:07 2021

begin time >>> Mon Oct  4 01:21:07 2021

begin time >>> Mon Oct  4 01:21:07 2021

begin time >>> Mon Oct  4 01:21:07 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_two_branch
args.type = train
args.name = MSC.pp
args.length = 10001
===========================


-> h5_weights/MSC.pp folder already exist. pass.
-> result/MSC.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/MSC.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/MSC.pp/onehot_embedding_dense folder already exist. pass.
-> result/MSC.pp/onehot_dense folder already exist. pass.
-> result/MSC.pp/onehot_resnet18 folder already exist. pass.
-> result/MSC.pp/onehot_resnet34 folder already exist. pass.
-> result/MSC.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/MSC.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/MSC.pp/embedding_dense folder already exist. pass.
-> result/MSC.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/MSC.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
MSC.pp
########################################

########################################
model_name
onehot_embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
124/124 - 18s - loss: 0.9195 - accuracy: 0.4991 - val_loss: 0.6902 - val_accuracy: 0.5460
Epoch 2/500
124/124 - 17s - loss: 0.9016 - accuracy: 0.5196 - val_loss: 0.6921 - val_accuracy: 0.5460
Epoch 3/500
124/124 - 17s - loss: 0.8964 - accuracy: 0.5032 - val_loss: 0.6914 - val_accuracy: 0.5460
Epoch 4/500
124/124 - 17s - loss: 0.8772 - accuracy: 0.5090 - val_loss: 0.6950 - val_accuracy: 0.5460
Epoch 5/500
124/124 - 17s - loss: 0.8468 - accuracy: 0.5194 - val_loss: 0.6961 - val_accuracy: 0.5460
Epoch 6/500
124/124 - 17s - loss: 0.8666 - accuracy: 0.5244 - val_loss: 0.6940 - val_accuracy: 0.5583
Epoch 7/500
124/124 - 17s - loss: 0.8411 - accuracy: 0.5310 - val_loss: 0.6921 - val_accuracy: 0.5562
Epoch 8/500
124/124 - 17s - loss: 0.8203 - accuracy: 0.5358 - val_loss: 0.6915 - val_accuracy: 0.5501
Epoch 9/500
124/124 - 17s - loss: 0.8332 - accuracy: 0.5270 - val_loss: 0.6893 - val_accuracy: 0.5521
Epoch 10/500
124/124 - 17s - loss: 0.8190 - accuracy: 0.5439 - val_loss: 0.6884 - val_accuracy: 0.5501
Epoch 11/500
124/124 - 17s - loss: 0.8162 - accuracy: 0.5482 - val_loss: 0.6857 - val_accuracy: 0.5481
Epoch 12/500
124/124 - 17s - loss: 0.8104 - accuracy: 0.5417 - val_loss: 0.6847 - val_accuracy: 0.5481
Epoch 13/500
124/124 - 17s - loss: 0.7820 - accuracy: 0.5558 - val_loss: 0.6842 - val_accuracy: 0.5501
Epoch 14/500
124/124 - 17s - loss: 0.7829 - accuracy: 0.5465 - val_loss: 0.6827 - val_accuracy: 0.5603
Epoch 15/500
124/124 - 17s - loss: 0.7669 - accuracy: 0.5639 - val_loss: 0.6815 - val_accuracy: 0.5644
Epoch 16/500
124/124 - 17s - loss: 0.7559 - accuracy: 0.5801 - val_loss: 0.6803 - val_accuracy: 0.5665
Epoch 17/500
124/124 - 17s - loss: 0.7727 - accuracy: 0.5647 - val_loss: 0.6800 - val_accuracy: 0.5665
Epoch 18/500
124/124 - 17s - loss: 0.7492 - accuracy: 0.5834 - val_loss: 0.6784 - val_accuracy: 0.5665
Epoch 19/500
124/124 - 17s - loss: 0.7580 - accuracy: 0.5743 - val_loss: 0.6779 - val_accuracy: 0.5787
Epoch 20/500
124/124 - 17s - loss: 0.7458 - accuracy: 0.5801 - val_loss: 0.6770 - val_accuracy: 0.5808
Epoch 21/500
124/124 - 17s - loss: 0.7301 - accuracy: 0.5877 - val_loss: 0.6764 - val_accuracy: 0.5808
Epoch 22/500
124/124 - 17s - loss: 0.7378 - accuracy: 0.5898 - val_loss: 0.6758 - val_accuracy: 0.5808
Epoch 23/500
124/124 - 17s - loss: 0.7157 - accuracy: 0.6088 - val_loss: 0.6756 - val_accuracy: 0.5849
Epoch 24/500
124/124 - 17s - loss: 0.7168 - accuracy: 0.5885 - val_loss: 0.6749 - val_accuracy: 0.5890
Epoch 25/500
124/124 - 17s - loss: 0.7145 - accuracy: 0.6098 - val_loss: 0.6741 - val_accuracy: 0.5849
Epoch 26/500
124/124 - 17s - loss: 0.6872 - accuracy: 0.6242 - val_loss: 0.6745 - val_accuracy: 0.5828
Epoch 27/500
124/124 - 17s - loss: 0.6929 - accuracy: 0.6080 - val_loss: 0.6738 - val_accuracy: 0.5787
Epoch 28/500
124/124 - 17s - loss: 0.7086 - accuracy: 0.6052 - val_loss: 0.6729 - val_accuracy: 0.5849
Epoch 29/500
124/124 - 17s - loss: 0.6829 - accuracy: 0.6250 - val_loss: 0.6727 - val_accuracy: 0.5849
Epoch 30/500
124/124 - 17s - loss: 0.6777 - accuracy: 0.6326 - val_loss: 0.6722 - val_accuracy: 0.5828
Epoch 31/500
124/124 - 17s - loss: 0.6674 - accuracy: 0.6364 - val_loss: 0.6714 - val_accuracy: 0.5869
Epoch 32/500
124/124 - 17s - loss: 0.6582 - accuracy: 0.6429 - val_loss: 0.6710 - val_accuracy: 0.5869
Epoch 33/500
124/124 - 17s - loss: 0.6617 - accuracy: 0.6288 - val_loss: 0.6708 - val_accuracy: 0.5930
Epoch 34/500
124/124 - 17s - loss: 0.6521 - accuracy: 0.6480 - val_loss: 0.6699 - val_accuracy: 0.5910
Epoch 35/500
124/124 - 17s - loss: 0.6639 - accuracy: 0.6409 - val_loss: 0.6704 - val_accuracy: 0.5992
Epoch 36/500
124/124 - 17s - loss: 0.6504 - accuracy: 0.6460 - val_loss: 0.6701 - val_accuracy: 0.5971
Epoch 37/500
124/124 - 17s - loss: 0.6283 - accuracy: 0.6589 - val_loss: 0.6693 - val_accuracy: 0.5951
Epoch 38/500
124/124 - 17s - loss: 0.6338 - accuracy: 0.6617 - val_loss: 0.6686 - val_accuracy: 0.5951
Epoch 39/500
124/124 - 17s - loss: 0.5995 - accuracy: 0.6875 - val_loss: 0.6686 - val_accuracy: 0.5992
Epoch 40/500
124/124 - 17s - loss: 0.6217 - accuracy: 0.6733 - val_loss: 0.6689 - val_accuracy: 0.6094
Epoch 41/500
124/124 - 17s - loss: 0.6075 - accuracy: 0.6842 - val_loss: 0.6690 - val_accuracy: 0.6053
Epoch 42/500
124/124 - 17s - loss: 0.5954 - accuracy: 0.6832 - val_loss: 0.6686 - val_accuracy: 0.6012
Epoch 43/500
124/124 - 17s - loss: 0.5839 - accuracy: 0.6898 - val_loss: 0.6689 - val_accuracy: 0.5971
Epoch 44/500
124/124 - 17s - loss: 0.5761 - accuracy: 0.7012 - val_loss: 0.6691 - val_accuracy: 0.6033
Epoch 45/500
124/124 - 17s - loss: 0.5673 - accuracy: 0.6971 - val_loss: 0.6690 - val_accuracy: 0.6012
Epoch 46/500
124/124 - 17s - loss: 0.5745 - accuracy: 0.7065 - val_loss: 0.6694 - val_accuracy: 0.5992
Epoch 47/500
124/124 - 17s - loss: 0.5526 - accuracy: 0.7179 - val_loss: 0.6682 - val_accuracy: 0.5951
Epoch 48/500
124/124 - 17s - loss: 0.5505 - accuracy: 0.7149 - val_loss: 0.6689 - val_accuracy: 0.6012
Epoch 49/500
124/124 - 17s - loss: 0.5424 - accuracy: 0.7217 - val_loss: 0.6693 - val_accuracy: 0.5951
Epoch 50/500
124/124 - 17s - loss: 0.5314 - accuracy: 0.7293 - val_loss: 0.6699 - val_accuracy: 0.5951
Epoch 51/500
124/124 - 17s - loss: 0.5431 - accuracy: 0.7278 - val_loss: 0.6704 - val_accuracy: 0.5910
Epoch 52/500
124/124 - 17s - loss: 0.5228 - accuracy: 0.7399 - val_loss: 0.6704 - val_accuracy: 0.5971
Epoch 53/500
124/124 - 17s - loss: 0.5149 - accuracy: 0.7478 - val_loss: 0.6712 - val_accuracy: 0.5910
Epoch 54/500
124/124 - 17s - loss: 0.5191 - accuracy: 0.7422 - val_loss: 0.6712 - val_accuracy: 0.5971
Epoch 55/500
124/124 - 17s - loss: 0.5082 - accuracy: 0.7447 - val_loss: 0.6713 - val_accuracy: 0.5971
Epoch 56/500
124/124 - 17s - loss: 0.5029 - accuracy: 0.7549 - val_loss: 0.6720 - val_accuracy: 0.5930
Epoch 57/500
124/124 - 17s - loss: 0.4874 - accuracy: 0.7587 - val_loss: 0.6722 - val_accuracy: 0.5951
Epoch 58/500
124/124 - 17s - loss: 0.4801 - accuracy: 0.7675 - val_loss: 0.6742 - val_accuracy: 0.5930
Epoch 59/500
124/124 - 17s - loss: 0.4681 - accuracy: 0.7777 - val_loss: 0.6750 - val_accuracy: 0.5910
Epoch 60/500
124/124 - 17s - loss: 0.4675 - accuracy: 0.7792 - val_loss: 0.6747 - val_accuracy: 0.5930
========================================
save_weights
h5_weights/MSC.pp/onehot_embedding_cnn_two_branch.h5
========================================

end time >>> Mon Oct  4 01:38:35 2021

end time >>> Mon Oct  4 01:38:35 2021

end time >>> Mon Oct  4 01:38:35 2021

end time >>> Mon Oct  4 01:38:35 2021

end time >>> Mon Oct  4 01:38:35 2021












args.model = onehot_embedding_cnn_two_branch
time used = 1047.6603927612305


