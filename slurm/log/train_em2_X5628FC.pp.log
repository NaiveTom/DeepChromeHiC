************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 07:30:40 2021

begin time >>> Mon Oct  4 07:30:40 2021

begin time >>> Mon Oct  4 07:30:40 2021

begin time >>> Mon Oct  4 07:30:40 2021

begin time >>> Mon Oct  4 07:30:40 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_dense
args.type = train
args.name = X5628FC.pp
args.length = 10001
===========================


-> h5_weights/X5628FC.pp folder already exist. pass.
-> result/X5628FC.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/X5628FC.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/X5628FC.pp/onehot_embedding_dense folder already exist. pass.
-> result/X5628FC.pp/onehot_dense folder already exist. pass.
-> result/X5628FC.pp/onehot_resnet18 folder already exist. pass.
-> result/X5628FC.pp/onehot_resnet34 folder already exist. pass.
-> result/X5628FC.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/X5628FC.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/X5628FC.pp/embedding_dense folder already exist. pass.
-> result/X5628FC.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/X5628FC.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
X5628FC.pp
########################################

########################################
model_name
onehot_embedding_dense
########################################

Found 7224 images belonging to 2 classes.
Found 892 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 20002, 5, 1, 8)    32776     
_________________________________________________________________
flatten (Flatten)            (None, 800080)            0         
_________________________________________________________________
batch_normalization (BatchNo (None, 800080)            3200320   
_________________________________________________________________
dense (Dense)                (None, 512)               409641472 
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_3 (Batch (None, 512)               2048      
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_3 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 413,671,754
Trainable params: 412,067,498
Non-trainable params: 1,604,256
_________________________________________________________________
Epoch 1/500
225/225 - 42s - loss: 0.7528 - accuracy: 0.5709 - val_loss: 0.7286 - val_accuracy: 0.4965
Epoch 2/500
225/225 - 41s - loss: 0.6349 - accuracy: 0.6648 - val_loss: 0.9749 - val_accuracy: 0.4954
Epoch 3/500
225/225 - 42s - loss: 0.5376 - accuracy: 0.7404 - val_loss: 1.0793 - val_accuracy: 0.5255
Epoch 4/500
225/225 - 42s - loss: 0.4521 - accuracy: 0.7868 - val_loss: 1.1580 - val_accuracy: 0.5775
Epoch 5/500
225/225 - 42s - loss: 0.3694 - accuracy: 0.8412 - val_loss: 1.3473 - val_accuracy: 0.5822
Epoch 6/500
225/225 - 42s - loss: 0.3060 - accuracy: 0.8731 - val_loss: 1.5102 - val_accuracy: 0.5856
Epoch 7/500
225/225 - 42s - loss: 0.2730 - accuracy: 0.8871 - val_loss: 1.5855 - val_accuracy: 0.5926
Epoch 8/500
225/225 - 42s - loss: 0.2397 - accuracy: 0.9036 - val_loss: 1.6786 - val_accuracy: 0.5972
Epoch 9/500
225/225 - 42s - loss: 0.2118 - accuracy: 0.9162 - val_loss: 1.7516 - val_accuracy: 0.6123
Epoch 10/500
225/225 - 41s - loss: 0.1742 - accuracy: 0.9285 - val_loss: 1.8553 - val_accuracy: 0.6088
Epoch 11/500
225/225 - 41s - loss: 0.1702 - accuracy: 0.9320 - val_loss: 1.9263 - val_accuracy: 0.6019
Epoch 12/500
225/225 - 41s - loss: 0.1436 - accuracy: 0.9449 - val_loss: 1.9563 - val_accuracy: 0.5984
Epoch 13/500
225/225 - 41s - loss: 0.1382 - accuracy: 0.9455 - val_loss: 1.9827 - val_accuracy: 0.6053
Epoch 14/500
225/225 - 41s - loss: 0.1270 - accuracy: 0.9494 - val_loss: 1.9994 - val_accuracy: 0.6065
Epoch 15/500
225/225 - 42s - loss: 0.1065 - accuracy: 0.9593 - val_loss: 2.0641 - val_accuracy: 0.6134
Epoch 16/500
225/225 - 41s - loss: 0.1104 - accuracy: 0.9598 - val_loss: 2.1787 - val_accuracy: 0.6123
Epoch 17/500
225/225 - 42s - loss: 0.0994 - accuracy: 0.9597 - val_loss: 2.2145 - val_accuracy: 0.6192
Epoch 18/500
225/225 - 42s - loss: 0.1013 - accuracy: 0.9608 - val_loss: 2.1946 - val_accuracy: 0.6215
Epoch 19/500
225/225 - 42s - loss: 0.0968 - accuracy: 0.9615 - val_loss: 2.1509 - val_accuracy: 0.6319
Epoch 20/500
225/225 - 42s - loss: 0.0939 - accuracy: 0.9618 - val_loss: 2.1648 - val_accuracy: 0.6343
Epoch 21/500
225/225 - 42s - loss: 0.0811 - accuracy: 0.9718 - val_loss: 2.1205 - val_accuracy: 0.6447
Epoch 22/500
225/225 - 41s - loss: 0.0840 - accuracy: 0.9677 - val_loss: 2.1680 - val_accuracy: 0.6400
Epoch 23/500
225/225 - 41s - loss: 0.0804 - accuracy: 0.9702 - val_loss: 2.2142 - val_accuracy: 0.6424
Epoch 24/500
225/225 - 41s - loss: 0.0835 - accuracy: 0.9707 - val_loss: 2.2354 - val_accuracy: 0.6412
Epoch 25/500
225/225 - 41s - loss: 0.0695 - accuracy: 0.9753 - val_loss: 2.1942 - val_accuracy: 0.6343
Epoch 26/500
225/225 - 41s - loss: 0.0576 - accuracy: 0.9779 - val_loss: 2.2370 - val_accuracy: 0.6389
Epoch 27/500
225/225 - 41s - loss: 0.0628 - accuracy: 0.9758 - val_loss: 2.2487 - val_accuracy: 0.6412
Epoch 28/500
225/225 - 41s - loss: 0.0675 - accuracy: 0.9759 - val_loss: 2.2928 - val_accuracy: 0.6331
Epoch 29/500
225/225 - 41s - loss: 0.0599 - accuracy: 0.9787 - val_loss: 2.2815 - val_accuracy: 0.6343
Epoch 30/500
225/225 - 41s - loss: 0.0614 - accuracy: 0.9771 - val_loss: 2.1942 - val_accuracy: 0.6435
Epoch 31/500
225/225 - 42s - loss: 0.0619 - accuracy: 0.9769 - val_loss: 2.1898 - val_accuracy: 0.6597
Epoch 32/500
225/225 - 41s - loss: 0.0500 - accuracy: 0.9825 - val_loss: 2.1995 - val_accuracy: 0.6516
Epoch 33/500
225/225 - 41s - loss: 0.0554 - accuracy: 0.9816 - val_loss: 2.2717 - val_accuracy: 0.6481
Epoch 34/500
225/225 - 41s - loss: 0.0578 - accuracy: 0.9800 - val_loss: 2.2490 - val_accuracy: 0.6562
Epoch 35/500
225/225 - 41s - loss: 0.0502 - accuracy: 0.9829 - val_loss: 2.2290 - val_accuracy: 0.6458
Epoch 36/500
225/225 - 41s - loss: 0.0460 - accuracy: 0.9841 - val_loss: 2.2840 - val_accuracy: 0.6296
Epoch 37/500
225/225 - 41s - loss: 0.0372 - accuracy: 0.9867 - val_loss: 2.2837 - val_accuracy: 0.6308
Epoch 38/500
225/225 - 41s - loss: 0.0447 - accuracy: 0.9839 - val_loss: 2.2855 - val_accuracy: 0.6308
Epoch 39/500
225/225 - 41s - loss: 0.0427 - accuracy: 0.9851 - val_loss: 2.2671 - val_accuracy: 0.6528
Epoch 40/500
225/225 - 41s - loss: 0.0414 - accuracy: 0.9850 - val_loss: 2.3814 - val_accuracy: 0.6319
Epoch 41/500
225/225 - 42s - loss: 0.0405 - accuracy: 0.9857 - val_loss: 2.3249 - val_accuracy: 0.6551
========================================
save_weights
h5_weights/X5628FC.pp/onehot_embedding_dense.h5
========================================

end time >>> Mon Oct  4 07:59:18 2021

end time >>> Mon Oct  4 07:59:18 2021

end time >>> Mon Oct  4 07:59:18 2021

end time >>> Mon Oct  4 07:59:18 2021

end time >>> Mon Oct  4 07:59:18 2021












args.model = onehot_embedding_dense
time used = 1718.1181683540344


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 07:59:19 2021

begin time >>> Mon Oct  4 07:59:19 2021

begin time >>> Mon Oct  4 07:59:19 2021

begin time >>> Mon Oct  4 07:59:19 2021

begin time >>> Mon Oct  4 07:59:19 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_one_branch
args.type = train
args.name = X5628FC.pp
args.length = 10001
===========================


-> h5_weights/X5628FC.pp folder already exist. pass.
-> result/X5628FC.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/X5628FC.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/X5628FC.pp/onehot_embedding_dense folder already exist. pass.
-> result/X5628FC.pp/onehot_dense folder already exist. pass.
-> result/X5628FC.pp/onehot_resnet18 folder already exist. pass.
-> result/X5628FC.pp/onehot_resnet34 folder already exist. pass.
-> result/X5628FC.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/X5628FC.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/X5628FC.pp/embedding_dense folder already exist. pass.
-> result/X5628FC.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/X5628FC.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
X5628FC.pp
########################################

########################################
model_name
onehot_embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
226/226 - 32s - loss: 0.9605 - accuracy: 0.5015 - val_loss: 0.7030 - val_accuracy: 0.4658
Epoch 2/500
226/226 - 31s - loss: 0.9205 - accuracy: 0.5031 - val_loss: 0.6975 - val_accuracy: 0.4748
Epoch 3/500
226/226 - 31s - loss: 0.8809 - accuracy: 0.5152 - val_loss: 0.7000 - val_accuracy: 0.5140
Epoch 4/500
226/226 - 31s - loss: 0.8614 - accuracy: 0.5282 - val_loss: 0.7098 - val_accuracy: 0.5062
Epoch 5/500
226/226 - 31s - loss: 0.8677 - accuracy: 0.5121 - val_loss: 0.7061 - val_accuracy: 0.5218
Epoch 6/500
226/226 - 31s - loss: 0.8486 - accuracy: 0.5272 - val_loss: 0.7028 - val_accuracy: 0.5196
Epoch 7/500
226/226 - 31s - loss: 0.8434 - accuracy: 0.5182 - val_loss: 0.6978 - val_accuracy: 0.5274
Epoch 8/500
226/226 - 31s - loss: 0.8290 - accuracy: 0.5308 - val_loss: 0.6928 - val_accuracy: 0.5342
Epoch 9/500
226/226 - 31s - loss: 0.8176 - accuracy: 0.5311 - val_loss: 0.6895 - val_accuracy: 0.5454
Epoch 10/500
226/226 - 31s - loss: 0.8034 - accuracy: 0.5404 - val_loss: 0.6863 - val_accuracy: 0.5498
Epoch 11/500
226/226 - 31s - loss: 0.7934 - accuracy: 0.5477 - val_loss: 0.6826 - val_accuracy: 0.5577
Epoch 12/500
226/226 - 31s - loss: 0.7764 - accuracy: 0.5678 - val_loss: 0.6799 - val_accuracy: 0.5677
Epoch 13/500
226/226 - 31s - loss: 0.7811 - accuracy: 0.5579 - val_loss: 0.6782 - val_accuracy: 0.5633
Epoch 14/500
226/226 - 31s - loss: 0.7709 - accuracy: 0.5679 - val_loss: 0.6749 - val_accuracy: 0.5633
Epoch 15/500
226/226 - 31s - loss: 0.7737 - accuracy: 0.5631 - val_loss: 0.6727 - val_accuracy: 0.5733
Epoch 16/500
226/226 - 31s - loss: 0.7685 - accuracy: 0.5638 - val_loss: 0.6700 - val_accuracy: 0.5745
Epoch 17/500
226/226 - 31s - loss: 0.7541 - accuracy: 0.5773 - val_loss: 0.6674 - val_accuracy: 0.5935
Epoch 18/500
226/226 - 31s - loss: 0.7415 - accuracy: 0.5848 - val_loss: 0.6659 - val_accuracy: 0.5868
Epoch 19/500
226/226 - 31s - loss: 0.7505 - accuracy: 0.5758 - val_loss: 0.6630 - val_accuracy: 0.5991
Epoch 20/500
226/226 - 31s - loss: 0.7315 - accuracy: 0.5913 - val_loss: 0.6613 - val_accuracy: 0.6092
Epoch 21/500
226/226 - 31s - loss: 0.7223 - accuracy: 0.6017 - val_loss: 0.6597 - val_accuracy: 0.6081
Epoch 22/500
226/226 - 31s - loss: 0.7035 - accuracy: 0.6126 - val_loss: 0.6575 - val_accuracy: 0.6114
Epoch 23/500
226/226 - 31s - loss: 0.7005 - accuracy: 0.6083 - val_loss: 0.6551 - val_accuracy: 0.6159
Epoch 24/500
226/226 - 31s - loss: 0.7066 - accuracy: 0.6125 - val_loss: 0.6526 - val_accuracy: 0.6237
Epoch 25/500
226/226 - 31s - loss: 0.7082 - accuracy: 0.6092 - val_loss: 0.6506 - val_accuracy: 0.6305
Epoch 26/500
226/226 - 31s - loss: 0.6836 - accuracy: 0.6236 - val_loss: 0.6488 - val_accuracy: 0.6293
Epoch 27/500
226/226 - 31s - loss: 0.6915 - accuracy: 0.6207 - val_loss: 0.6472 - val_accuracy: 0.6327
Epoch 28/500
226/226 - 31s - loss: 0.6899 - accuracy: 0.6272 - val_loss: 0.6453 - val_accuracy: 0.6361
Epoch 29/500
226/226 - 31s - loss: 0.6780 - accuracy: 0.6299 - val_loss: 0.6438 - val_accuracy: 0.6349
Epoch 30/500
226/226 - 31s - loss: 0.6659 - accuracy: 0.6409 - val_loss: 0.6412 - val_accuracy: 0.6338
Epoch 31/500
226/226 - 31s - loss: 0.6552 - accuracy: 0.6513 - val_loss: 0.6396 - val_accuracy: 0.6338
Epoch 32/500
226/226 - 31s - loss: 0.6575 - accuracy: 0.6441 - val_loss: 0.6371 - val_accuracy: 0.6372
Epoch 33/500
226/226 - 31s - loss: 0.6508 - accuracy: 0.6546 - val_loss: 0.6357 - val_accuracy: 0.6417
Epoch 34/500
226/226 - 31s - loss: 0.6436 - accuracy: 0.6575 - val_loss: 0.6340 - val_accuracy: 0.6394
Epoch 35/500
226/226 - 31s - loss: 0.6220 - accuracy: 0.6747 - val_loss: 0.6321 - val_accuracy: 0.6428
Epoch 36/500
226/226 - 31s - loss: 0.6198 - accuracy: 0.6662 - val_loss: 0.6308 - val_accuracy: 0.6473
Epoch 37/500
226/226 - 31s - loss: 0.6127 - accuracy: 0.6832 - val_loss: 0.6293 - val_accuracy: 0.6473
Epoch 38/500
226/226 - 31s - loss: 0.6185 - accuracy: 0.6751 - val_loss: 0.6278 - val_accuracy: 0.6562
Epoch 39/500
226/226 - 31s - loss: 0.5994 - accuracy: 0.6926 - val_loss: 0.6266 - val_accuracy: 0.6540
Epoch 40/500
226/226 - 31s - loss: 0.5933 - accuracy: 0.6936 - val_loss: 0.6257 - val_accuracy: 0.6573
Epoch 41/500
226/226 - 31s - loss: 0.5883 - accuracy: 0.6965 - val_loss: 0.6243 - val_accuracy: 0.6573
Epoch 42/500
226/226 - 31s - loss: 0.5634 - accuracy: 0.7167 - val_loss: 0.6238 - val_accuracy: 0.6585
Epoch 43/500
226/226 - 31s - loss: 0.5648 - accuracy: 0.7158 - val_loss: 0.6225 - val_accuracy: 0.6596
Epoch 44/500
226/226 - 31s - loss: 0.5643 - accuracy: 0.7158 - val_loss: 0.6226 - val_accuracy: 0.6652
Epoch 45/500
226/226 - 31s - loss: 0.5497 - accuracy: 0.7259 - val_loss: 0.6220 - val_accuracy: 0.6663
Epoch 46/500
226/226 - 31s - loss: 0.5493 - accuracy: 0.7231 - val_loss: 0.6218 - val_accuracy: 0.6685
Epoch 47/500
226/226 - 31s - loss: 0.5318 - accuracy: 0.7336 - val_loss: 0.6215 - val_accuracy: 0.6641
Epoch 48/500
226/226 - 31s - loss: 0.5243 - accuracy: 0.7400 - val_loss: 0.6211 - val_accuracy: 0.6652
Epoch 49/500
226/226 - 31s - loss: 0.5306 - accuracy: 0.7410 - val_loss: 0.6212 - val_accuracy: 0.6663
Epoch 50/500
226/226 - 31s - loss: 0.5196 - accuracy: 0.7448 - val_loss: 0.6213 - val_accuracy: 0.6629
Epoch 51/500
226/226 - 31s - loss: 0.5061 - accuracy: 0.7563 - val_loss: 0.6222 - val_accuracy: 0.6607
Epoch 52/500
226/226 - 31s - loss: 0.5014 - accuracy: 0.7554 - val_loss: 0.6207 - val_accuracy: 0.6674
Epoch 53/500
226/226 - 31s - loss: 0.4967 - accuracy: 0.7584 - val_loss: 0.6221 - val_accuracy: 0.6652
Epoch 54/500
226/226 - 31s - loss: 0.4799 - accuracy: 0.7717 - val_loss: 0.6219 - val_accuracy: 0.6708
Epoch 55/500
226/226 - 31s - loss: 0.4682 - accuracy: 0.7763 - val_loss: 0.6219 - val_accuracy: 0.6708
Epoch 56/500
226/226 - 31s - loss: 0.4767 - accuracy: 0.7670 - val_loss: 0.6221 - val_accuracy: 0.6753
Epoch 57/500
226/226 - 31s - loss: 0.4640 - accuracy: 0.7800 - val_loss: 0.6231 - val_accuracy: 0.6753
Epoch 58/500
226/226 - 31s - loss: 0.4589 - accuracy: 0.7844 - val_loss: 0.6242 - val_accuracy: 0.6708
Epoch 59/500
226/226 - 31s - loss: 0.4506 - accuracy: 0.7876 - val_loss: 0.6261 - val_accuracy: 0.6730
Epoch 60/500
226/226 - 31s - loss: 0.4256 - accuracy: 0.8013 - val_loss: 0.6270 - val_accuracy: 0.6719
Epoch 61/500
226/226 - 31s - loss: 0.4416 - accuracy: 0.7972 - val_loss: 0.6288 - val_accuracy: 0.6708
Epoch 62/500
226/226 - 31s - loss: 0.4236 - accuracy: 0.8062 - val_loss: 0.6287 - val_accuracy: 0.6697
Epoch 63/500
226/226 - 31s - loss: 0.4164 - accuracy: 0.8073 - val_loss: 0.6300 - val_accuracy: 0.6697
Epoch 64/500
226/226 - 31s - loss: 0.4149 - accuracy: 0.8066 - val_loss: 0.6316 - val_accuracy: 0.6685
Epoch 65/500
226/226 - 31s - loss: 0.4153 - accuracy: 0.8112 - val_loss: 0.6331 - val_accuracy: 0.6719
Epoch 66/500
226/226 - 31s - loss: 0.3929 - accuracy: 0.8242 - val_loss: 0.6357 - val_accuracy: 0.6753
Epoch 67/500
226/226 - 31s - loss: 0.3808 - accuracy: 0.8296 - val_loss: 0.6384 - val_accuracy: 0.6764
Epoch 68/500
226/226 - 31s - loss: 0.3768 - accuracy: 0.8340 - val_loss: 0.6386 - val_accuracy: 0.6741
Epoch 69/500
226/226 - 31s - loss: 0.3808 - accuracy: 0.8250 - val_loss: 0.6419 - val_accuracy: 0.6764
Epoch 70/500
226/226 - 31s - loss: 0.3711 - accuracy: 0.8361 - val_loss: 0.6442 - val_accuracy: 0.6797
Epoch 71/500
226/226 - 31s - loss: 0.3815 - accuracy: 0.8333 - val_loss: 0.6465 - val_accuracy: 0.6775
Epoch 72/500
226/226 - 31s - loss: 0.3760 - accuracy: 0.8304 - val_loss: 0.6485 - val_accuracy: 0.6741
Epoch 73/500
226/226 - 31s - loss: 0.3688 - accuracy: 0.8348 - val_loss: 0.6522 - val_accuracy: 0.6775
Epoch 74/500
226/226 - 31s - loss: 0.3554 - accuracy: 0.8440 - val_loss: 0.6546 - val_accuracy: 0.6797
Epoch 75/500
226/226 - 31s - loss: 0.3406 - accuracy: 0.8510 - val_loss: 0.6565 - val_accuracy: 0.6809
Epoch 76/500
226/226 - 31s - loss: 0.3401 - accuracy: 0.8542 - val_loss: 0.6592 - val_accuracy: 0.6842
Epoch 77/500
226/226 - 31s - loss: 0.3342 - accuracy: 0.8528 - val_loss: 0.6597 - val_accuracy: 0.6797
Epoch 78/500
226/226 - 31s - loss: 0.3331 - accuracy: 0.8562 - val_loss: 0.6622 - val_accuracy: 0.6809
Epoch 79/500
226/226 - 31s - loss: 0.3310 - accuracy: 0.8578 - val_loss: 0.6660 - val_accuracy: 0.6842
Epoch 80/500
226/226 - 31s - loss: 0.3136 - accuracy: 0.8645 - val_loss: 0.6681 - val_accuracy: 0.6820
Epoch 81/500
226/226 - 31s - loss: 0.3104 - accuracy: 0.8708 - val_loss: 0.6729 - val_accuracy: 0.6853
Epoch 82/500
226/226 - 31s - loss: 0.3111 - accuracy: 0.8688 - val_loss: 0.6730 - val_accuracy: 0.6865
Epoch 83/500
226/226 - 31s - loss: 0.3162 - accuracy: 0.8664 - val_loss: 0.6766 - val_accuracy: 0.6865
Epoch 84/500
226/226 - 31s - loss: 0.2965 - accuracy: 0.8732 - val_loss: 0.6816 - val_accuracy: 0.6842
Epoch 85/500
226/226 - 31s - loss: 0.3105 - accuracy: 0.8730 - val_loss: 0.6842 - val_accuracy: 0.6820
Epoch 86/500
226/226 - 31s - loss: 0.2892 - accuracy: 0.8757 - val_loss: 0.6868 - val_accuracy: 0.6842
Epoch 87/500
226/226 - 31s - loss: 0.2844 - accuracy: 0.8766 - val_loss: 0.6913 - val_accuracy: 0.6853
Epoch 88/500
226/226 - 31s - loss: 0.2854 - accuracy: 0.8748 - val_loss: 0.6951 - val_accuracy: 0.6842
Epoch 89/500
226/226 - 31s - loss: 0.2820 - accuracy: 0.8852 - val_loss: 0.6976 - val_accuracy: 0.6865
Epoch 90/500
226/226 - 31s - loss: 0.2802 - accuracy: 0.8861 - val_loss: 0.7013 - val_accuracy: 0.6842
Epoch 91/500
226/226 - 31s - loss: 0.2815 - accuracy: 0.8850 - val_loss: 0.7066 - val_accuracy: 0.6820
Epoch 92/500
226/226 - 31s - loss: 0.2725 - accuracy: 0.8891 - val_loss: 0.7105 - val_accuracy: 0.6820
Epoch 93/500
226/226 - 31s - loss: 0.2622 - accuracy: 0.8888 - val_loss: 0.7148 - val_accuracy: 0.6853
Epoch 94/500
226/226 - 31s - loss: 0.2545 - accuracy: 0.8927 - val_loss: 0.7184 - val_accuracy: 0.6842
Epoch 95/500
226/226 - 31s - loss: 0.2557 - accuracy: 0.8944 - val_loss: 0.7210 - val_accuracy: 0.6842
Epoch 96/500
226/226 - 31s - loss: 0.2461 - accuracy: 0.8959 - val_loss: 0.7247 - val_accuracy: 0.6842
Epoch 97/500
226/226 - 31s - loss: 0.2455 - accuracy: 0.8995 - val_loss: 0.7288 - val_accuracy: 0.6887
Epoch 98/500
226/226 - 31s - loss: 0.2378 - accuracy: 0.9011 - val_loss: 0.7344 - val_accuracy: 0.6865
Epoch 99/500
226/226 - 31s - loss: 0.2418 - accuracy: 0.9006 - val_loss: 0.7396 - val_accuracy: 0.6853
Epoch 100/500
226/226 - 31s - loss: 0.2417 - accuracy: 0.8984 - val_loss: 0.7443 - val_accuracy: 0.6876
Epoch 101/500
226/226 - 31s - loss: 0.2281 - accuracy: 0.9097 - val_loss: 0.7477 - val_accuracy: 0.6876
Epoch 102/500
226/226 - 31s - loss: 0.2397 - accuracy: 0.8974 - val_loss: 0.7518 - val_accuracy: 0.6831
Epoch 103/500
226/226 - 31s - loss: 0.2346 - accuracy: 0.9023 - val_loss: 0.7551 - val_accuracy: 0.6853
Epoch 104/500
226/226 - 31s - loss: 0.2186 - accuracy: 0.9135 - val_loss: 0.7578 - val_accuracy: 0.6809
Epoch 105/500
226/226 - 31s - loss: 0.2279 - accuracy: 0.9070 - val_loss: 0.7615 - val_accuracy: 0.6853
Epoch 106/500
226/226 - 31s - loss: 0.2202 - accuracy: 0.9135 - val_loss: 0.7690 - val_accuracy: 0.6797
Epoch 107/500
226/226 - 31s - loss: 0.2216 - accuracy: 0.9070 - val_loss: 0.7726 - val_accuracy: 0.6820
Epoch 108/500
226/226 - 31s - loss: 0.2083 - accuracy: 0.9160 - val_loss: 0.7747 - val_accuracy: 0.6887
Epoch 109/500
226/226 - 31s - loss: 0.2113 - accuracy: 0.9160 - val_loss: 0.7817 - val_accuracy: 0.6797
Epoch 110/500
226/226 - 31s - loss: 0.2017 - accuracy: 0.9178 - val_loss: 0.7854 - val_accuracy: 0.6842
Epoch 111/500
226/226 - 31s - loss: 0.2154 - accuracy: 0.9154 - val_loss: 0.7880 - val_accuracy: 0.6898
Epoch 112/500
226/226 - 31s - loss: 0.1997 - accuracy: 0.9214 - val_loss: 0.7948 - val_accuracy: 0.6887
Epoch 113/500
226/226 - 31s - loss: 0.2009 - accuracy: 0.9197 - val_loss: 0.7994 - val_accuracy: 0.6842
Epoch 114/500
226/226 - 31s - loss: 0.1964 - accuracy: 0.9234 - val_loss: 0.8020 - val_accuracy: 0.6876
Epoch 115/500
226/226 - 31s - loss: 0.1971 - accuracy: 0.9208 - val_loss: 0.8065 - val_accuracy: 0.6809
Epoch 116/500
226/226 - 31s - loss: 0.1927 - accuracy: 0.9222 - val_loss: 0.8098 - val_accuracy: 0.6853
Epoch 117/500
226/226 - 31s - loss: 0.1924 - accuracy: 0.9265 - val_loss: 0.8144 - val_accuracy: 0.6876
Epoch 118/500
226/226 - 31s - loss: 0.2034 - accuracy: 0.9214 - val_loss: 0.8192 - val_accuracy: 0.6853
Epoch 119/500
226/226 - 31s - loss: 0.1916 - accuracy: 0.9203 - val_loss: 0.8228 - val_accuracy: 0.6865
Epoch 120/500
226/226 - 31s - loss: 0.1810 - accuracy: 0.9262 - val_loss: 0.8284 - val_accuracy: 0.6809
Epoch 121/500
226/226 - 31s - loss: 0.1810 - accuracy: 0.9279 - val_loss: 0.8330 - val_accuracy: 0.6820
Epoch 122/500
226/226 - 31s - loss: 0.1879 - accuracy: 0.9251 - val_loss: 0.8378 - val_accuracy: 0.6820
Epoch 123/500
226/226 - 31s - loss: 0.1850 - accuracy: 0.9269 - val_loss: 0.8418 - val_accuracy: 0.6775
Epoch 124/500
226/226 - 31s - loss: 0.1738 - accuracy: 0.9319 - val_loss: 0.8472 - val_accuracy: 0.6775
Epoch 125/500
226/226 - 31s - loss: 0.1800 - accuracy: 0.9275 - val_loss: 0.8463 - val_accuracy: 0.6820
Epoch 126/500
226/226 - 31s - loss: 0.1620 - accuracy: 0.9374 - val_loss: 0.8524 - val_accuracy: 0.6786
Epoch 127/500
226/226 - 31s - loss: 0.1691 - accuracy: 0.9349 - val_loss: 0.8579 - val_accuracy: 0.6831
Epoch 128/500
226/226 - 31s - loss: 0.1719 - accuracy: 0.9338 - val_loss: 0.8619 - val_accuracy: 0.6797
Epoch 129/500
226/226 - 31s - loss: 0.1659 - accuracy: 0.9371 - val_loss: 0.8657 - val_accuracy: 0.6786
Epoch 130/500
226/226 - 31s - loss: 0.1694 - accuracy: 0.9334 - val_loss: 0.8680 - val_accuracy: 0.6820
Epoch 131/500
226/226 - 31s - loss: 0.1675 - accuracy: 0.9345 - val_loss: 0.8724 - val_accuracy: 0.6797
========================================
save_weights
h5_weights/X5628FC.pp/onehot_embedding_cnn_one_branch.h5
========================================

end time >>> Mon Oct  4 09:07:10 2021

end time >>> Mon Oct  4 09:07:10 2021

end time >>> Mon Oct  4 09:07:10 2021

end time >>> Mon Oct  4 09:07:10 2021

end time >>> Mon Oct  4 09:07:10 2021












args.model = onehot_embedding_cnn_one_branch
time used = 4071.2713537216187


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 09:07:12 2021

begin time >>> Mon Oct  4 09:07:12 2021

begin time >>> Mon Oct  4 09:07:12 2021

begin time >>> Mon Oct  4 09:07:12 2021

begin time >>> Mon Oct  4 09:07:12 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_two_branch
args.type = train
args.name = X5628FC.pp
args.length = 10001
===========================


-> h5_weights/X5628FC.pp folder already exist. pass.
-> result/X5628FC.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/X5628FC.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/X5628FC.pp/onehot_embedding_dense folder already exist. pass.
-> result/X5628FC.pp/onehot_dense folder already exist. pass.
-> result/X5628FC.pp/onehot_resnet18 folder already exist. pass.
-> result/X5628FC.pp/onehot_resnet34 folder already exist. pass.
-> result/X5628FC.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/X5628FC.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/X5628FC.pp/embedding_dense folder already exist. pass.
-> result/X5628FC.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/X5628FC.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
X5628FC.pp
########################################

########################################
model_name
onehot_embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
226/226 - 31s - loss: 0.8509 - accuracy: 0.5070 - val_loss: 0.6916 - val_accuracy: 0.5342
Epoch 2/500
226/226 - 31s - loss: 0.8486 - accuracy: 0.5070 - val_loss: 0.6916 - val_accuracy: 0.5252
Epoch 3/500
226/226 - 31s - loss: 0.8354 - accuracy: 0.5084 - val_loss: 0.6941 - val_accuracy: 0.5465
Epoch 4/500
226/226 - 31s - loss: 0.8142 - accuracy: 0.5312 - val_loss: 0.6957 - val_accuracy: 0.5454
Epoch 5/500
226/226 - 31s - loss: 0.8162 - accuracy: 0.5172 - val_loss: 0.6934 - val_accuracy: 0.5566
Epoch 6/500
226/226 - 30s - loss: 0.7972 - accuracy: 0.5440 - val_loss: 0.6915 - val_accuracy: 0.5465
Epoch 7/500
226/226 - 31s - loss: 0.7967 - accuracy: 0.5388 - val_loss: 0.6872 - val_accuracy: 0.5610
Epoch 8/500
226/226 - 31s - loss: 0.7765 - accuracy: 0.5459 - val_loss: 0.6853 - val_accuracy: 0.5655
Epoch 9/500
226/226 - 31s - loss: 0.7890 - accuracy: 0.5377 - val_loss: 0.6834 - val_accuracy: 0.5711
Epoch 10/500
226/226 - 30s - loss: 0.7687 - accuracy: 0.5600 - val_loss: 0.6799 - val_accuracy: 0.5733
Epoch 11/500
226/226 - 31s - loss: 0.7677 - accuracy: 0.5575 - val_loss: 0.6769 - val_accuracy: 0.5778
Epoch 12/500
226/226 - 30s - loss: 0.7521 - accuracy: 0.5664 - val_loss: 0.6750 - val_accuracy: 0.5823
Epoch 13/500
226/226 - 30s - loss: 0.7542 - accuracy: 0.5664 - val_loss: 0.6729 - val_accuracy: 0.5857
Epoch 14/500
226/226 - 30s - loss: 0.7457 - accuracy: 0.5665 - val_loss: 0.6708 - val_accuracy: 0.6013
Epoch 15/500
226/226 - 30s - loss: 0.7387 - accuracy: 0.5675 - val_loss: 0.6672 - val_accuracy: 0.6047
Epoch 16/500
226/226 - 30s - loss: 0.7352 - accuracy: 0.5766 - val_loss: 0.6661 - val_accuracy: 0.6036
Epoch 17/500
226/226 - 30s - loss: 0.7252 - accuracy: 0.5901 - val_loss: 0.6646 - val_accuracy: 0.6114
Epoch 18/500
226/226 - 31s - loss: 0.7251 - accuracy: 0.5908 - val_loss: 0.6603 - val_accuracy: 0.6170
Epoch 19/500
226/226 - 30s - loss: 0.7103 - accuracy: 0.5967 - val_loss: 0.6585 - val_accuracy: 0.6193
Epoch 20/500
226/226 - 30s - loss: 0.6975 - accuracy: 0.6097 - val_loss: 0.6581 - val_accuracy: 0.6159
Epoch 21/500
226/226 - 30s - loss: 0.6937 - accuracy: 0.6054 - val_loss: 0.6551 - val_accuracy: 0.6282
Epoch 22/500
226/226 - 30s - loss: 0.6844 - accuracy: 0.6222 - val_loss: 0.6521 - val_accuracy: 0.6338
Epoch 23/500
226/226 - 30s - loss: 0.6900 - accuracy: 0.6223 - val_loss: 0.6510 - val_accuracy: 0.6338
Epoch 24/500
226/226 - 30s - loss: 0.6872 - accuracy: 0.6183 - val_loss: 0.6496 - val_accuracy: 0.6383
Epoch 25/500
226/226 - 30s - loss: 0.6669 - accuracy: 0.6330 - val_loss: 0.6464 - val_accuracy: 0.6383
Epoch 26/500
226/226 - 30s - loss: 0.6675 - accuracy: 0.6334 - val_loss: 0.6446 - val_accuracy: 0.6417
Epoch 27/500
226/226 - 30s - loss: 0.6577 - accuracy: 0.6366 - val_loss: 0.6425 - val_accuracy: 0.6394
Epoch 28/500
226/226 - 30s - loss: 0.6451 - accuracy: 0.6457 - val_loss: 0.6398 - val_accuracy: 0.6439
Epoch 29/500
226/226 - 31s - loss: 0.6343 - accuracy: 0.6589 - val_loss: 0.6386 - val_accuracy: 0.6473
Epoch 30/500
226/226 - 30s - loss: 0.6375 - accuracy: 0.6550 - val_loss: 0.6363 - val_accuracy: 0.6450
Epoch 31/500
226/226 - 30s - loss: 0.6224 - accuracy: 0.6661 - val_loss: 0.6353 - val_accuracy: 0.6439
Epoch 32/500
226/226 - 30s - loss: 0.6215 - accuracy: 0.6701 - val_loss: 0.6341 - val_accuracy: 0.6394
Epoch 33/500
226/226 - 30s - loss: 0.6158 - accuracy: 0.6722 - val_loss: 0.6314 - val_accuracy: 0.6417
Epoch 34/500
226/226 - 30s - loss: 0.6018 - accuracy: 0.6802 - val_loss: 0.6287 - val_accuracy: 0.6450
Epoch 35/500
226/226 - 30s - loss: 0.5895 - accuracy: 0.6933 - val_loss: 0.6277 - val_accuracy: 0.6473
Epoch 36/500
226/226 - 30s - loss: 0.5717 - accuracy: 0.7068 - val_loss: 0.6260 - val_accuracy: 0.6450
Epoch 37/500
226/226 - 30s - loss: 0.5829 - accuracy: 0.6983 - val_loss: 0.6254 - val_accuracy: 0.6428
Epoch 38/500
226/226 - 30s - loss: 0.5554 - accuracy: 0.7163 - val_loss: 0.6239 - val_accuracy: 0.6517
Epoch 39/500
226/226 - 30s - loss: 0.5456 - accuracy: 0.7206 - val_loss: 0.6244 - val_accuracy: 0.6540
Epoch 40/500
226/226 - 31s - loss: 0.5480 - accuracy: 0.7286 - val_loss: 0.6224 - val_accuracy: 0.6573
Epoch 41/500
226/226 - 30s - loss: 0.5346 - accuracy: 0.7375 - val_loss: 0.6236 - val_accuracy: 0.6540
Epoch 42/500
226/226 - 30s - loss: 0.5337 - accuracy: 0.7329 - val_loss: 0.6213 - val_accuracy: 0.6529
Epoch 43/500
226/226 - 30s - loss: 0.5279 - accuracy: 0.7303 - val_loss: 0.6225 - val_accuracy: 0.6529
Epoch 44/500
226/226 - 30s - loss: 0.5211 - accuracy: 0.7396 - val_loss: 0.6212 - val_accuracy: 0.6573
Epoch 45/500
226/226 - 30s - loss: 0.5168 - accuracy: 0.7418 - val_loss: 0.6183 - val_accuracy: 0.6562
Epoch 46/500
226/226 - 30s - loss: 0.5021 - accuracy: 0.7523 - val_loss: 0.6182 - val_accuracy: 0.6629
Epoch 47/500
226/226 - 30s - loss: 0.4977 - accuracy: 0.7552 - val_loss: 0.6179 - val_accuracy: 0.6641
Epoch 48/500
226/226 - 30s - loss: 0.4797 - accuracy: 0.7732 - val_loss: 0.6197 - val_accuracy: 0.6573
Epoch 49/500
226/226 - 30s - loss: 0.4847 - accuracy: 0.7703 - val_loss: 0.6202 - val_accuracy: 0.6551
Epoch 50/500
226/226 - 30s - loss: 0.4778 - accuracy: 0.7667 - val_loss: 0.6209 - val_accuracy: 0.6573
Epoch 51/500
226/226 - 30s - loss: 0.4632 - accuracy: 0.7875 - val_loss: 0.6208 - val_accuracy: 0.6573
Epoch 52/500
226/226 - 30s - loss: 0.4472 - accuracy: 0.7918 - val_loss: 0.6208 - val_accuracy: 0.6607
Epoch 53/500
226/226 - 30s - loss: 0.4467 - accuracy: 0.7943 - val_loss: 0.6221 - val_accuracy: 0.6607
Epoch 54/500
226/226 - 30s - loss: 0.4462 - accuracy: 0.7897 - val_loss: 0.6242 - val_accuracy: 0.6529
Epoch 55/500
226/226 - 30s - loss: 0.4360 - accuracy: 0.7944 - val_loss: 0.6256 - val_accuracy: 0.6551
Epoch 56/500
226/226 - 30s - loss: 0.4138 - accuracy: 0.8099 - val_loss: 0.6251 - val_accuracy: 0.6562
Epoch 57/500
226/226 - 30s - loss: 0.4217 - accuracy: 0.8081 - val_loss: 0.6261 - val_accuracy: 0.6629
Epoch 58/500
226/226 - 30s - loss: 0.4030 - accuracy: 0.8191 - val_loss: 0.6266 - val_accuracy: 0.6663
Epoch 59/500
226/226 - 30s - loss: 0.3858 - accuracy: 0.8280 - val_loss: 0.6288 - val_accuracy: 0.6607
Epoch 60/500
226/226 - 30s - loss: 0.3982 - accuracy: 0.8258 - val_loss: 0.6310 - val_accuracy: 0.6596
Epoch 61/500
226/226 - 30s - loss: 0.3836 - accuracy: 0.8221 - val_loss: 0.6352 - val_accuracy: 0.6585
Epoch 62/500
226/226 - 30s - loss: 0.3685 - accuracy: 0.8375 - val_loss: 0.6357 - val_accuracy: 0.6585
Epoch 63/500
226/226 - 30s - loss: 0.3696 - accuracy: 0.8351 - val_loss: 0.6341 - val_accuracy: 0.6629
Epoch 64/500
226/226 - 30s - loss: 0.3590 - accuracy: 0.8418 - val_loss: 0.6389 - val_accuracy: 0.6607
Epoch 65/500
226/226 - 30s - loss: 0.3484 - accuracy: 0.8429 - val_loss: 0.6423 - val_accuracy: 0.6551
Epoch 66/500
226/226 - 30s - loss: 0.3587 - accuracy: 0.8411 - val_loss: 0.6423 - val_accuracy: 0.6607
Epoch 67/500
226/226 - 30s - loss: 0.3454 - accuracy: 0.8490 - val_loss: 0.6486 - val_accuracy: 0.6607
Epoch 68/500
226/226 - 30s - loss: 0.3313 - accuracy: 0.8595 - val_loss: 0.6525 - val_accuracy: 0.6607
Epoch 69/500
226/226 - 30s - loss: 0.3333 - accuracy: 0.8537 - val_loss: 0.6570 - val_accuracy: 0.6596
Epoch 70/500
226/226 - 30s - loss: 0.3328 - accuracy: 0.8542 - val_loss: 0.6592 - val_accuracy: 0.6596
Epoch 71/500
226/226 - 30s - loss: 0.3246 - accuracy: 0.8584 - val_loss: 0.6635 - val_accuracy: 0.6551
Epoch 72/500
226/226 - 30s - loss: 0.3215 - accuracy: 0.8593 - val_loss: 0.6663 - val_accuracy: 0.6551
Epoch 73/500
226/226 - 30s - loss: 0.3031 - accuracy: 0.8766 - val_loss: 0.6677 - val_accuracy: 0.6596
Epoch 74/500
226/226 - 30s - loss: 0.3061 - accuracy: 0.8718 - val_loss: 0.6722 - val_accuracy: 0.6585
Epoch 75/500
226/226 - 31s - loss: 0.2945 - accuracy: 0.8765 - val_loss: 0.6763 - val_accuracy: 0.6607
Epoch 76/500
226/226 - 30s - loss: 0.2995 - accuracy: 0.8714 - val_loss: 0.6787 - val_accuracy: 0.6618
Epoch 77/500
226/226 - 30s - loss: 0.2838 - accuracy: 0.8798 - val_loss: 0.6801 - val_accuracy: 0.6641
Epoch 78/500
226/226 - 30s - loss: 0.2896 - accuracy: 0.8791 - val_loss: 0.6866 - val_accuracy: 0.6629
========================================
save_weights
h5_weights/X5628FC.pp/onehot_embedding_cnn_two_branch.h5
========================================

end time >>> Mon Oct  4 09:47:08 2021

end time >>> Mon Oct  4 09:47:08 2021

end time >>> Mon Oct  4 09:47:08 2021

end time >>> Mon Oct  4 09:47:08 2021

end time >>> Mon Oct  4 09:47:08 2021












args.model = onehot_embedding_cnn_two_branch
time used = 2396.1350758075714


