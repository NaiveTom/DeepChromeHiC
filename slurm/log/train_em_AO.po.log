************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sat Oct  2 21:49:04 2021

begin time >>> Sat Oct  2 21:49:04 2021

begin time >>> Sat Oct  2 21:49:04 2021

begin time >>> Sat Oct  2 21:49:04 2021

begin time >>> Sat Oct  2 21:49:04 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_dense
args.type = train
args.name = AO.po
args.length = 10001
===========================


-> h5_weights/AO.po folder already exist. pass.
-> result/AO.po/onehot_cnn_one_branch folder already exist. pass.
-> result/AO.po/onehot_cnn_two_branch folder already exist. pass.
-> result/AO.po/onehot_embedding_dense folder already exist. pass.
-> result/AO.po/onehot_dense folder already exist. pass.
-> result/AO.po/onehot_resnet18 folder already exist. pass.
-> result/AO.po/onehot_resnet34 folder already exist. pass.
-> result/AO.po/embedding_cnn_one_branch folder already exist. pass.
-> result/AO.po/embedding_cnn_two_branch folder already exist. pass.
-> result/AO.po/embedding_dense folder already exist. pass.
-> result/AO.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/AO.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
AO.po
########################################

########################################
model_name
embedding_dense
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 626, 64)      409664      concatenate[0][0]                
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, 38, 64)       0           conv1d[0][0]                     
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 38, 64)       256         max_pooling1d[0][0]              
__________________________________________________________________________________________________
flatten (Flatten)               (None, 2432)         0           batch_normalization[0][0]        
__________________________________________________________________________________________________
dropout (Dropout)               (None, 2432)         0           flatten[0][0]                    
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          1245696     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation (Activation)         (None, 512)          0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation[0][0]                 
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 512)          0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_1[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 512)          2048        dense_2[0][0]                    
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 512)          0           activation_2[0][0]               
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            513         dropout_3[0][0]                  
==================================================================================================
Total params: 3,006,985
Trainable params: 3,003,785
Non-trainable params: 3,200
__________________________________________________________________________________________________
Epoch 1/500
162/162 - 21s - loss: 0.8918 - accuracy: 0.4997 - val_loss: 0.6938 - val_accuracy: 0.5000
Epoch 2/500
162/162 - 21s - loss: 0.8643 - accuracy: 0.5072 - val_loss: 0.7042 - val_accuracy: 0.4891
Epoch 3/500
162/162 - 21s - loss: 0.8676 - accuracy: 0.5057 - val_loss: 0.7059 - val_accuracy: 0.4984
Epoch 4/500
162/162 - 21s - loss: 0.8801 - accuracy: 0.4974 - val_loss: 0.7067 - val_accuracy: 0.4984
Epoch 5/500
162/162 - 21s - loss: 0.8712 - accuracy: 0.4999 - val_loss: 0.7058 - val_accuracy: 0.5016
Epoch 6/500
162/162 - 21s - loss: 0.8737 - accuracy: 0.4993 - val_loss: 0.7050 - val_accuracy: 0.5047
Epoch 7/500
162/162 - 21s - loss: 0.8790 - accuracy: 0.4972 - val_loss: 0.7040 - val_accuracy: 0.5031
Epoch 8/500
162/162 - 21s - loss: 0.8595 - accuracy: 0.5071 - val_loss: 0.7031 - val_accuracy: 0.4984
Epoch 9/500
162/162 - 21s - loss: 0.8679 - accuracy: 0.5043 - val_loss: 0.7032 - val_accuracy: 0.5016
Epoch 10/500
162/162 - 21s - loss: 0.8658 - accuracy: 0.5055 - val_loss: 0.7029 - val_accuracy: 0.4984
Epoch 11/500
162/162 - 21s - loss: 0.8692 - accuracy: 0.4972 - val_loss: 0.7025 - val_accuracy: 0.5000
Epoch 12/500
162/162 - 21s - loss: 0.8517 - accuracy: 0.5086 - val_loss: 0.7020 - val_accuracy: 0.5016
Epoch 13/500
162/162 - 21s - loss: 0.8614 - accuracy: 0.5018 - val_loss: 0.7018 - val_accuracy: 0.4938
Epoch 14/500
162/162 - 21s - loss: 0.8668 - accuracy: 0.4986 - val_loss: 0.7015 - val_accuracy: 0.5047
Epoch 15/500
162/162 - 21s - loss: 0.8443 - accuracy: 0.5183 - val_loss: 0.7015 - val_accuracy: 0.5000
Epoch 16/500
162/162 - 21s - loss: 0.8609 - accuracy: 0.5127 - val_loss: 0.7016 - val_accuracy: 0.5063
Epoch 17/500
162/162 - 21s - loss: 0.8411 - accuracy: 0.5098 - val_loss: 0.7013 - val_accuracy: 0.5016
Epoch 18/500
162/162 - 21s - loss: 0.8475 - accuracy: 0.5217 - val_loss: 0.7010 - val_accuracy: 0.5047
Epoch 19/500
162/162 - 21s - loss: 0.8435 - accuracy: 0.5088 - val_loss: 0.7010 - val_accuracy: 0.4969
Epoch 20/500
162/162 - 21s - loss: 0.8437 - accuracy: 0.5125 - val_loss: 0.7004 - val_accuracy: 0.4984
Epoch 21/500
162/162 - 21s - loss: 0.8509 - accuracy: 0.5117 - val_loss: 0.7008 - val_accuracy: 0.4969
Epoch 22/500
162/162 - 21s - loss: 0.8449 - accuracy: 0.5082 - val_loss: 0.7005 - val_accuracy: 0.4938
Epoch 23/500
162/162 - 21s - loss: 0.8448 - accuracy: 0.5127 - val_loss: 0.7006 - val_accuracy: 0.4953
Epoch 24/500
162/162 - 21s - loss: 0.8255 - accuracy: 0.5269 - val_loss: 0.7013 - val_accuracy: 0.4984
Epoch 25/500
162/162 - 21s - loss: 0.8415 - accuracy: 0.5113 - val_loss: 0.7012 - val_accuracy: 0.5031
Epoch 26/500
162/162 - 21s - loss: 0.8438 - accuracy: 0.5049 - val_loss: 0.7005 - val_accuracy: 0.4984
Epoch 27/500
162/162 - 21s - loss: 0.8406 - accuracy: 0.4995 - val_loss: 0.7004 - val_accuracy: 0.4922
Epoch 28/500
162/162 - 21s - loss: 0.8246 - accuracy: 0.5231 - val_loss: 0.7006 - val_accuracy: 0.4984
Epoch 29/500
162/162 - 21s - loss: 0.8191 - accuracy: 0.5271 - val_loss: 0.7005 - val_accuracy: 0.5000
Epoch 30/500
162/162 - 21s - loss: 0.8366 - accuracy: 0.5136 - val_loss: 0.7004 - val_accuracy: 0.5063
Epoch 31/500
162/162 - 21s - loss: 0.8259 - accuracy: 0.5171 - val_loss: 0.7005 - val_accuracy: 0.5063
Epoch 32/500
162/162 - 21s - loss: 0.8213 - accuracy: 0.5246 - val_loss: 0.7008 - val_accuracy: 0.5031
Epoch 33/500
162/162 - 21s - loss: 0.8237 - accuracy: 0.5136 - val_loss: 0.7003 - val_accuracy: 0.5063
Epoch 34/500
162/162 - 21s - loss: 0.8273 - accuracy: 0.5169 - val_loss: 0.7001 - val_accuracy: 0.5016
Epoch 35/500
162/162 - 21s - loss: 0.8292 - accuracy: 0.5105 - val_loss: 0.7007 - val_accuracy: 0.5031
Epoch 36/500
162/162 - 21s - loss: 0.8323 - accuracy: 0.5134 - val_loss: 0.6996 - val_accuracy: 0.4984
========================================
save_weights
h5_weights/AO.po/embedding_dense.h5
========================================

end time >>> Sat Oct  2 22:01:47 2021

end time >>> Sat Oct  2 22:01:47 2021

end time >>> Sat Oct  2 22:01:47 2021

end time >>> Sat Oct  2 22:01:47 2021

end time >>> Sat Oct  2 22:01:47 2021












args.model = embedding_dense
time used = 762.0967378616333


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sat Oct  2 22:01:48 2021

begin time >>> Sat Oct  2 22:01:48 2021

begin time >>> Sat Oct  2 22:01:48 2021

begin time >>> Sat Oct  2 22:01:48 2021

begin time >>> Sat Oct  2 22:01:48 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_cnn_one_branch
args.type = train
args.name = AO.po
args.length = 10001
===========================


-> h5_weights/AO.po folder already exist. pass.
-> result/AO.po/onehot_cnn_one_branch folder already exist. pass.
-> result/AO.po/onehot_cnn_two_branch folder already exist. pass.
-> result/AO.po/onehot_embedding_dense folder already exist. pass.
-> result/AO.po/onehot_dense folder already exist. pass.
-> result/AO.po/onehot_resnet18 folder already exist. pass.
-> result/AO.po/onehot_resnet34 folder already exist. pass.
-> result/AO.po/embedding_cnn_one_branch folder already exist. pass.
-> result/AO.po/embedding_cnn_two_branch folder already exist. pass.
-> result/AO.po/embedding_dense folder already exist. pass.
-> result/AO.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/AO.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
AO.po
########################################

########################################
model_name
embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
162/162 - 22s - loss: 0.9662 - accuracy: 0.5043 - val_loss: 0.7494 - val_accuracy: 0.4844
Epoch 2/500
162/162 - 22s - loss: 0.9639 - accuracy: 0.4989 - val_loss: 0.7533 - val_accuracy: 0.4844
Epoch 3/500
162/162 - 22s - loss: 0.9076 - accuracy: 0.5072 - val_loss: 0.7443 - val_accuracy: 0.4906
Epoch 4/500
162/162 - 22s - loss: 0.9070 - accuracy: 0.5109 - val_loss: 0.7388 - val_accuracy: 0.4922
Epoch 5/500
162/162 - 22s - loss: 0.8955 - accuracy: 0.5076 - val_loss: 0.7320 - val_accuracy: 0.4969
Epoch 6/500
162/162 - 22s - loss: 0.8758 - accuracy: 0.5107 - val_loss: 0.7283 - val_accuracy: 0.4953
Epoch 7/500
162/162 - 22s - loss: 0.8825 - accuracy: 0.5128 - val_loss: 0.7201 - val_accuracy: 0.5016
Epoch 8/500
162/162 - 22s - loss: 0.8503 - accuracy: 0.5198 - val_loss: 0.7174 - val_accuracy: 0.5000
Epoch 9/500
162/162 - 22s - loss: 0.8329 - accuracy: 0.5364 - val_loss: 0.7133 - val_accuracy: 0.5063
Epoch 10/500
162/162 - 22s - loss: 0.8392 - accuracy: 0.5271 - val_loss: 0.7105 - val_accuracy: 0.5156
Epoch 11/500
162/162 - 22s - loss: 0.8233 - accuracy: 0.5306 - val_loss: 0.7078 - val_accuracy: 0.5125
Epoch 12/500
162/162 - 22s - loss: 0.8241 - accuracy: 0.5341 - val_loss: 0.7057 - val_accuracy: 0.5203
Epoch 13/500
162/162 - 22s - loss: 0.8154 - accuracy: 0.5356 - val_loss: 0.7037 - val_accuracy: 0.5094
Epoch 14/500
162/162 - 22s - loss: 0.8114 - accuracy: 0.5445 - val_loss: 0.7024 - val_accuracy: 0.5156
Epoch 15/500
162/162 - 22s - loss: 0.7870 - accuracy: 0.5609 - val_loss: 0.7014 - val_accuracy: 0.5125
Epoch 16/500
162/162 - 22s - loss: 0.7842 - accuracy: 0.5555 - val_loss: 0.6986 - val_accuracy: 0.5203
Epoch 17/500
162/162 - 22s - loss: 0.7884 - accuracy: 0.5551 - val_loss: 0.6968 - val_accuracy: 0.5312
Epoch 18/500
162/162 - 22s - loss: 0.7861 - accuracy: 0.5596 - val_loss: 0.6961 - val_accuracy: 0.5375
Epoch 19/500
162/162 - 22s - loss: 0.7796 - accuracy: 0.5631 - val_loss: 0.6944 - val_accuracy: 0.5422
Epoch 20/500
162/162 - 22s - loss: 0.7955 - accuracy: 0.5559 - val_loss: 0.6936 - val_accuracy: 0.5422
Epoch 21/500
162/162 - 22s - loss: 0.7788 - accuracy: 0.5582 - val_loss: 0.6929 - val_accuracy: 0.5453
Epoch 22/500
162/162 - 22s - loss: 0.7612 - accuracy: 0.5602 - val_loss: 0.6921 - val_accuracy: 0.5562
Epoch 23/500
162/162 - 22s - loss: 0.7543 - accuracy: 0.5768 - val_loss: 0.6905 - val_accuracy: 0.5641
Epoch 24/500
162/162 - 22s - loss: 0.7625 - accuracy: 0.5685 - val_loss: 0.6904 - val_accuracy: 0.5625
Epoch 25/500
162/162 - 22s - loss: 0.7645 - accuracy: 0.5681 - val_loss: 0.6902 - val_accuracy: 0.5625
Epoch 26/500
162/162 - 22s - loss: 0.7625 - accuracy: 0.5658 - val_loss: 0.6894 - val_accuracy: 0.5656
Epoch 27/500
162/162 - 22s - loss: 0.7422 - accuracy: 0.5857 - val_loss: 0.6881 - val_accuracy: 0.5750
Epoch 28/500
162/162 - 22s - loss: 0.7462 - accuracy: 0.5833 - val_loss: 0.6877 - val_accuracy: 0.5734
Epoch 29/500
162/162 - 22s - loss: 0.7422 - accuracy: 0.5806 - val_loss: 0.6871 - val_accuracy: 0.5766
Epoch 30/500
162/162 - 22s - loss: 0.7364 - accuracy: 0.5886 - val_loss: 0.6861 - val_accuracy: 0.5766
Epoch 31/500
162/162 - 22s - loss: 0.7207 - accuracy: 0.5893 - val_loss: 0.6875 - val_accuracy: 0.5703
Epoch 32/500
162/162 - 22s - loss: 0.7231 - accuracy: 0.5990 - val_loss: 0.6876 - val_accuracy: 0.5688
Epoch 33/500
162/162 - 22s - loss: 0.7233 - accuracy: 0.5891 - val_loss: 0.6857 - val_accuracy: 0.5734
Epoch 34/500
162/162 - 22s - loss: 0.7181 - accuracy: 0.5961 - val_loss: 0.6859 - val_accuracy: 0.5750
Epoch 35/500
162/162 - 22s - loss: 0.7107 - accuracy: 0.6077 - val_loss: 0.6847 - val_accuracy: 0.5750
Epoch 36/500
162/162 - 22s - loss: 0.7213 - accuracy: 0.5988 - val_loss: 0.6847 - val_accuracy: 0.5781
Epoch 37/500
162/162 - 22s - loss: 0.7044 - accuracy: 0.6150 - val_loss: 0.6852 - val_accuracy: 0.5719
Epoch 38/500
162/162 - 22s - loss: 0.6928 - accuracy: 0.6137 - val_loss: 0.6843 - val_accuracy: 0.5703
Epoch 39/500
162/162 - 22s - loss: 0.7023 - accuracy: 0.6088 - val_loss: 0.6846 - val_accuracy: 0.5703
Epoch 40/500
162/162 - 22s - loss: 0.7091 - accuracy: 0.6059 - val_loss: 0.6845 - val_accuracy: 0.5719
Epoch 41/500
162/162 - 22s - loss: 0.6881 - accuracy: 0.6268 - val_loss: 0.6848 - val_accuracy: 0.5766
Epoch 42/500
162/162 - 22s - loss: 0.6858 - accuracy: 0.6235 - val_loss: 0.6838 - val_accuracy: 0.5797
Epoch 43/500
162/162 - 22s - loss: 0.6688 - accuracy: 0.6226 - val_loss: 0.6836 - val_accuracy: 0.5750
Epoch 44/500
162/162 - 22s - loss: 0.6742 - accuracy: 0.6264 - val_loss: 0.6832 - val_accuracy: 0.5750
Epoch 45/500
162/162 - 22s - loss: 0.6720 - accuracy: 0.6340 - val_loss: 0.6841 - val_accuracy: 0.5781
Epoch 46/500
162/162 - 22s - loss: 0.6535 - accuracy: 0.6438 - val_loss: 0.6832 - val_accuracy: 0.5828
Epoch 47/500
162/162 - 22s - loss: 0.6538 - accuracy: 0.6436 - val_loss: 0.6835 - val_accuracy: 0.5797
Epoch 48/500
162/162 - 22s - loss: 0.6497 - accuracy: 0.6481 - val_loss: 0.6841 - val_accuracy: 0.5797
Epoch 49/500
162/162 - 22s - loss: 0.6493 - accuracy: 0.6539 - val_loss: 0.6841 - val_accuracy: 0.5797
Epoch 50/500
162/162 - 22s - loss: 0.6440 - accuracy: 0.6564 - val_loss: 0.6860 - val_accuracy: 0.5766
Epoch 51/500
162/162 - 22s - loss: 0.6289 - accuracy: 0.6620 - val_loss: 0.6850 - val_accuracy: 0.5797
Epoch 52/500
162/162 - 22s - loss: 0.6287 - accuracy: 0.6674 - val_loss: 0.6859 - val_accuracy: 0.5766
Epoch 53/500
162/162 - 22s - loss: 0.6324 - accuracy: 0.6548 - val_loss: 0.6854 - val_accuracy: 0.5766
Epoch 54/500
162/162 - 22s - loss: 0.6300 - accuracy: 0.6633 - val_loss: 0.6863 - val_accuracy: 0.5828
Epoch 55/500
162/162 - 22s - loss: 0.6337 - accuracy: 0.6695 - val_loss: 0.6858 - val_accuracy: 0.5844
Epoch 56/500
162/162 - 22s - loss: 0.6291 - accuracy: 0.6732 - val_loss: 0.6859 - val_accuracy: 0.5797
Epoch 57/500
162/162 - 22s - loss: 0.6078 - accuracy: 0.6809 - val_loss: 0.6870 - val_accuracy: 0.5797
Epoch 58/500
162/162 - 22s - loss: 0.6056 - accuracy: 0.6882 - val_loss: 0.6879 - val_accuracy: 0.5828
Epoch 59/500
162/162 - 22s - loss: 0.5914 - accuracy: 0.6853 - val_loss: 0.6864 - val_accuracy: 0.5734
Epoch 60/500
162/162 - 22s - loss: 0.5936 - accuracy: 0.6931 - val_loss: 0.6871 - val_accuracy: 0.5813
Epoch 61/500
162/162 - 22s - loss: 0.5755 - accuracy: 0.7048 - val_loss: 0.6878 - val_accuracy: 0.5828
Epoch 62/500
162/162 - 22s - loss: 0.5800 - accuracy: 0.6971 - val_loss: 0.6882 - val_accuracy: 0.5844
Epoch 63/500
162/162 - 22s - loss: 0.5687 - accuracy: 0.7039 - val_loss: 0.6889 - val_accuracy: 0.5813
Epoch 64/500
162/162 - 22s - loss: 0.5722 - accuracy: 0.7072 - val_loss: 0.6885 - val_accuracy: 0.5875
Epoch 65/500
162/162 - 22s - loss: 0.5717 - accuracy: 0.7153 - val_loss: 0.6892 - val_accuracy: 0.5875
Epoch 66/500
162/162 - 22s - loss: 0.5563 - accuracy: 0.7182 - val_loss: 0.6905 - val_accuracy: 0.5844
Epoch 67/500
162/162 - 22s - loss: 0.5393 - accuracy: 0.7228 - val_loss: 0.6907 - val_accuracy: 0.5906
Epoch 68/500
162/162 - 22s - loss: 0.5594 - accuracy: 0.7184 - val_loss: 0.6913 - val_accuracy: 0.5922
Epoch 69/500
162/162 - 22s - loss: 0.5331 - accuracy: 0.7340 - val_loss: 0.6922 - val_accuracy: 0.5906
Epoch 70/500
162/162 - 22s - loss: 0.5346 - accuracy: 0.7303 - val_loss: 0.6933 - val_accuracy: 0.5875
Epoch 71/500
162/162 - 22s - loss: 0.5387 - accuracy: 0.7302 - val_loss: 0.6949 - val_accuracy: 0.5875
Epoch 72/500
162/162 - 22s - loss: 0.5309 - accuracy: 0.7332 - val_loss: 0.6957 - val_accuracy: 0.5891
Epoch 73/500
162/162 - 22s - loss: 0.5295 - accuracy: 0.7348 - val_loss: 0.6972 - val_accuracy: 0.5875
Epoch 74/500
162/162 - 22s - loss: 0.5186 - accuracy: 0.7365 - val_loss: 0.6975 - val_accuracy: 0.5813
Epoch 75/500
162/162 - 22s - loss: 0.5042 - accuracy: 0.7543 - val_loss: 0.6980 - val_accuracy: 0.5938
Epoch 76/500
162/162 - 22s - loss: 0.5012 - accuracy: 0.7508 - val_loss: 0.6987 - val_accuracy: 0.5906
Epoch 77/500
162/162 - 22s - loss: 0.5014 - accuracy: 0.7504 - val_loss: 0.7014 - val_accuracy: 0.5875
Epoch 78/500
162/162 - 22s - loss: 0.4900 - accuracy: 0.7659 - val_loss: 0.7020 - val_accuracy: 0.5875
Epoch 79/500
162/162 - 22s - loss: 0.4989 - accuracy: 0.7566 - val_loss: 0.7033 - val_accuracy: 0.5875
Epoch 80/500
162/162 - 22s - loss: 0.4923 - accuracy: 0.7537 - val_loss: 0.7051 - val_accuracy: 0.5875
Epoch 81/500
162/162 - 22s - loss: 0.4904 - accuracy: 0.7582 - val_loss: 0.7060 - val_accuracy: 0.5875
Epoch 82/500
162/162 - 22s - loss: 0.4673 - accuracy: 0.7750 - val_loss: 0.7073 - val_accuracy: 0.5891
Epoch 83/500
162/162 - 22s - loss: 0.4579 - accuracy: 0.7835 - val_loss: 0.7081 - val_accuracy: 0.5938
Epoch 84/500
162/162 - 22s - loss: 0.4609 - accuracy: 0.7833 - val_loss: 0.7102 - val_accuracy: 0.5875
Epoch 85/500
162/162 - 22s - loss: 0.4639 - accuracy: 0.7804 - val_loss: 0.7127 - val_accuracy: 0.5828
Epoch 86/500
162/162 - 22s - loss: 0.4460 - accuracy: 0.7929 - val_loss: 0.7122 - val_accuracy: 0.5844
Epoch 87/500
162/162 - 22s - loss: 0.4443 - accuracy: 0.7825 - val_loss: 0.7139 - val_accuracy: 0.5844
Epoch 88/500
162/162 - 22s - loss: 0.4424 - accuracy: 0.7931 - val_loss: 0.7146 - val_accuracy: 0.5828
Epoch 89/500
162/162 - 22s - loss: 0.4320 - accuracy: 0.7939 - val_loss: 0.7189 - val_accuracy: 0.5734
Epoch 90/500
162/162 - 22s - loss: 0.4312 - accuracy: 0.7978 - val_loss: 0.7192 - val_accuracy: 0.5828
Epoch 91/500
162/162 - 22s - loss: 0.4226 - accuracy: 0.8059 - val_loss: 0.7211 - val_accuracy: 0.5813
Epoch 92/500
162/162 - 22s - loss: 0.4199 - accuracy: 0.8080 - val_loss: 0.7232 - val_accuracy: 0.5781
Epoch 93/500
162/162 - 22s - loss: 0.4102 - accuracy: 0.8090 - val_loss: 0.7230 - val_accuracy: 0.5813
Epoch 94/500
162/162 - 22s - loss: 0.4109 - accuracy: 0.8117 - val_loss: 0.7250 - val_accuracy: 0.5797
Epoch 95/500
162/162 - 22s - loss: 0.3919 - accuracy: 0.8227 - val_loss: 0.7271 - val_accuracy: 0.5797
========================================
save_weights
h5_weights/AO.po/embedding_cnn_one_branch.h5
========================================

end time >>> Sat Oct  2 22:36:32 2021

end time >>> Sat Oct  2 22:36:32 2021

end time >>> Sat Oct  2 22:36:32 2021

end time >>> Sat Oct  2 22:36:32 2021

end time >>> Sat Oct  2 22:36:32 2021












args.model = embedding_cnn_one_branch
time used = 2083.8729503154755


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sat Oct  2 22:36:33 2021

begin time >>> Sat Oct  2 22:36:33 2021

begin time >>> Sat Oct  2 22:36:33 2021

begin time >>> Sat Oct  2 22:36:33 2021

begin time >>> Sat Oct  2 22:36:33 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_cnn_two_branch
args.type = train
args.name = AO.po
args.length = 10001
===========================


-> h5_weights/AO.po folder already exist. pass.
-> result/AO.po/onehot_cnn_one_branch folder already exist. pass.
-> result/AO.po/onehot_cnn_two_branch folder already exist. pass.
-> result/AO.po/onehot_embedding_dense folder already exist. pass.
-> result/AO.po/onehot_dense folder already exist. pass.
-> result/AO.po/onehot_resnet18 folder already exist. pass.
-> result/AO.po/onehot_resnet34 folder already exist. pass.
-> result/AO.po/embedding_cnn_one_branch folder already exist. pass.
-> result/AO.po/embedding_cnn_two_branch folder already exist. pass.
-> result/AO.po/embedding_dense folder already exist. pass.
-> result/AO.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/AO.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
AO.po
########################################

########################################
model_name
embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
162/162 - 22s - loss: 0.8525 - accuracy: 0.5094 - val_loss: 0.6911 - val_accuracy: 0.5172
Epoch 2/500
162/162 - 22s - loss: 0.8436 - accuracy: 0.5119 - val_loss: 0.6947 - val_accuracy: 0.5437
Epoch 3/500
162/162 - 22s - loss: 0.8686 - accuracy: 0.4904 - val_loss: 0.6973 - val_accuracy: 0.5391
Epoch 4/500
162/162 - 22s - loss: 0.8397 - accuracy: 0.5200 - val_loss: 0.6979 - val_accuracy: 0.5250
Epoch 5/500
162/162 - 22s - loss: 0.8309 - accuracy: 0.5136 - val_loss: 0.6975 - val_accuracy: 0.5359
Epoch 6/500
162/162 - 22s - loss: 0.8467 - accuracy: 0.5067 - val_loss: 0.6958 - val_accuracy: 0.5422
Epoch 7/500
162/162 - 22s - loss: 0.8350 - accuracy: 0.5142 - val_loss: 0.6948 - val_accuracy: 0.5375
Epoch 8/500
162/162 - 22s - loss: 0.8206 - accuracy: 0.5248 - val_loss: 0.6940 - val_accuracy: 0.5437
Epoch 9/500
162/162 - 22s - loss: 0.7996 - accuracy: 0.5422 - val_loss: 0.6932 - val_accuracy: 0.5453
Epoch 10/500
162/162 - 22s - loss: 0.8194 - accuracy: 0.5190 - val_loss: 0.6921 - val_accuracy: 0.5453
Epoch 11/500
162/162 - 22s - loss: 0.8097 - accuracy: 0.5225 - val_loss: 0.6919 - val_accuracy: 0.5406
Epoch 12/500
162/162 - 22s - loss: 0.7808 - accuracy: 0.5532 - val_loss: 0.6904 - val_accuracy: 0.5437
Epoch 13/500
162/162 - 22s - loss: 0.7893 - accuracy: 0.5378 - val_loss: 0.6901 - val_accuracy: 0.5500
Epoch 14/500
162/162 - 22s - loss: 0.7937 - accuracy: 0.5395 - val_loss: 0.6893 - val_accuracy: 0.5437
Epoch 15/500
162/162 - 22s - loss: 0.7665 - accuracy: 0.5586 - val_loss: 0.6885 - val_accuracy: 0.5453
Epoch 16/500
162/162 - 22s - loss: 0.7845 - accuracy: 0.5476 - val_loss: 0.6880 - val_accuracy: 0.5500
Epoch 17/500
162/162 - 22s - loss: 0.7833 - accuracy: 0.5422 - val_loss: 0.6869 - val_accuracy: 0.5547
Epoch 18/500
162/162 - 22s - loss: 0.7660 - accuracy: 0.5580 - val_loss: 0.6863 - val_accuracy: 0.5484
Epoch 19/500
162/162 - 22s - loss: 0.7687 - accuracy: 0.5486 - val_loss: 0.6859 - val_accuracy: 0.5422
Epoch 20/500
162/162 - 22s - loss: 0.7632 - accuracy: 0.5592 - val_loss: 0.6853 - val_accuracy: 0.5500
Epoch 21/500
162/162 - 22s - loss: 0.7560 - accuracy: 0.5600 - val_loss: 0.6842 - val_accuracy: 0.5547
Epoch 22/500
162/162 - 22s - loss: 0.7506 - accuracy: 0.5737 - val_loss: 0.6841 - val_accuracy: 0.5625
Epoch 23/500
162/162 - 22s - loss: 0.7567 - accuracy: 0.5627 - val_loss: 0.6835 - val_accuracy: 0.5562
Epoch 24/500
162/162 - 22s - loss: 0.7396 - accuracy: 0.5739 - val_loss: 0.6836 - val_accuracy: 0.5609
Epoch 25/500
162/162 - 22s - loss: 0.7465 - accuracy: 0.5712 - val_loss: 0.6833 - val_accuracy: 0.5594
Epoch 26/500
162/162 - 22s - loss: 0.7300 - accuracy: 0.5841 - val_loss: 0.6829 - val_accuracy: 0.5641
Epoch 27/500
162/162 - 22s - loss: 0.7238 - accuracy: 0.5890 - val_loss: 0.6821 - val_accuracy: 0.5578
Epoch 28/500
162/162 - 22s - loss: 0.7291 - accuracy: 0.5911 - val_loss: 0.6813 - val_accuracy: 0.5562
Epoch 29/500
162/162 - 22s - loss: 0.7301 - accuracy: 0.5917 - val_loss: 0.6814 - val_accuracy: 0.5578
Epoch 30/500
162/162 - 22s - loss: 0.7118 - accuracy: 0.5936 - val_loss: 0.6806 - val_accuracy: 0.5625
Epoch 31/500
162/162 - 22s - loss: 0.7042 - accuracy: 0.6059 - val_loss: 0.6802 - val_accuracy: 0.5641
Epoch 32/500
162/162 - 22s - loss: 0.7205 - accuracy: 0.5872 - val_loss: 0.6799 - val_accuracy: 0.5609
Epoch 33/500
162/162 - 22s - loss: 0.7099 - accuracy: 0.5986 - val_loss: 0.6795 - val_accuracy: 0.5656
Epoch 34/500
162/162 - 22s - loss: 0.7132 - accuracy: 0.5980 - val_loss: 0.6790 - val_accuracy: 0.5672
Epoch 35/500
162/162 - 22s - loss: 0.7138 - accuracy: 0.5947 - val_loss: 0.6781 - val_accuracy: 0.5688
Epoch 36/500
162/162 - 22s - loss: 0.6812 - accuracy: 0.6185 - val_loss: 0.6776 - val_accuracy: 0.5703
Epoch 37/500
162/162 - 22s - loss: 0.6877 - accuracy: 0.6137 - val_loss: 0.6773 - val_accuracy: 0.5688
Epoch 38/500
162/162 - 22s - loss: 0.7013 - accuracy: 0.6083 - val_loss: 0.6774 - val_accuracy: 0.5750
Epoch 39/500
162/162 - 22s - loss: 0.6825 - accuracy: 0.6152 - val_loss: 0.6768 - val_accuracy: 0.5703
Epoch 40/500
162/162 - 21s - loss: 0.6877 - accuracy: 0.6218 - val_loss: 0.6768 - val_accuracy: 0.5688
Epoch 41/500
162/162 - 22s - loss: 0.6748 - accuracy: 0.6299 - val_loss: 0.6760 - val_accuracy: 0.5609
Epoch 42/500
162/162 - 21s - loss: 0.6736 - accuracy: 0.6299 - val_loss: 0.6755 - val_accuracy: 0.5672
Epoch 43/500
162/162 - 21s - loss: 0.6585 - accuracy: 0.6415 - val_loss: 0.6750 - val_accuracy: 0.5766
Epoch 44/500
162/162 - 21s - loss: 0.6557 - accuracy: 0.6417 - val_loss: 0.6746 - val_accuracy: 0.5672
Epoch 45/500
162/162 - 21s - loss: 0.6397 - accuracy: 0.6506 - val_loss: 0.6739 - val_accuracy: 0.5719
Epoch 46/500
162/162 - 21s - loss: 0.6477 - accuracy: 0.6492 - val_loss: 0.6739 - val_accuracy: 0.5797
Epoch 47/500
162/162 - 21s - loss: 0.6328 - accuracy: 0.6519 - val_loss: 0.6733 - val_accuracy: 0.5781
Epoch 48/500
162/162 - 22s - loss: 0.6435 - accuracy: 0.6479 - val_loss: 0.6731 - val_accuracy: 0.5781
Epoch 49/500
162/162 - 21s - loss: 0.6375 - accuracy: 0.6625 - val_loss: 0.6735 - val_accuracy: 0.5734
Epoch 50/500
162/162 - 22s - loss: 0.6275 - accuracy: 0.6627 - val_loss: 0.6730 - val_accuracy: 0.5781
Epoch 51/500
162/162 - 22s - loss: 0.6189 - accuracy: 0.6722 - val_loss: 0.6731 - val_accuracy: 0.5719
Epoch 52/500
162/162 - 22s - loss: 0.6369 - accuracy: 0.6596 - val_loss: 0.6726 - val_accuracy: 0.5844
Epoch 53/500
162/162 - 22s - loss: 0.6189 - accuracy: 0.6670 - val_loss: 0.6718 - val_accuracy: 0.5859
Epoch 54/500
162/162 - 22s - loss: 0.6089 - accuracy: 0.6674 - val_loss: 0.6716 - val_accuracy: 0.5922
Epoch 55/500
162/162 - 22s - loss: 0.6035 - accuracy: 0.6786 - val_loss: 0.6717 - val_accuracy: 0.5844
Epoch 56/500
162/162 - 22s - loss: 0.5937 - accuracy: 0.6850 - val_loss: 0.6714 - val_accuracy: 0.5844
Epoch 57/500
162/162 - 22s - loss: 0.6025 - accuracy: 0.6784 - val_loss: 0.6715 - val_accuracy: 0.5813
Epoch 58/500
162/162 - 21s - loss: 0.5847 - accuracy: 0.6994 - val_loss: 0.6716 - val_accuracy: 0.5875
Epoch 59/500
162/162 - 21s - loss: 0.5920 - accuracy: 0.6944 - val_loss: 0.6713 - val_accuracy: 0.5875
Epoch 60/500
162/162 - 22s - loss: 0.5731 - accuracy: 0.7020 - val_loss: 0.6721 - val_accuracy: 0.5813
Epoch 61/500
162/162 - 21s - loss: 0.5661 - accuracy: 0.7058 - val_loss: 0.6725 - val_accuracy: 0.5797
Epoch 62/500
162/162 - 22s - loss: 0.5635 - accuracy: 0.7060 - val_loss: 0.6720 - val_accuracy: 0.5859
Epoch 63/500
162/162 - 21s - loss: 0.5441 - accuracy: 0.7240 - val_loss: 0.6716 - val_accuracy: 0.5828
Epoch 64/500
162/162 - 22s - loss: 0.5398 - accuracy: 0.7226 - val_loss: 0.6728 - val_accuracy: 0.5859
Epoch 65/500
162/162 - 21s - loss: 0.5311 - accuracy: 0.7344 - val_loss: 0.6730 - val_accuracy: 0.5844
Epoch 66/500
162/162 - 21s - loss: 0.5319 - accuracy: 0.7346 - val_loss: 0.6736 - val_accuracy: 0.5828
Epoch 67/500
162/162 - 21s - loss: 0.5304 - accuracy: 0.7425 - val_loss: 0.6741 - val_accuracy: 0.5781
Epoch 68/500
162/162 - 21s - loss: 0.5081 - accuracy: 0.7452 - val_loss: 0.6732 - val_accuracy: 0.5859
Epoch 69/500
162/162 - 21s - loss: 0.5207 - accuracy: 0.7473 - val_loss: 0.6745 - val_accuracy: 0.5828
Epoch 70/500
162/162 - 21s - loss: 0.5035 - accuracy: 0.7613 - val_loss: 0.6740 - val_accuracy: 0.5859
Epoch 71/500
162/162 - 21s - loss: 0.5074 - accuracy: 0.7450 - val_loss: 0.6739 - val_accuracy: 0.5844
Epoch 72/500
162/162 - 21s - loss: 0.4769 - accuracy: 0.7686 - val_loss: 0.6748 - val_accuracy: 0.5859
Epoch 73/500
162/162 - 22s - loss: 0.4919 - accuracy: 0.7553 - val_loss: 0.6753 - val_accuracy: 0.5844
Epoch 74/500
162/162 - 21s - loss: 0.4852 - accuracy: 0.7669 - val_loss: 0.6758 - val_accuracy: 0.5875
========================================
save_weights
h5_weights/AO.po/embedding_cnn_two_branch.h5
========================================

end time >>> Sat Oct  2 23:03:30 2021

end time >>> Sat Oct  2 23:03:30 2021

end time >>> Sat Oct  2 23:03:30 2021

end time >>> Sat Oct  2 23:03:30 2021

end time >>> Sat Oct  2 23:03:30 2021












args.model = embedding_cnn_two_branch
time used = 1616.7309403419495


