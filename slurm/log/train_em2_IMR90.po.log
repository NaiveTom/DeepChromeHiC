************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 11:48:07 2021

begin time >>> Sun Oct  3 11:48:07 2021

begin time >>> Sun Oct  3 11:48:07 2021

begin time >>> Sun Oct  3 11:48:07 2021

begin time >>> Sun Oct  3 11:48:07 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_dense
args.type = train
args.name = IMR90.po
args.length = 10001
===========================


-> h5_weights/IMR90.po folder already exist. pass.
-> result/IMR90.po/onehot_cnn_one_branch folder already exist. pass.
-> result/IMR90.po/onehot_cnn_two_branch folder already exist. pass.
-> result/IMR90.po/onehot_embedding_dense folder already exist. pass.
-> result/IMR90.po/onehot_dense folder already exist. pass.
-> result/IMR90.po/onehot_resnet18 folder already exist. pass.
-> result/IMR90.po/onehot_resnet34 folder already exist. pass.
-> result/IMR90.po/embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.po/embedding_cnn_two_branch folder already exist. pass.
-> result/IMR90.po/embedding_dense folder already exist. pass.
-> result/IMR90.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
IMR90.po
########################################

########################################
model_name
onehot_embedding_dense
########################################

Found 13814 images belonging to 2 classes.
Found 1706 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 20002, 5, 1, 8)    32776     
_________________________________________________________________
flatten (Flatten)            (None, 800080)            0         
_________________________________________________________________
batch_normalization (BatchNo (None, 800080)            3200320   
_________________________________________________________________
dense (Dense)                (None, 512)               409641472 
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_3 (Batch (None, 512)               2048      
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_3 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 413,671,754
Trainable params: 412,067,498
Non-trainable params: 1,604,256
_________________________________________________________________
Epoch 1/500
431/431 - 82s - loss: 0.7032 - accuracy: 0.6096 - val_loss: 0.8475 - val_accuracy: 0.5018
Epoch 2/500
431/431 - 84s - loss: 0.5565 - accuracy: 0.7234 - val_loss: 0.9813 - val_accuracy: 0.5737
Epoch 3/500
431/431 - 81s - loss: 0.4345 - accuracy: 0.8053 - val_loss: 1.3250 - val_accuracy: 0.5690
Epoch 4/500
431/431 - 83s - loss: 0.3340 - accuracy: 0.8598 - val_loss: 1.5455 - val_accuracy: 0.5784
Epoch 5/500
431/431 - 81s - loss: 0.2643 - accuracy: 0.8933 - val_loss: 1.7327 - val_accuracy: 0.5908
Epoch 6/500
431/431 - 81s - loss: 0.2135 - accuracy: 0.9147 - val_loss: 1.8895 - val_accuracy: 0.5926
Epoch 7/500
431/431 - 82s - loss: 0.1755 - accuracy: 0.9309 - val_loss: 1.9009 - val_accuracy: 0.6073
Epoch 8/500
431/431 - 84s - loss: 0.1476 - accuracy: 0.9433 - val_loss: 2.0227 - val_accuracy: 0.6120
Epoch 9/500
431/431 - 82s - loss: 0.1253 - accuracy: 0.9498 - val_loss: 2.0341 - val_accuracy: 0.6179
Epoch 10/500
431/431 - 82s - loss: 0.1154 - accuracy: 0.9552 - val_loss: 2.1362 - val_accuracy: 0.6232
Epoch 11/500
431/431 - 82s - loss: 0.1002 - accuracy: 0.9621 - val_loss: 2.0589 - val_accuracy: 0.6279
Epoch 12/500
431/431 - 81s - loss: 0.0957 - accuracy: 0.9653 - val_loss: 2.1685 - val_accuracy: 0.6226
Epoch 13/500
431/431 - 80s - loss: 0.0921 - accuracy: 0.9644 - val_loss: 2.1568 - val_accuracy: 0.6256
Epoch 14/500
431/431 - 82s - loss: 0.0845 - accuracy: 0.9695 - val_loss: 2.1707 - val_accuracy: 0.6321
Epoch 15/500
431/431 - 81s - loss: 0.0769 - accuracy: 0.9699 - val_loss: 2.1503 - val_accuracy: 0.6374
Epoch 16/500
431/431 - 83s - loss: 0.0675 - accuracy: 0.9741 - val_loss: 2.1362 - val_accuracy: 0.6456
Epoch 17/500
431/431 - 85s - loss: 0.0621 - accuracy: 0.9785 - val_loss: 2.2184 - val_accuracy: 0.6350
Epoch 18/500
431/431 - 83s - loss: 0.0550 - accuracy: 0.9804 - val_loss: 2.2513 - val_accuracy: 0.6456
Epoch 19/500
431/431 - 82s - loss: 0.0569 - accuracy: 0.9812 - val_loss: 2.1934 - val_accuracy: 0.6486
Epoch 20/500
431/431 - 79s - loss: 0.0547 - accuracy: 0.9817 - val_loss: 2.1794 - val_accuracy: 0.6421
Epoch 21/500
431/431 - 80s - loss: 0.0490 - accuracy: 0.9819 - val_loss: 2.2276 - val_accuracy: 0.6439
Epoch 22/500
431/431 - 81s - loss: 0.0461 - accuracy: 0.9833 - val_loss: 2.2186 - val_accuracy: 0.6539
Epoch 23/500
431/431 - 80s - loss: 0.0420 - accuracy: 0.9859 - val_loss: 2.2098 - val_accuracy: 0.6515
Epoch 24/500
431/431 - 86s - loss: 0.0414 - accuracy: 0.9855 - val_loss: 2.2871 - val_accuracy: 0.6380
Epoch 25/500
431/431 - 81s - loss: 0.0413 - accuracy: 0.9861 - val_loss: 2.2294 - val_accuracy: 0.6427
Epoch 26/500
431/431 - 81s - loss: 0.0392 - accuracy: 0.9858 - val_loss: 2.2697 - val_accuracy: 0.6415
Epoch 27/500
431/431 - 80s - loss: 0.0409 - accuracy: 0.9845 - val_loss: 2.2518 - val_accuracy: 0.6403
Epoch 28/500
431/431 - 82s - loss: 0.0380 - accuracy: 0.9869 - val_loss: 2.1822 - val_accuracy: 0.6562
Epoch 29/500
431/431 - 82s - loss: 0.0412 - accuracy: 0.9864 - val_loss: 2.1548 - val_accuracy: 0.6539
Epoch 30/500
431/431 - 82s - loss: 0.0333 - accuracy: 0.9886 - val_loss: 2.1772 - val_accuracy: 0.6633
Epoch 31/500
431/431 - 79s - loss: 0.0390 - accuracy: 0.9864 - val_loss: 2.1536 - val_accuracy: 0.6633
Epoch 32/500
431/431 - 80s - loss: 0.0309 - accuracy: 0.9902 - val_loss: 2.1622 - val_accuracy: 0.6574
Epoch 33/500
431/431 - 80s - loss: 0.0284 - accuracy: 0.9907 - val_loss: 2.1670 - val_accuracy: 0.6627
Epoch 34/500
431/431 - 80s - loss: 0.0298 - accuracy: 0.9901 - val_loss: 2.2005 - val_accuracy: 0.6592
Epoch 35/500
431/431 - 81s - loss: 0.0296 - accuracy: 0.9900 - val_loss: 2.1173 - val_accuracy: 0.6657
Epoch 36/500
431/431 - 80s - loss: 0.0228 - accuracy: 0.9922 - val_loss: 2.1879 - val_accuracy: 0.6639
Epoch 37/500
431/431 - 80s - loss: 0.0211 - accuracy: 0.9922 - val_loss: 2.2423 - val_accuracy: 0.6645
Epoch 38/500
431/431 - 82s - loss: 0.0215 - accuracy: 0.9927 - val_loss: 2.3740 - val_accuracy: 0.6574
Epoch 39/500
431/431 - 82s - loss: 0.0208 - accuracy: 0.9936 - val_loss: 2.2315 - val_accuracy: 0.6680
Epoch 40/500
431/431 - 81s - loss: 0.0273 - accuracy: 0.9914 - val_loss: 2.1580 - val_accuracy: 0.6722
Epoch 41/500
431/431 - 80s - loss: 0.0199 - accuracy: 0.9941 - val_loss: 2.2078 - val_accuracy: 0.6716
Epoch 42/500
431/431 - 82s - loss: 0.0248 - accuracy: 0.9928 - val_loss: 2.1475 - val_accuracy: 0.6728
Epoch 43/500
431/431 - 80s - loss: 0.0234 - accuracy: 0.9927 - val_loss: 2.2131 - val_accuracy: 0.6675
Epoch 44/500
431/431 - 80s - loss: 0.0198 - accuracy: 0.9931 - val_loss: 2.1940 - val_accuracy: 0.6633
Epoch 45/500
431/431 - 80s - loss: 0.0206 - accuracy: 0.9932 - val_loss: 2.3121 - val_accuracy: 0.6698
Epoch 46/500
431/431 - 81s - loss: 0.0211 - accuracy: 0.9933 - val_loss: 2.2963 - val_accuracy: 0.6745
Epoch 47/500
431/431 - 81s - loss: 0.0175 - accuracy: 0.9948 - val_loss: 2.2706 - val_accuracy: 0.6745
Epoch 48/500
431/431 - 82s - loss: 0.0137 - accuracy: 0.9956 - val_loss: 2.3699 - val_accuracy: 0.6792
Epoch 49/500
431/431 - 81s - loss: 0.0164 - accuracy: 0.9941 - val_loss: 2.3825 - val_accuracy: 0.6739
Epoch 50/500
431/431 - 82s - loss: 0.0171 - accuracy: 0.9945 - val_loss: 2.3987 - val_accuracy: 0.6710
Epoch 51/500
431/431 - 84s - loss: 0.0184 - accuracy: 0.9946 - val_loss: 2.2555 - val_accuracy: 0.6739
Epoch 52/500
431/431 - 80s - loss: 0.0186 - accuracy: 0.9946 - val_loss: 2.3493 - val_accuracy: 0.6745
Epoch 53/500
431/431 - 81s - loss: 0.0188 - accuracy: 0.9940 - val_loss: 2.3666 - val_accuracy: 0.6751
Epoch 54/500
431/431 - 80s - loss: 0.0147 - accuracy: 0.9952 - val_loss: 2.2987 - val_accuracy: 0.6763
Epoch 55/500
431/431 - 81s - loss: 0.0145 - accuracy: 0.9953 - val_loss: 2.3723 - val_accuracy: 0.6757
Epoch 56/500
431/431 - 81s - loss: 0.0164 - accuracy: 0.9954 - val_loss: 2.3346 - val_accuracy: 0.6733
Epoch 57/500
431/431 - 85s - loss: 0.0113 - accuracy: 0.9967 - val_loss: 2.3469 - val_accuracy: 0.6757
Epoch 58/500
431/431 - 81s - loss: 0.0139 - accuracy: 0.9955 - val_loss: 2.3030 - val_accuracy: 0.6804
Epoch 59/500
431/431 - 83s - loss: 0.0144 - accuracy: 0.9958 - val_loss: 2.2989 - val_accuracy: 0.6869
Epoch 60/500
431/431 - 80s - loss: 0.0166 - accuracy: 0.9954 - val_loss: 2.2468 - val_accuracy: 0.6804
Epoch 61/500
431/431 - 82s - loss: 0.0126 - accuracy: 0.9963 - val_loss: 2.2353 - val_accuracy: 0.6810
Epoch 62/500
431/431 - 83s - loss: 0.0161 - accuracy: 0.9951 - val_loss: 2.2673 - val_accuracy: 0.6804
Epoch 63/500
431/431 - 80s - loss: 0.0137 - accuracy: 0.9965 - val_loss: 2.3148 - val_accuracy: 0.6781
Epoch 64/500
431/431 - 80s - loss: 0.0147 - accuracy: 0.9954 - val_loss: 2.3248 - val_accuracy: 0.6804
Epoch 65/500
431/431 - 80s - loss: 0.0106 - accuracy: 0.9971 - val_loss: 2.3244 - val_accuracy: 0.6851
Epoch 66/500
431/431 - 80s - loss: 0.0135 - accuracy: 0.9964 - val_loss: 2.2658 - val_accuracy: 0.6769
Epoch 67/500
431/431 - 82s - loss: 0.0114 - accuracy: 0.9972 - val_loss: 2.2829 - val_accuracy: 0.6869
Epoch 68/500
431/431 - 83s - loss: 0.0114 - accuracy: 0.9963 - val_loss: 2.2455 - val_accuracy: 0.6816
Epoch 69/500
431/431 - 84s - loss: 0.0098 - accuracy: 0.9967 - val_loss: 2.3660 - val_accuracy: 0.6834
========================================
save_weights
h5_weights/IMR90.po/onehot_embedding_dense.h5
========================================

end time >>> Sun Oct  3 13:22:14 2021

end time >>> Sun Oct  3 13:22:14 2021

end time >>> Sun Oct  3 13:22:14 2021

end time >>> Sun Oct  3 13:22:14 2021

end time >>> Sun Oct  3 13:22:14 2021












args.model = onehot_embedding_dense
time used = 5647.599619150162


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 13:22:15 2021

begin time >>> Sun Oct  3 13:22:15 2021

begin time >>> Sun Oct  3 13:22:15 2021

begin time >>> Sun Oct  3 13:22:15 2021

begin time >>> Sun Oct  3 13:22:15 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_one_branch
args.type = train
args.name = IMR90.po
args.length = 10001
===========================


-> h5_weights/IMR90.po folder already exist. pass.
-> result/IMR90.po/onehot_cnn_one_branch folder already exist. pass.
-> result/IMR90.po/onehot_cnn_two_branch folder already exist. pass.
-> result/IMR90.po/onehot_embedding_dense folder already exist. pass.
-> result/IMR90.po/onehot_dense folder already exist. pass.
-> result/IMR90.po/onehot_resnet18 folder already exist. pass.
-> result/IMR90.po/onehot_resnet34 folder already exist. pass.
-> result/IMR90.po/embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.po/embedding_cnn_two_branch folder already exist. pass.
-> result/IMR90.po/embedding_dense folder already exist. pass.
-> result/IMR90.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
IMR90.po
########################################

########################################
model_name
onehot_embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
432/432 - 61s - loss: 0.8858 - accuracy: 0.4917 - val_loss: 0.6933 - val_accuracy: 0.5146
Epoch 2/500
432/432 - 60s - loss: 0.8631 - accuracy: 0.5068 - val_loss: 0.6987 - val_accuracy: 0.5252
Epoch 3/500
432/432 - 60s - loss: 0.8422 - accuracy: 0.5128 - val_loss: 0.6960 - val_accuracy: 0.5287
Epoch 4/500
432/432 - 60s - loss: 0.8416 - accuracy: 0.5233 - val_loss: 0.6920 - val_accuracy: 0.5351
Epoch 5/500
432/432 - 60s - loss: 0.8263 - accuracy: 0.5316 - val_loss: 0.6888 - val_accuracy: 0.5433
Epoch 6/500
432/432 - 60s - loss: 0.8193 - accuracy: 0.5290 - val_loss: 0.6860 - val_accuracy: 0.5468
Epoch 7/500
432/432 - 60s - loss: 0.8134 - accuracy: 0.5306 - val_loss: 0.6828 - val_accuracy: 0.5591
Epoch 8/500
432/432 - 60s - loss: 0.8110 - accuracy: 0.5364 - val_loss: 0.6802 - val_accuracy: 0.5638
Epoch 9/500
432/432 - 60s - loss: 0.8069 - accuracy: 0.5410 - val_loss: 0.6778 - val_accuracy: 0.5679
Epoch 10/500
432/432 - 60s - loss: 0.7943 - accuracy: 0.5403 - val_loss: 0.6753 - val_accuracy: 0.5732
Epoch 11/500
432/432 - 60s - loss: 0.7875 - accuracy: 0.5454 - val_loss: 0.6730 - val_accuracy: 0.5808
Epoch 12/500
432/432 - 60s - loss: 0.7798 - accuracy: 0.5505 - val_loss: 0.6710 - val_accuracy: 0.5884
Epoch 13/500
432/432 - 60s - loss: 0.7721 - accuracy: 0.5610 - val_loss: 0.6688 - val_accuracy: 0.5902
Epoch 14/500
432/432 - 60s - loss: 0.7554 - accuracy: 0.5685 - val_loss: 0.6663 - val_accuracy: 0.5913
Epoch 15/500
432/432 - 60s - loss: 0.7567 - accuracy: 0.5665 - val_loss: 0.6641 - val_accuracy: 0.6001
Epoch 16/500
432/432 - 60s - loss: 0.7429 - accuracy: 0.5768 - val_loss: 0.6619 - val_accuracy: 0.5989
Epoch 17/500
432/432 - 60s - loss: 0.7425 - accuracy: 0.5726 - val_loss: 0.6604 - val_accuracy: 0.5978
Epoch 18/500
432/432 - 60s - loss: 0.7442 - accuracy: 0.5791 - val_loss: 0.6578 - val_accuracy: 0.6048
Epoch 19/500
432/432 - 60s - loss: 0.7370 - accuracy: 0.5816 - val_loss: 0.6558 - val_accuracy: 0.6124
Epoch 20/500
432/432 - 60s - loss: 0.7307 - accuracy: 0.5831 - val_loss: 0.6538 - val_accuracy: 0.6171
Epoch 21/500
432/432 - 60s - loss: 0.7091 - accuracy: 0.5937 - val_loss: 0.6517 - val_accuracy: 0.6206
Epoch 22/500
432/432 - 60s - loss: 0.7111 - accuracy: 0.5991 - val_loss: 0.6489 - val_accuracy: 0.6200
Epoch 23/500
432/432 - 60s - loss: 0.7036 - accuracy: 0.6016 - val_loss: 0.6475 - val_accuracy: 0.6230
Epoch 24/500
432/432 - 60s - loss: 0.7025 - accuracy: 0.6067 - val_loss: 0.6455 - val_accuracy: 0.6253
Epoch 25/500
432/432 - 60s - loss: 0.6857 - accuracy: 0.6209 - val_loss: 0.6430 - val_accuracy: 0.6276
Epoch 26/500
432/432 - 60s - loss: 0.6779 - accuracy: 0.6281 - val_loss: 0.6414 - val_accuracy: 0.6311
Epoch 27/500
432/432 - 60s - loss: 0.6794 - accuracy: 0.6227 - val_loss: 0.6388 - val_accuracy: 0.6364
Epoch 28/500
432/432 - 60s - loss: 0.6598 - accuracy: 0.6408 - val_loss: 0.6370 - val_accuracy: 0.6317
Epoch 29/500
432/432 - 60s - loss: 0.6690 - accuracy: 0.6306 - val_loss: 0.6345 - val_accuracy: 0.6341
Epoch 30/500
432/432 - 60s - loss: 0.6439 - accuracy: 0.6560 - val_loss: 0.6334 - val_accuracy: 0.6341
Epoch 31/500
432/432 - 60s - loss: 0.6370 - accuracy: 0.6579 - val_loss: 0.6315 - val_accuracy: 0.6376
Epoch 32/500
432/432 - 60s - loss: 0.6464 - accuracy: 0.6530 - val_loss: 0.6291 - val_accuracy: 0.6388
Epoch 33/500
432/432 - 60s - loss: 0.6256 - accuracy: 0.6671 - val_loss: 0.6280 - val_accuracy: 0.6341
Epoch 34/500
432/432 - 60s - loss: 0.6172 - accuracy: 0.6717 - val_loss: 0.6266 - val_accuracy: 0.6370
Epoch 35/500
432/432 - 60s - loss: 0.6065 - accuracy: 0.6799 - val_loss: 0.6243 - val_accuracy: 0.6382
Epoch 36/500
432/432 - 60s - loss: 0.5993 - accuracy: 0.6882 - val_loss: 0.6231 - val_accuracy: 0.6341
Epoch 37/500
432/432 - 60s - loss: 0.5907 - accuracy: 0.6926 - val_loss: 0.6219 - val_accuracy: 0.6329
Epoch 38/500
432/432 - 60s - loss: 0.5849 - accuracy: 0.6990 - val_loss: 0.6210 - val_accuracy: 0.6376
Epoch 39/500
432/432 - 60s - loss: 0.5716 - accuracy: 0.7109 - val_loss: 0.6211 - val_accuracy: 0.6341
Epoch 40/500
432/432 - 60s - loss: 0.5620 - accuracy: 0.7111 - val_loss: 0.6203 - val_accuracy: 0.6364
Epoch 41/500
432/432 - 60s - loss: 0.5535 - accuracy: 0.7208 - val_loss: 0.6198 - val_accuracy: 0.6405
Epoch 42/500
432/432 - 60s - loss: 0.5439 - accuracy: 0.7242 - val_loss: 0.6189 - val_accuracy: 0.6440
Epoch 43/500
432/432 - 60s - loss: 0.5371 - accuracy: 0.7334 - val_loss: 0.6180 - val_accuracy: 0.6470
Epoch 44/500
432/432 - 60s - loss: 0.5271 - accuracy: 0.7374 - val_loss: 0.6179 - val_accuracy: 0.6505
Epoch 45/500
432/432 - 60s - loss: 0.5133 - accuracy: 0.7522 - val_loss: 0.6184 - val_accuracy: 0.6487
Epoch 46/500
432/432 - 60s - loss: 0.5126 - accuracy: 0.7498 - val_loss: 0.6184 - val_accuracy: 0.6522
Epoch 47/500
432/432 - 60s - loss: 0.5049 - accuracy: 0.7514 - val_loss: 0.6190 - val_accuracy: 0.6546
Epoch 48/500
432/432 - 60s - loss: 0.4778 - accuracy: 0.7693 - val_loss: 0.6199 - val_accuracy: 0.6534
Epoch 49/500
432/432 - 60s - loss: 0.4841 - accuracy: 0.7706 - val_loss: 0.6201 - val_accuracy: 0.6528
Epoch 50/500
432/432 - 60s - loss: 0.4720 - accuracy: 0.7738 - val_loss: 0.6223 - val_accuracy: 0.6528
Epoch 51/500
432/432 - 60s - loss: 0.4629 - accuracy: 0.7787 - val_loss: 0.6218 - val_accuracy: 0.6569
Epoch 52/500
432/432 - 60s - loss: 0.4584 - accuracy: 0.7832 - val_loss: 0.6232 - val_accuracy: 0.6587
Epoch 53/500
432/432 - 60s - loss: 0.4463 - accuracy: 0.7915 - val_loss: 0.6246 - val_accuracy: 0.6593
Epoch 54/500
432/432 - 60s - loss: 0.4327 - accuracy: 0.7999 - val_loss: 0.6245 - val_accuracy: 0.6628
Epoch 55/500
432/432 - 60s - loss: 0.4293 - accuracy: 0.7998 - val_loss: 0.6276 - val_accuracy: 0.6616
Epoch 56/500
432/432 - 60s - loss: 0.4199 - accuracy: 0.8021 - val_loss: 0.6273 - val_accuracy: 0.6669
Epoch 57/500
432/432 - 60s - loss: 0.4074 - accuracy: 0.8139 - val_loss: 0.6295 - val_accuracy: 0.6680
Epoch 58/500
432/432 - 60s - loss: 0.4021 - accuracy: 0.8156 - val_loss: 0.6331 - val_accuracy: 0.6657
Epoch 59/500
432/432 - 60s - loss: 0.3929 - accuracy: 0.8247 - val_loss: 0.6366 - val_accuracy: 0.6680
Epoch 60/500
432/432 - 60s - loss: 0.3927 - accuracy: 0.8207 - val_loss: 0.6367 - val_accuracy: 0.6715
Epoch 61/500
432/432 - 60s - loss: 0.3792 - accuracy: 0.8294 - val_loss: 0.6382 - val_accuracy: 0.6721
Epoch 62/500
432/432 - 60s - loss: 0.3679 - accuracy: 0.8336 - val_loss: 0.6402 - val_accuracy: 0.6721
Epoch 63/500
432/432 - 60s - loss: 0.3664 - accuracy: 0.8372 - val_loss: 0.6441 - val_accuracy: 0.6704
Epoch 64/500
432/432 - 60s - loss: 0.3447 - accuracy: 0.8481 - val_loss: 0.6462 - val_accuracy: 0.6727
Epoch 65/500
432/432 - 60s - loss: 0.3531 - accuracy: 0.8469 - val_loss: 0.6483 - val_accuracy: 0.6774
Epoch 66/500
432/432 - 60s - loss: 0.3342 - accuracy: 0.8559 - val_loss: 0.6520 - val_accuracy: 0.6715
Epoch 67/500
432/432 - 60s - loss: 0.3261 - accuracy: 0.8590 - val_loss: 0.6550 - val_accuracy: 0.6762
Epoch 68/500
432/432 - 60s - loss: 0.3300 - accuracy: 0.8542 - val_loss: 0.6588 - val_accuracy: 0.6756
Epoch 69/500
432/432 - 60s - loss: 0.3206 - accuracy: 0.8611 - val_loss: 0.6617 - val_accuracy: 0.6792
Epoch 70/500
432/432 - 60s - loss: 0.3116 - accuracy: 0.8643 - val_loss: 0.6646 - val_accuracy: 0.6774
Epoch 71/500
432/432 - 60s - loss: 0.3073 - accuracy: 0.8676 - val_loss: 0.6684 - val_accuracy: 0.6768
Epoch 72/500
432/432 - 60s - loss: 0.3113 - accuracy: 0.8630 - val_loss: 0.6744 - val_accuracy: 0.6733
Epoch 73/500
432/432 - 60s - loss: 0.2933 - accuracy: 0.8759 - val_loss: 0.6751 - val_accuracy: 0.6768
Epoch 74/500
432/432 - 60s - loss: 0.2870 - accuracy: 0.8776 - val_loss: 0.6802 - val_accuracy: 0.6780
Epoch 75/500
432/432 - 60s - loss: 0.2829 - accuracy: 0.8808 - val_loss: 0.6862 - val_accuracy: 0.6739
Epoch 76/500
432/432 - 60s - loss: 0.2762 - accuracy: 0.8824 - val_loss: 0.6888 - val_accuracy: 0.6768
Epoch 77/500
432/432 - 60s - loss: 0.2685 - accuracy: 0.8871 - val_loss: 0.6936 - val_accuracy: 0.6751
Epoch 78/500
432/432 - 60s - loss: 0.2591 - accuracy: 0.8904 - val_loss: 0.6956 - val_accuracy: 0.6780
Epoch 79/500
432/432 - 60s - loss: 0.2579 - accuracy: 0.8905 - val_loss: 0.6992 - val_accuracy: 0.6780
Epoch 80/500
432/432 - 60s - loss: 0.2510 - accuracy: 0.8981 - val_loss: 0.7044 - val_accuracy: 0.6803
Epoch 81/500
432/432 - 60s - loss: 0.2338 - accuracy: 0.9046 - val_loss: 0.7089 - val_accuracy: 0.6792
Epoch 82/500
432/432 - 60s - loss: 0.2407 - accuracy: 0.8992 - val_loss: 0.7149 - val_accuracy: 0.6803
Epoch 83/500
432/432 - 60s - loss: 0.2259 - accuracy: 0.9072 - val_loss: 0.7193 - val_accuracy: 0.6786
Epoch 84/500
432/432 - 60s - loss: 0.2282 - accuracy: 0.9046 - val_loss: 0.7247 - val_accuracy: 0.6745
Epoch 85/500
432/432 - 60s - loss: 0.2260 - accuracy: 0.9085 - val_loss: 0.7262 - val_accuracy: 0.6827
Epoch 86/500
432/432 - 60s - loss: 0.2115 - accuracy: 0.9149 - val_loss: 0.7320 - val_accuracy: 0.6803
Epoch 87/500
432/432 - 60s - loss: 0.2208 - accuracy: 0.9081 - val_loss: 0.7385 - val_accuracy: 0.6762
Epoch 88/500
432/432 - 60s - loss: 0.2061 - accuracy: 0.9146 - val_loss: 0.7393 - val_accuracy: 0.6774
Epoch 89/500
432/432 - 60s - loss: 0.2031 - accuracy: 0.9196 - val_loss: 0.7485 - val_accuracy: 0.6751
Epoch 90/500
432/432 - 60s - loss: 0.1983 - accuracy: 0.9178 - val_loss: 0.7541 - val_accuracy: 0.6762
Epoch 91/500
432/432 - 60s - loss: 0.1912 - accuracy: 0.9236 - val_loss: 0.7556 - val_accuracy: 0.6792
Epoch 92/500
432/432 - 60s - loss: 0.1856 - accuracy: 0.9245 - val_loss: 0.7614 - val_accuracy: 0.6786
Epoch 93/500
432/432 - 60s - loss: 0.1876 - accuracy: 0.9251 - val_loss: 0.7697 - val_accuracy: 0.6780
Epoch 94/500
432/432 - 60s - loss: 0.1885 - accuracy: 0.9241 - val_loss: 0.7710 - val_accuracy: 0.6827
Epoch 95/500
432/432 - 60s - loss: 0.1778 - accuracy: 0.9286 - val_loss: 0.7775 - val_accuracy: 0.6786
Epoch 96/500
432/432 - 60s - loss: 0.1627 - accuracy: 0.9377 - val_loss: 0.7811 - val_accuracy: 0.6786
Epoch 97/500
432/432 - 60s - loss: 0.1783 - accuracy: 0.9270 - val_loss: 0.7852 - val_accuracy: 0.6821
Epoch 98/500
432/432 - 60s - loss: 0.1597 - accuracy: 0.9359 - val_loss: 0.7914 - val_accuracy: 0.6780
Epoch 99/500
432/432 - 60s - loss: 0.1591 - accuracy: 0.9380 - val_loss: 0.7987 - val_accuracy: 0.6768
Epoch 100/500
432/432 - 60s - loss: 0.1553 - accuracy: 0.9398 - val_loss: 0.7955 - val_accuracy: 0.6821
Epoch 101/500
432/432 - 60s - loss: 0.1636 - accuracy: 0.9335 - val_loss: 0.8026 - val_accuracy: 0.6803
Epoch 102/500
432/432 - 60s - loss: 0.1519 - accuracy: 0.9414 - val_loss: 0.8091 - val_accuracy: 0.6786
Epoch 103/500
432/432 - 60s - loss: 0.1486 - accuracy: 0.9434 - val_loss: 0.8112 - val_accuracy: 0.6792
Epoch 104/500
432/432 - 60s - loss: 0.1481 - accuracy: 0.9430 - val_loss: 0.8168 - val_accuracy: 0.6803
Epoch 105/500
432/432 - 60s - loss: 0.1390 - accuracy: 0.9456 - val_loss: 0.8207 - val_accuracy: 0.6838
Epoch 106/500
432/432 - 60s - loss: 0.1417 - accuracy: 0.9440 - val_loss: 0.8249 - val_accuracy: 0.6850
Epoch 107/500
432/432 - 60s - loss: 0.1353 - accuracy: 0.9508 - val_loss: 0.8288 - val_accuracy: 0.6838
Epoch 108/500
432/432 - 60s - loss: 0.1356 - accuracy: 0.9483 - val_loss: 0.8346 - val_accuracy: 0.6821
Epoch 109/500
432/432 - 60s - loss: 0.1311 - accuracy: 0.9477 - val_loss: 0.8382 - val_accuracy: 0.6850
Epoch 110/500
432/432 - 60s - loss: 0.1223 - accuracy: 0.9527 - val_loss: 0.8479 - val_accuracy: 0.6815
Epoch 111/500
432/432 - 60s - loss: 0.1296 - accuracy: 0.9485 - val_loss: 0.8454 - val_accuracy: 0.6868
Epoch 112/500
432/432 - 60s - loss: 0.1254 - accuracy: 0.9501 - val_loss: 0.8525 - val_accuracy: 0.6856
Epoch 113/500
432/432 - 60s - loss: 0.1200 - accuracy: 0.9526 - val_loss: 0.8571 - val_accuracy: 0.6850
Epoch 114/500
432/432 - 60s - loss: 0.1193 - accuracy: 0.9550 - val_loss: 0.8623 - val_accuracy: 0.6809
Epoch 115/500
432/432 - 60s - loss: 0.1184 - accuracy: 0.9559 - val_loss: 0.8663 - val_accuracy: 0.6833
Epoch 116/500
432/432 - 60s - loss: 0.1158 - accuracy: 0.9566 - val_loss: 0.8702 - val_accuracy: 0.6827
Epoch 117/500
432/432 - 60s - loss: 0.1137 - accuracy: 0.9560 - val_loss: 0.8750 - val_accuracy: 0.6838
Epoch 118/500
432/432 - 60s - loss: 0.1076 - accuracy: 0.9601 - val_loss: 0.8812 - val_accuracy: 0.6797
Epoch 119/500
432/432 - 60s - loss: 0.1073 - accuracy: 0.9600 - val_loss: 0.8838 - val_accuracy: 0.6833
Epoch 120/500
432/432 - 60s - loss: 0.1060 - accuracy: 0.9596 - val_loss: 0.8933 - val_accuracy: 0.6780
Epoch 121/500
432/432 - 60s - loss: 0.1081 - accuracy: 0.9598 - val_loss: 0.8986 - val_accuracy: 0.6803
Epoch 122/500
432/432 - 60s - loss: 0.1017 - accuracy: 0.9617 - val_loss: 0.9058 - val_accuracy: 0.6815
Epoch 123/500
432/432 - 60s - loss: 0.0969 - accuracy: 0.9651 - val_loss: 0.9003 - val_accuracy: 0.6833
Epoch 124/500
432/432 - 60s - loss: 0.0940 - accuracy: 0.9648 - val_loss: 0.9121 - val_accuracy: 0.6809
Epoch 125/500
432/432 - 60s - loss: 0.0952 - accuracy: 0.9653 - val_loss: 0.9139 - val_accuracy: 0.6827
Epoch 126/500
432/432 - 60s - loss: 0.0992 - accuracy: 0.9645 - val_loss: 0.9212 - val_accuracy: 0.6838
Epoch 127/500
432/432 - 60s - loss: 0.0940 - accuracy: 0.9657 - val_loss: 0.9237 - val_accuracy: 0.6821
Epoch 128/500
432/432 - 60s - loss: 0.0879 - accuracy: 0.9677 - val_loss: 0.9279 - val_accuracy: 0.6833
Epoch 129/500
432/432 - 60s - loss: 0.0917 - accuracy: 0.9659 - val_loss: 0.9365 - val_accuracy: 0.6827
Epoch 130/500
432/432 - 60s - loss: 0.0891 - accuracy: 0.9668 - val_loss: 0.9343 - val_accuracy: 0.6827
Epoch 131/500
432/432 - 60s - loss: 0.0804 - accuracy: 0.9720 - val_loss: 0.9363 - val_accuracy: 0.6792
========================================
save_weights
h5_weights/IMR90.po/onehot_embedding_cnn_one_branch.h5
========================================

end time >>> Sun Oct  3 15:34:08 2021

end time >>> Sun Oct  3 15:34:08 2021

end time >>> Sun Oct  3 15:34:08 2021

end time >>> Sun Oct  3 15:34:08 2021

end time >>> Sun Oct  3 15:34:08 2021












args.model = onehot_embedding_cnn_one_branch
time used = 7912.757056951523


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 15:34:09 2021

begin time >>> Sun Oct  3 15:34:09 2021

begin time >>> Sun Oct  3 15:34:09 2021

begin time >>> Sun Oct  3 15:34:09 2021

begin time >>> Sun Oct  3 15:34:09 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_two_branch
args.type = train
args.name = IMR90.po
args.length = 10001
===========================


-> h5_weights/IMR90.po folder already exist. pass.
-> result/IMR90.po/onehot_cnn_one_branch folder already exist. pass.
-> result/IMR90.po/onehot_cnn_two_branch folder already exist. pass.
-> result/IMR90.po/onehot_embedding_dense folder already exist. pass.
-> result/IMR90.po/onehot_dense folder already exist. pass.
-> result/IMR90.po/onehot_resnet18 folder already exist. pass.
-> result/IMR90.po/onehot_resnet34 folder already exist. pass.
-> result/IMR90.po/embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.po/embedding_cnn_two_branch folder already exist. pass.
-> result/IMR90.po/embedding_dense folder already exist. pass.
-> result/IMR90.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
IMR90.po
########################################

########################################
model_name
onehot_embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
432/432 - 61s - loss: 0.8844 - accuracy: 0.5066 - val_loss: 0.7315 - val_accuracy: 0.4900
Epoch 2/500
432/432 - 60s - loss: 0.8693 - accuracy: 0.5066 - val_loss: 0.7088 - val_accuracy: 0.5217
Epoch 3/500
432/432 - 60s - loss: 0.8582 - accuracy: 0.5095 - val_loss: 0.7019 - val_accuracy: 0.5263
Epoch 4/500
432/432 - 60s - loss: 0.8376 - accuracy: 0.5223 - val_loss: 0.6951 - val_accuracy: 0.5381
Epoch 5/500
432/432 - 60s - loss: 0.8342 - accuracy: 0.5164 - val_loss: 0.6913 - val_accuracy: 0.5550
Epoch 6/500
432/432 - 60s - loss: 0.8226 - accuracy: 0.5304 - val_loss: 0.6863 - val_accuracy: 0.5621
Epoch 7/500
432/432 - 60s - loss: 0.8089 - accuracy: 0.5352 - val_loss: 0.6834 - val_accuracy: 0.5679
Epoch 8/500
432/432 - 60s - loss: 0.8106 - accuracy: 0.5346 - val_loss: 0.6794 - val_accuracy: 0.5708
Epoch 9/500
432/432 - 60s - loss: 0.8024 - accuracy: 0.5415 - val_loss: 0.6768 - val_accuracy: 0.5761
Epoch 10/500
432/432 - 60s - loss: 0.7938 - accuracy: 0.5454 - val_loss: 0.6741 - val_accuracy: 0.5767
Epoch 11/500
432/432 - 60s - loss: 0.7814 - accuracy: 0.5468 - val_loss: 0.6716 - val_accuracy: 0.5849
Epoch 12/500
432/432 - 60s - loss: 0.7770 - accuracy: 0.5560 - val_loss: 0.6684 - val_accuracy: 0.5826
Epoch 13/500
432/432 - 60s - loss: 0.7611 - accuracy: 0.5633 - val_loss: 0.6658 - val_accuracy: 0.5837
Epoch 14/500
432/432 - 60s - loss: 0.7598 - accuracy: 0.5654 - val_loss: 0.6636 - val_accuracy: 0.5896
Epoch 15/500
432/432 - 60s - loss: 0.7612 - accuracy: 0.5624 - val_loss: 0.6612 - val_accuracy: 0.5960
Epoch 16/500
432/432 - 60s - loss: 0.7440 - accuracy: 0.5736 - val_loss: 0.6586 - val_accuracy: 0.6007
Epoch 17/500
432/432 - 60s - loss: 0.7364 - accuracy: 0.5849 - val_loss: 0.6571 - val_accuracy: 0.6013
Epoch 18/500
432/432 - 60s - loss: 0.7246 - accuracy: 0.5953 - val_loss: 0.6550 - val_accuracy: 0.6083
Epoch 19/500
432/432 - 60s - loss: 0.7250 - accuracy: 0.5907 - val_loss: 0.6525 - val_accuracy: 0.6048
Epoch 20/500
432/432 - 60s - loss: 0.7144 - accuracy: 0.5975 - val_loss: 0.6496 - val_accuracy: 0.6101
Epoch 21/500
432/432 - 60s - loss: 0.7028 - accuracy: 0.6123 - val_loss: 0.6468 - val_accuracy: 0.6153
Epoch 22/500
432/432 - 60s - loss: 0.6964 - accuracy: 0.6141 - val_loss: 0.6453 - val_accuracy: 0.6218
Epoch 23/500
432/432 - 60s - loss: 0.6963 - accuracy: 0.6133 - val_loss: 0.6428 - val_accuracy: 0.6235
Epoch 24/500
432/432 - 60s - loss: 0.6815 - accuracy: 0.6230 - val_loss: 0.6403 - val_accuracy: 0.6294
Epoch 25/500
432/432 - 60s - loss: 0.6707 - accuracy: 0.6356 - val_loss: 0.6389 - val_accuracy: 0.6265
Epoch 26/500
432/432 - 60s - loss: 0.6641 - accuracy: 0.6414 - val_loss: 0.6364 - val_accuracy: 0.6341
Epoch 27/500
432/432 - 60s - loss: 0.6540 - accuracy: 0.6413 - val_loss: 0.6347 - val_accuracy: 0.6376
Epoch 28/500
432/432 - 60s - loss: 0.6387 - accuracy: 0.6587 - val_loss: 0.6326 - val_accuracy: 0.6358
Epoch 29/500
432/432 - 60s - loss: 0.6342 - accuracy: 0.6627 - val_loss: 0.6303 - val_accuracy: 0.6405
Epoch 30/500
432/432 - 60s - loss: 0.6365 - accuracy: 0.6581 - val_loss: 0.6279 - val_accuracy: 0.6440
Epoch 31/500
432/432 - 60s - loss: 0.6243 - accuracy: 0.6699 - val_loss: 0.6271 - val_accuracy: 0.6505
Epoch 32/500
432/432 - 60s - loss: 0.6175 - accuracy: 0.6740 - val_loss: 0.6253 - val_accuracy: 0.6493
Epoch 33/500
432/432 - 60s - loss: 0.5936 - accuracy: 0.6902 - val_loss: 0.6253 - val_accuracy: 0.6540
Epoch 34/500
432/432 - 60s - loss: 0.5915 - accuracy: 0.6909 - val_loss: 0.6209 - val_accuracy: 0.6511
Epoch 35/500
432/432 - 60s - loss: 0.5892 - accuracy: 0.6987 - val_loss: 0.6195 - val_accuracy: 0.6528
Epoch 36/500
432/432 - 60s - loss: 0.5632 - accuracy: 0.7143 - val_loss: 0.6204 - val_accuracy: 0.6569
Epoch 37/500
432/432 - 60s - loss: 0.5738 - accuracy: 0.7077 - val_loss: 0.6184 - val_accuracy: 0.6563
Epoch 38/500
432/432 - 60s - loss: 0.5564 - accuracy: 0.7174 - val_loss: 0.6172 - val_accuracy: 0.6552
Epoch 39/500
432/432 - 60s - loss: 0.5473 - accuracy: 0.7260 - val_loss: 0.6186 - val_accuracy: 0.6569
Epoch 40/500
432/432 - 60s - loss: 0.5315 - accuracy: 0.7364 - val_loss: 0.6176 - val_accuracy: 0.6575
Epoch 41/500
432/432 - 60s - loss: 0.5267 - accuracy: 0.7414 - val_loss: 0.6162 - val_accuracy: 0.6587
Epoch 42/500
432/432 - 60s - loss: 0.5198 - accuracy: 0.7441 - val_loss: 0.6168 - val_accuracy: 0.6610
Epoch 43/500
432/432 - 60s - loss: 0.5090 - accuracy: 0.7499 - val_loss: 0.6169 - val_accuracy: 0.6628
Epoch 44/500
432/432 - 60s - loss: 0.4972 - accuracy: 0.7589 - val_loss: 0.6171 - val_accuracy: 0.6633
Epoch 45/500
432/432 - 60s - loss: 0.4831 - accuracy: 0.7685 - val_loss: 0.6219 - val_accuracy: 0.6674
Epoch 46/500
432/432 - 60s - loss: 0.4794 - accuracy: 0.7724 - val_loss: 0.6214 - val_accuracy: 0.6674
Epoch 47/500
432/432 - 60s - loss: 0.4597 - accuracy: 0.7822 - val_loss: 0.6218 - val_accuracy: 0.6692
Epoch 48/500
432/432 - 60s - loss: 0.4513 - accuracy: 0.7903 - val_loss: 0.6220 - val_accuracy: 0.6698
Epoch 49/500
432/432 - 60s - loss: 0.4434 - accuracy: 0.7956 - val_loss: 0.6220 - val_accuracy: 0.6715
Epoch 50/500
432/432 - 60s - loss: 0.4336 - accuracy: 0.7953 - val_loss: 0.6245 - val_accuracy: 0.6721
Epoch 51/500
432/432 - 60s - loss: 0.4209 - accuracy: 0.8059 - val_loss: 0.6275 - val_accuracy: 0.6727
Epoch 52/500
432/432 - 60s - loss: 0.4200 - accuracy: 0.8001 - val_loss: 0.6280 - val_accuracy: 0.6739
Epoch 53/500
432/432 - 60s - loss: 0.4115 - accuracy: 0.8133 - val_loss: 0.6355 - val_accuracy: 0.6797
Epoch 54/500
432/432 - 60s - loss: 0.4048 - accuracy: 0.8155 - val_loss: 0.6327 - val_accuracy: 0.6762
Epoch 55/500
432/432 - 60s - loss: 0.3836 - accuracy: 0.8263 - val_loss: 0.6384 - val_accuracy: 0.6762
Epoch 56/500
432/432 - 60s - loss: 0.3866 - accuracy: 0.8253 - val_loss: 0.6414 - val_accuracy: 0.6780
Epoch 57/500
432/432 - 60s - loss: 0.3760 - accuracy: 0.8321 - val_loss: 0.6441 - val_accuracy: 0.6827
Epoch 58/500
432/432 - 60s - loss: 0.3629 - accuracy: 0.8368 - val_loss: 0.6481 - val_accuracy: 0.6844
Epoch 59/500
432/432 - 60s - loss: 0.3504 - accuracy: 0.8448 - val_loss: 0.6486 - val_accuracy: 0.6850
Epoch 60/500
432/432 - 60s - loss: 0.3459 - accuracy: 0.8496 - val_loss: 0.6524 - val_accuracy: 0.6874
Epoch 61/500
432/432 - 60s - loss: 0.3421 - accuracy: 0.8517 - val_loss: 0.6559 - val_accuracy: 0.6891
Epoch 62/500
432/432 - 60s - loss: 0.3331 - accuracy: 0.8534 - val_loss: 0.6577 - val_accuracy: 0.6885
Epoch 63/500
432/432 - 60s - loss: 0.3315 - accuracy: 0.8500 - val_loss: 0.6619 - val_accuracy: 0.6909
Epoch 64/500
432/432 - 60s - loss: 0.3194 - accuracy: 0.8600 - val_loss: 0.6674 - val_accuracy: 0.6932
Epoch 65/500
432/432 - 60s - loss: 0.3076 - accuracy: 0.8656 - val_loss: 0.6715 - val_accuracy: 0.6909
Epoch 66/500
432/432 - 60s - loss: 0.2959 - accuracy: 0.8732 - val_loss: 0.6775 - val_accuracy: 0.6926
Epoch 67/500
432/432 - 60s - loss: 0.2927 - accuracy: 0.8756 - val_loss: 0.6812 - val_accuracy: 0.6909
Epoch 68/500
432/432 - 60s - loss: 0.2871 - accuracy: 0.8803 - val_loss: 0.6821 - val_accuracy: 0.6885
Epoch 69/500
432/432 - 60s - loss: 0.2774 - accuracy: 0.8821 - val_loss: 0.6884 - val_accuracy: 0.6891
Epoch 70/500
432/432 - 60s - loss: 0.2690 - accuracy: 0.8878 - val_loss: 0.6909 - val_accuracy: 0.6856
Epoch 71/500
432/432 - 60s - loss: 0.2615 - accuracy: 0.8879 - val_loss: 0.6960 - val_accuracy: 0.6909
Epoch 72/500
432/432 - 60s - loss: 0.2603 - accuracy: 0.8887 - val_loss: 0.7022 - val_accuracy: 0.6903
Epoch 73/500
432/432 - 60s - loss: 0.2511 - accuracy: 0.8933 - val_loss: 0.7074 - val_accuracy: 0.6879
Epoch 74/500
432/432 - 60s - loss: 0.2489 - accuracy: 0.8965 - val_loss: 0.7103 - val_accuracy: 0.6920
Epoch 75/500
432/432 - 60s - loss: 0.2432 - accuracy: 0.8965 - val_loss: 0.7169 - val_accuracy: 0.6879
Epoch 76/500
432/432 - 60s - loss: 0.2389 - accuracy: 0.9005 - val_loss: 0.7227 - val_accuracy: 0.6862
Epoch 77/500
432/432 - 60s - loss: 0.2318 - accuracy: 0.9075 - val_loss: 0.7305 - val_accuracy: 0.6868
Epoch 78/500
432/432 - 60s - loss: 0.2253 - accuracy: 0.9062 - val_loss: 0.7316 - val_accuracy: 0.6856
Epoch 79/500
432/432 - 60s - loss: 0.2234 - accuracy: 0.9082 - val_loss: 0.7372 - val_accuracy: 0.6844
Epoch 80/500
432/432 - 60s - loss: 0.2161 - accuracy: 0.9134 - val_loss: 0.7437 - val_accuracy: 0.6850
Epoch 81/500
432/432 - 60s - loss: 0.2025 - accuracy: 0.9192 - val_loss: 0.7515 - val_accuracy: 0.6850
Epoch 82/500
432/432 - 60s - loss: 0.1974 - accuracy: 0.9208 - val_loss: 0.7561 - val_accuracy: 0.6821
Epoch 83/500
432/432 - 60s - loss: 0.2007 - accuracy: 0.9198 - val_loss: 0.7569 - val_accuracy: 0.6780
Epoch 84/500
432/432 - 60s - loss: 0.1851 - accuracy: 0.9270 - val_loss: 0.7614 - val_accuracy: 0.6803
========================================
save_weights
h5_weights/IMR90.po/onehot_embedding_cnn_two_branch.h5
========================================

end time >>> Sun Oct  3 16:58:26 2021

end time >>> Sun Oct  3 16:58:26 2021

end time >>> Sun Oct  3 16:58:26 2021

end time >>> Sun Oct  3 16:58:26 2021

end time >>> Sun Oct  3 16:58:26 2021












args.model = onehot_embedding_cnn_two_branch
time used = 5056.256384849548


