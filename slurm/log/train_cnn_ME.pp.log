************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 18:41:52 2021

begin time >>> Sun Oct  3 18:41:52 2021

begin time >>> Sun Oct  3 18:41:52 2021

begin time >>> Sun Oct  3 18:41:52 2021

begin time >>> Sun Oct  3 18:41:52 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_cnn_one_branch
args.type = train
args.name = ME.pp
args.length = 10001
===========================


-> make new folder: h5_weights/ME.pp
-> make new folder: result/ME.pp/onehot_cnn_one_branch
-> make new folder: result/ME.pp/onehot_cnn_two_branch
-> make new folder: result/ME.pp/onehot_embedding_dense
-> make new folder: result/ME.pp/onehot_dense
-> make new folder: result/ME.pp/onehot_resnet18
-> make new folder: result/ME.pp/onehot_resnet34
-> make new folder: result/ME.pp/embedding_cnn_one_branch
-> make new folder: result/ME.pp/embedding_cnn_two_branch
-> make new folder: result/ME.pp/embedding_dense
-> make new folder: result/ME.pp/onehot_embedding_cnn_one_branch
-> make new folder: result/ME.pp/onehot_embedding_cnn_two_branch
########################################
gen_name
ME.pp
########################################

########################################
model_name
onehot_cnn_one_branch
########################################

Found 3436 images belonging to 2 classes.
Found 424 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 5001, 5, 64)       1600      
_________________________________________________________________
batch_normalization (BatchNo (None, 5001, 5, 64)       256       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1251, 5, 64)       98368     
_________________________________________________________________
batch_normalization_1 (Batch (None, 1251, 5, 64)       256       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 313, 5, 64)        98368     
_________________________________________________________________
batch_normalization_2 (Batch (None, 313, 5, 64)        256       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 156, 5, 64)        0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 156, 5, 64)        256       
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 39, 5, 128)        196736    
_________________________________________________________________
batch_normalization_4 (Batch (None, 39, 5, 128)        512       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 10, 5, 128)        393344    
_________________________________________________________________
batch_normalization_5 (Batch (None, 10, 5, 128)        512       
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 3, 5, 128)         393344    
_________________________________________________________________
batch_normalization_6 (Batch (None, 3, 5, 128)         512       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 1, 5, 128)         0         
_________________________________________________________________
flatten (Flatten)            (None, 640)               0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 640)               2560      
_________________________________________________________________
dense (Dense)                (None, 512)               328192    
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
dense_3 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 2,041,410
Trainable params: 2,038,850
Non-trainable params: 2,560
_________________________________________________________________
Epoch 1/500
107/107 - 130s - loss: 0.7815 - accuracy: 0.4985 - val_loss: 0.6912 - val_accuracy: 0.5072
Epoch 2/500
107/107 - 13s - loss: 0.7225 - accuracy: 0.5458 - val_loss: 0.6880 - val_accuracy: 0.5288
Epoch 3/500
107/107 - 13s - loss: 0.6792 - accuracy: 0.5890 - val_loss: 0.6898 - val_accuracy: 0.5409
Epoch 4/500
107/107 - 13s - loss: 0.6210 - accuracy: 0.6428 - val_loss: 0.6909 - val_accuracy: 0.5312
Epoch 5/500
107/107 - 13s - loss: 0.5756 - accuracy: 0.7006 - val_loss: 0.7191 - val_accuracy: 0.5697
Epoch 6/500
107/107 - 13s - loss: 0.4968 - accuracy: 0.7579 - val_loss: 0.9572 - val_accuracy: 0.5048
Epoch 7/500
107/107 - 13s - loss: 0.3822 - accuracy: 0.8381 - val_loss: 1.3290 - val_accuracy: 0.5168
Epoch 8/500
107/107 - 13s - loss: 0.2691 - accuracy: 0.9048 - val_loss: 0.9780 - val_accuracy: 0.5312
Epoch 9/500
107/107 - 13s - loss: 0.1613 - accuracy: 0.9451 - val_loss: 3.3742 - val_accuracy: 0.5000
Epoch 10/500
107/107 - 14s - loss: 0.0984 - accuracy: 0.9733 - val_loss: 2.0651 - val_accuracy: 0.5240
Epoch 11/500
107/107 - 13s - loss: 0.0629 - accuracy: 0.9821 - val_loss: 2.6743 - val_accuracy: 0.5312
Epoch 12/500
107/107 - 14s - loss: 0.0456 - accuracy: 0.9885 - val_loss: 2.8308 - val_accuracy: 0.5072
Epoch 13/500
107/107 - 13s - loss: 0.0367 - accuracy: 0.9891 - val_loss: 2.3926 - val_accuracy: 0.5673
Epoch 14/500
107/107 - 13s - loss: 0.0338 - accuracy: 0.9900 - val_loss: 8.6968 - val_accuracy: 0.5048
Epoch 15/500
107/107 - 13s - loss: 0.0219 - accuracy: 0.9938 - val_loss: 2.6596 - val_accuracy: 0.5337
========================================
save_weights
h5_weights/ME.pp/onehot_cnn_one_branch.h5
========================================

end time >>> Sun Oct  3 18:47:24 2021

end time >>> Sun Oct  3 18:47:24 2021

end time >>> Sun Oct  3 18:47:24 2021

end time >>> Sun Oct  3 18:47:24 2021

end time >>> Sun Oct  3 18:47:24 2021












args.model = onehot_cnn_one_branch
time used = 332.6549746990204


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 18:47:25 2021

begin time >>> Sun Oct  3 18:47:25 2021

begin time >>> Sun Oct  3 18:47:25 2021

begin time >>> Sun Oct  3 18:47:25 2021

begin time >>> Sun Oct  3 18:47:25 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_cnn_two_branch
args.type = train
args.name = ME.pp
args.length = 10001
===========================


-> h5_weights/ME.pp folder already exist. pass.
-> result/ME.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/ME.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/ME.pp/onehot_embedding_dense folder already exist. pass.
-> result/ME.pp/onehot_dense folder already exist. pass.
-> result/ME.pp/onehot_resnet18 folder already exist. pass.
-> result/ME.pp/onehot_resnet34 folder already exist. pass.
-> result/ME.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/ME.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/ME.pp/embedding_dense folder already exist. pass.
-> result/ME.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/ME.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001, 5, 1) 0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001, 5, 1) 0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 2501, 5, 64)  1600        input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 2501, 5, 64)  1600        input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 2501, 5, 64)  256         conv2d[0][0]                     
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 2501, 5, 64)  256         conv2d_6[0][0]                   
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 626, 5, 64)   98368       batch_normalization[0][0]        
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 626, 5, 64)   98368       batch_normalization_8[0][0]      
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 626, 5, 64)   256         conv2d_1[0][0]                   
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 626, 5, 64)   256         conv2d_7[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 157, 5, 64)   98368       batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 157, 5, 64)   98368       batch_normalization_9[0][0]      
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 157, 5, 64)   256         conv2d_2[0][0]                   
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 157, 5, 64)   256         conv2d_8[0][0]                   
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 78, 5, 64)    0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 78, 5, 64)    0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 78, 5, 64)    256         max_pooling2d[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 78, 5, 64)    256         max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 20, 5, 128)   196736      batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 20, 5, 128)   196736      batch_normalization_11[0][0]     
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 20, 5, 128)   512         conv2d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 20, 5, 128)   512         conv2d_9[0][0]                   
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 5, 5, 128)    393344      batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 5, 5, 128)    393344      batch_normalization_12[0][0]     
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 5, 5, 128)    512         conv2d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 5, 5, 128)    512         conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 2, 5, 128)    393344      batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 2, 5, 128)    393344      batch_normalization_13[0][0]     
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 2, 5, 128)    512         conv2d_5[0][0]                   
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 2, 5, 128)    512         conv2d_11[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 1, 5, 128)    0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 1, 5, 128)    0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
flatten (Flatten)               (None, 640)          0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 640)          0           max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 640)          2560        flatten[0][0]                    
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 640)          2560        flatten_1[0][0]                  
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          328192      batch_normalization_7[0][0]      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          328192      batch_normalization_15[0][0]     
__________________________________________________________________________________________________
dropout (Dropout)               (None, 512)          0           dense[0][0]                      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 1024)         0           dropout[0][0]                    
                                                                 dropout_1[0][0]                  
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          524800      concatenate[0][0]                
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 512)          262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 2)            1026        dense_3[0][0]                    
==================================================================================================
Total params: 3,818,626
Trainable params: 3,813,506
Non-trainable params: 5,120
__________________________________________________________________________________________________
Found 3436 images belonging to 2 classes.
Found 3436 images belonging to 2 classes.
Epoch 1/500
Found 424 images belonging to 2 classes.
Found 424 images belonging to 2 classes.
1535/1535 - 442s - loss: 0.3404 - accuracy: 0.8291 - val_loss: 1.6061 - val_accuracy: 0.6072
Epoch 2/500
1535/1535 - 224s - loss: 0.0359 - accuracy: 0.9882 - val_loss: 3.1676 - val_accuracy: 0.6181
Epoch 3/500
1535/1535 - 222s - loss: 0.0183 - accuracy: 0.9948 - val_loss: 3.6075 - val_accuracy: 0.5748
Epoch 4/500
1535/1535 - 223s - loss: 0.0202 - accuracy: 0.9945 - val_loss: 3.3746 - val_accuracy: 0.6259
Epoch 5/500
1535/1535 - 229s - loss: 0.0164 - accuracy: 0.9953 - val_loss: 7.4396 - val_accuracy: 0.4926
Epoch 6/500
1535/1535 - 227s - loss: 0.0131 - accuracy: 0.9966 - val_loss: 3.6893 - val_accuracy: 0.5731
Epoch 7/500
1535/1535 - 227s - loss: 0.0115 - accuracy: 0.9973 - val_loss: 4.0100 - val_accuracy: 0.6312
Epoch 8/500
1535/1535 - 230s - loss: 0.0093 - accuracy: 0.9971 - val_loss: 3.1602 - val_accuracy: 0.5790
Epoch 9/500
1535/1535 - 228s - loss: 0.0077 - accuracy: 0.9983 - val_loss: 4.2975 - val_accuracy: 0.6167
Epoch 10/500
1535/1535 - 228s - loss: 0.0084 - accuracy: 0.9976 - val_loss: 4.1597 - val_accuracy: 0.6316
Epoch 11/500
1535/1535 - 228s - loss: 0.0073 - accuracy: 0.9981 - val_loss: 4.5582 - val_accuracy: 0.5848
Epoch 12/500
1535/1535 - 231s - loss: 0.0090 - accuracy: 0.9974 - val_loss: 4.7926 - val_accuracy: 0.5966
Epoch 13/500
1535/1535 - 228s - loss: 0.0076 - accuracy: 0.9979 - val_loss: 3.2682 - val_accuracy: 0.6321
Epoch 14/500
1535/1535 - 233s - loss: 0.0050 - accuracy: 0.9988 - val_loss: 3.7263 - val_accuracy: 0.6259
Epoch 15/500
1535/1535 - 229s - loss: 0.0068 - accuracy: 0.9982 - val_loss: 3.4943 - val_accuracy: 0.6350
Epoch 16/500
1535/1535 - 235s - loss: 0.0036 - accuracy: 0.9989 - val_loss: 5.7242 - val_accuracy: 0.5904
Epoch 17/500
1535/1535 - 233s - loss: 0.0027 - accuracy: 0.9993 - val_loss: 6.5833 - val_accuracy: 0.5298
Epoch 18/500
1535/1535 - 233s - loss: 0.0054 - accuracy: 0.9985 - val_loss: 4.4387 - val_accuracy: 0.6545
Epoch 19/500
1535/1535 - 231s - loss: 0.0047 - accuracy: 0.9989 - val_loss: 3.7797 - val_accuracy: 0.6482
Epoch 20/500
1535/1535 - 232s - loss: 0.0036 - accuracy: 0.9989 - val_loss: 3.8171 - val_accuracy: 0.6196
Epoch 21/500
1535/1535 - 225s - loss: 0.0036 - accuracy: 0.9991 - val_loss: 4.9656 - val_accuracy: 0.6116
Epoch 22/500
1535/1535 - 227s - loss: 0.0060 - accuracy: 0.9984 - val_loss: 4.7166 - val_accuracy: 0.5603
Epoch 23/500
1535/1535 - 225s - loss: 0.0022 - accuracy: 0.9993 - val_loss: 4.6820 - val_accuracy: 0.6304
Epoch 24/500
1535/1535 - 237s - loss: 0.0021 - accuracy: 0.9995 - val_loss: 5.9632 - val_accuracy: 0.6562
Epoch 25/500
1535/1535 - 239s - loss: 0.0056 - accuracy: 0.9988 - val_loss: 5.3874 - val_accuracy: 0.6052
Epoch 26/500
1535/1535 - 237s - loss: 0.0049 - accuracy: 0.9991 - val_loss: 3.7589 - val_accuracy: 0.6191
Epoch 27/500
1535/1535 - 241s - loss: 0.0017 - accuracy: 0.9996 - val_loss: 4.5216 - val_accuracy: 0.6189
Epoch 28/500
1535/1535 - 247s - loss: 0.0014 - accuracy: 0.9996 - val_loss: 5.8716 - val_accuracy: 0.6133
Epoch 29/500
1535/1535 - 243s - loss: 0.0042 - accuracy: 0.9987 - val_loss: 5.5389 - val_accuracy: 0.6114
Epoch 30/500
1535/1535 - 237s - loss: 0.0024 - accuracy: 0.9993 - val_loss: 5.3940 - val_accuracy: 0.6108
Epoch 31/500
1535/1535 - 226s - loss: 0.0027 - accuracy: 0.9993 - val_loss: 5.2089 - val_accuracy: 0.6136
Epoch 32/500
1535/1535 - 230s - loss: 0.0021 - accuracy: 0.9994 - val_loss: 4.8930 - val_accuracy: 0.6229
Epoch 33/500
1535/1535 - 233s - loss: 0.0038 - accuracy: 0.9989 - val_loss: 5.8632 - val_accuracy: 0.5964
Epoch 34/500
1535/1535 - 231s - loss: 0.0024 - accuracy: 0.9993 - val_loss: 6.2947 - val_accuracy: 0.6337
========================================
save_weights
h5_weights/ME.pp/onehot_cnn_two_branch.h5
========================================

end time >>> Sun Oct  3 21:02:14 2021

end time >>> Sun Oct  3 21:02:14 2021

end time >>> Sun Oct  3 21:02:14 2021

end time >>> Sun Oct  3 21:02:14 2021

end time >>> Sun Oct  3 21:02:14 2021












args.model = onehot_cnn_two_branch
time used = 8088.967936515808


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 21:02:16 2021

begin time >>> Sun Oct  3 21:02:16 2021

begin time >>> Sun Oct  3 21:02:16 2021

begin time >>> Sun Oct  3 21:02:16 2021

begin time >>> Sun Oct  3 21:02:16 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_dense
args.type = train
args.name = ME.pp
args.length = 10001
===========================


-> h5_weights/ME.pp folder already exist. pass.
-> result/ME.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/ME.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/ME.pp/onehot_embedding_dense folder already exist. pass.
-> result/ME.pp/onehot_dense folder already exist. pass.
-> result/ME.pp/onehot_resnet18 folder already exist. pass.
-> result/ME.pp/onehot_resnet34 folder already exist. pass.
-> result/ME.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/ME.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/ME.pp/embedding_dense folder already exist. pass.
-> result/ME.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/ME.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
ME.pp
########################################

########################################
model_name
onehot_dense
########################################

Found 3436 images belonging to 2 classes.
Found 424 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 100010)            0         
_________________________________________________________________
batch_normalization (BatchNo (None, 100010)            400040    
_________________________________________________________________
dense (Dense)                (None, 512)               51205632  
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_3 (Batch (None, 512)               2048      
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_3 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 52,402,858
Trainable params: 52,198,742
Non-trainable params: 204,116
_________________________________________________________________
Epoch 1/500
107/107 - 23s - loss: 0.8074 - accuracy: 0.5050 - val_loss: 0.6779 - val_accuracy: 0.5721
Epoch 2/500
107/107 - 10s - loss: 0.7137 - accuracy: 0.5787 - val_loss: 0.6689 - val_accuracy: 0.5865
Epoch 3/500
107/107 - 10s - loss: 0.6659 - accuracy: 0.6152 - val_loss: 0.6505 - val_accuracy: 0.5889
Epoch 4/500
107/107 - 10s - loss: 0.5895 - accuracy: 0.6842 - val_loss: 0.6678 - val_accuracy: 0.5913
Epoch 5/500
107/107 - 10s - loss: 0.5033 - accuracy: 0.7568 - val_loss: 0.7033 - val_accuracy: 0.5962
Epoch 6/500
107/107 - 10s - loss: 0.3986 - accuracy: 0.8334 - val_loss: 0.8110 - val_accuracy: 0.6010
Epoch 7/500
107/107 - 10s - loss: 0.2943 - accuracy: 0.8854 - val_loss: 0.9470 - val_accuracy: 0.6010
Epoch 8/500
107/107 - 10s - loss: 0.2317 - accuracy: 0.9157 - val_loss: 1.0708 - val_accuracy: 0.6058
Epoch 9/500
107/107 - 10s - loss: 0.1647 - accuracy: 0.9410 - val_loss: 1.2601 - val_accuracy: 0.5865
Epoch 10/500
107/107 - 10s - loss: 0.1454 - accuracy: 0.9492 - val_loss: 1.3919 - val_accuracy: 0.5913
Epoch 11/500
107/107 - 11s - loss: 0.1116 - accuracy: 0.9618 - val_loss: 1.4639 - val_accuracy: 0.6034
Epoch 12/500
107/107 - 10s - loss: 0.1055 - accuracy: 0.9624 - val_loss: 1.6040 - val_accuracy: 0.5986
Epoch 13/500
107/107 - 10s - loss: 0.0842 - accuracy: 0.9724 - val_loss: 1.6490 - val_accuracy: 0.6058
Epoch 14/500
107/107 - 11s - loss: 0.0736 - accuracy: 0.9739 - val_loss: 1.6836 - val_accuracy: 0.6082
Epoch 15/500
107/107 - 10s - loss: 0.0582 - accuracy: 0.9824 - val_loss: 1.7771 - val_accuracy: 0.5913
Epoch 16/500
107/107 - 10s - loss: 0.0644 - accuracy: 0.9788 - val_loss: 1.8325 - val_accuracy: 0.5889
Epoch 17/500
107/107 - 10s - loss: 0.0555 - accuracy: 0.9797 - val_loss: 1.8748 - val_accuracy: 0.5865
Epoch 18/500
107/107 - 10s - loss: 0.0598 - accuracy: 0.9812 - val_loss: 1.9503 - val_accuracy: 0.5889
Epoch 19/500
107/107 - 10s - loss: 0.0483 - accuracy: 0.9850 - val_loss: 1.9268 - val_accuracy: 0.5938
Epoch 20/500
107/107 - 10s - loss: 0.0494 - accuracy: 0.9833 - val_loss: 2.0083 - val_accuracy: 0.5986
Epoch 21/500
107/107 - 10s - loss: 0.0415 - accuracy: 0.9850 - val_loss: 2.0149 - val_accuracy: 0.5865
Epoch 22/500
107/107 - 10s - loss: 0.0409 - accuracy: 0.9841 - val_loss: 2.0052 - val_accuracy: 0.5986
Epoch 23/500
107/107 - 10s - loss: 0.0340 - accuracy: 0.9906 - val_loss: 2.0493 - val_accuracy: 0.6058
Epoch 24/500
107/107 - 10s - loss: 0.0363 - accuracy: 0.9885 - val_loss: 2.0489 - val_accuracy: 0.6034
========================================
save_weights
h5_weights/ME.pp/onehot_dense.h5
========================================

end time >>> Sun Oct  3 21:06:47 2021

end time >>> Sun Oct  3 21:06:47 2021

end time >>> Sun Oct  3 21:06:47 2021

end time >>> Sun Oct  3 21:06:47 2021

end time >>> Sun Oct  3 21:06:47 2021












args.model = onehot_dense
time used = 271.7813346385956


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 21:06:48 2021

begin time >>> Sun Oct  3 21:06:48 2021

begin time >>> Sun Oct  3 21:06:48 2021

begin time >>> Sun Oct  3 21:06:48 2021

begin time >>> Sun Oct  3 21:06:48 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_resnet18
args.type = train
args.name = ME.pp
args.length = 10001
===========================


-> h5_weights/ME.pp folder already exist. pass.
-> result/ME.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/ME.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/ME.pp/onehot_embedding_dense folder already exist. pass.
-> result/ME.pp/onehot_dense folder already exist. pass.
-> result/ME.pp/onehot_resnet18 folder already exist. pass.
-> result/ME.pp/onehot_resnet34 folder already exist. pass.
-> result/ME.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/ME.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/ME.pp/embedding_dense folder already exist. pass.
-> result/ME.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/ME.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
ME.pp
########################################

########################################
model_name
onehot_resnet18
########################################

Found 3436 images belonging to 2 classes.
Found 424 images belonging to 2 classes.
Model: "res_net_type_i"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              multiple                  1600      
_________________________________________________________________
batch_normalization (BatchNo multiple                  256       
_________________________________________________________________
max_pooling2d (MaxPooling2D) multiple                  0         
_________________________________________________________________
sequential (Sequential)      (None, 313, 1, 64)        398912    
_________________________________________________________________
sequential_2 (Sequential)    (None, 20, 1, 64)         398912    
_________________________________________________________________
sequential_4 (Sequential)    (None, 2, 1, 64)          398912    
_________________________________________________________________
sequential_6 (Sequential)    (None, 1, 1, 64)          398912    
_________________________________________________________________
global_average_pooling2d (Gl multiple                  0         
_________________________________________________________________
dense (Dense)                multiple                  130       
=================================================================
Total params: 1,597,634
Trainable params: 1,594,946
Non-trainable params: 2,688
_________________________________________________________________
Epoch 1/500
107/107 - 14s - loss: 0.8053 - accuracy: 0.5103 - val_loss: 0.6940 - val_accuracy: 0.4976
Epoch 2/500
107/107 - 13s - loss: 0.5916 - accuracy: 0.6880 - val_loss: 0.6983 - val_accuracy: 0.4976
Epoch 3/500
107/107 - 13s - loss: 0.4688 - accuracy: 0.7947 - val_loss: 0.7057 - val_accuracy: 0.4808
Epoch 4/500
107/107 - 13s - loss: 0.3767 - accuracy: 0.8593 - val_loss: 0.7437 - val_accuracy: 0.4976
Epoch 5/500
107/107 - 13s - loss: 0.3028 - accuracy: 0.8913 - val_loss: 0.8020 - val_accuracy: 0.5024
Epoch 6/500
107/107 - 14s - loss: 0.2218 - accuracy: 0.9330 - val_loss: 0.8242 - val_accuracy: 0.5264
Epoch 7/500
107/107 - 13s - loss: 0.1509 - accuracy: 0.9671 - val_loss: 0.8685 - val_accuracy: 0.5337
Epoch 8/500
107/107 - 13s - loss: 0.1066 - accuracy: 0.9783 - val_loss: 0.8762 - val_accuracy: 0.5769
Epoch 9/500
107/107 - 13s - loss: 0.0798 - accuracy: 0.9862 - val_loss: 0.9409 - val_accuracy: 0.5385
Epoch 10/500
107/107 - 13s - loss: 0.0627 - accuracy: 0.9915 - val_loss: 0.9445 - val_accuracy: 0.5553
Epoch 11/500
107/107 - 14s - loss: 0.0546 - accuracy: 0.9900 - val_loss: 0.9830 - val_accuracy: 0.5625
Epoch 12/500
107/107 - 14s - loss: 0.0467 - accuracy: 0.9903 - val_loss: 1.0851 - val_accuracy: 0.5361
Epoch 13/500
107/107 - 14s - loss: 0.0438 - accuracy: 0.9921 - val_loss: 1.1447 - val_accuracy: 0.5457
Epoch 14/500
107/107 - 13s - loss: 0.0483 - accuracy: 0.9891 - val_loss: 1.1145 - val_accuracy: 0.5769
Epoch 15/500
107/107 - 14s - loss: 0.0544 - accuracy: 0.9865 - val_loss: 1.2713 - val_accuracy: 0.5433
Epoch 16/500
107/107 - 13s - loss: 0.0665 - accuracy: 0.9797 - val_loss: 1.2446 - val_accuracy: 0.5553
Epoch 17/500
107/107 - 14s - loss: 0.0928 - accuracy: 0.9674 - val_loss: 1.2263 - val_accuracy: 0.5745
Epoch 18/500
107/107 - 14s - loss: 0.1303 - accuracy: 0.9492 - val_loss: 1.4186 - val_accuracy: 0.5312
========================================
save_weights
h5_weights/ME.pp/onehot_resnet18.h5
========================================

end time >>> Sun Oct  3 21:11:06 2021

end time >>> Sun Oct  3 21:11:06 2021

end time >>> Sun Oct  3 21:11:06 2021

end time >>> Sun Oct  3 21:11:06 2021

end time >>> Sun Oct  3 21:11:06 2021












args.model = onehot_resnet18
time used = 257.8213937282562


