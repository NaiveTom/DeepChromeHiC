************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 17:30:15 2021

begin time >>> Sun Oct  3 17:30:15 2021

begin time >>> Sun Oct  3 17:30:15 2021

begin time >>> Sun Oct  3 17:30:15 2021

begin time >>> Sun Oct  3 17:30:15 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_cnn_one_branch
args.type = train
args.name = LI11.po
args.length = 10001
===========================


-> make new folder: h5_weights/LI11.po
-> make new folder: result/LI11.po/onehot_cnn_one_branch
-> make new folder: result/LI11.po/onehot_cnn_two_branch
-> make new folder: result/LI11.po/onehot_embedding_dense
-> make new folder: result/LI11.po/onehot_dense
-> make new folder: result/LI11.po/onehot_resnet18
-> make new folder: result/LI11.po/onehot_resnet34
-> make new folder: result/LI11.po/embedding_cnn_one_branch
-> make new folder: result/LI11.po/embedding_cnn_two_branch
-> make new folder: result/LI11.po/embedding_dense
-> make new folder: result/LI11.po/onehot_embedding_cnn_one_branch
-> make new folder: result/LI11.po/onehot_embedding_cnn_two_branch
########################################
gen_name
LI11.po
########################################

########################################
model_name
onehot_cnn_one_branch
########################################

Found 5518 images belonging to 2 classes.
Found 680 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 5001, 5, 64)       1600      
_________________________________________________________________
batch_normalization (BatchNo (None, 5001, 5, 64)       256       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1251, 5, 64)       98368     
_________________________________________________________________
batch_normalization_1 (Batch (None, 1251, 5, 64)       256       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 313, 5, 64)        98368     
_________________________________________________________________
batch_normalization_2 (Batch (None, 313, 5, 64)        256       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 156, 5, 64)        0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 156, 5, 64)        256       
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 39, 5, 128)        196736    
_________________________________________________________________
batch_normalization_4 (Batch (None, 39, 5, 128)        512       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 10, 5, 128)        393344    
_________________________________________________________________
batch_normalization_5 (Batch (None, 10, 5, 128)        512       
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 3, 5, 128)         393344    
_________________________________________________________________
batch_normalization_6 (Batch (None, 3, 5, 128)         512       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 1, 5, 128)         0         
_________________________________________________________________
flatten (Flatten)            (None, 640)               0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 640)               2560      
_________________________________________________________________
dense (Dense)                (None, 512)               328192    
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
dense_3 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 2,041,410
Trainable params: 2,038,850
Non-trainable params: 2,560
_________________________________________________________________
Epoch 1/500
172/172 - 179s - loss: 0.7719 - accuracy: 0.5029 - val_loss: 0.6992 - val_accuracy: 0.5164
Epoch 2/500
172/172 - 21s - loss: 0.7082 - accuracy: 0.5527 - val_loss: 0.7087 - val_accuracy: 0.4673
Epoch 3/500
172/172 - 21s - loss: 0.6757 - accuracy: 0.5882 - val_loss: 0.7110 - val_accuracy: 0.4970
Epoch 4/500
172/172 - 21s - loss: 0.6380 - accuracy: 0.6342 - val_loss: 0.7085 - val_accuracy: 0.5283
Epoch 5/500
172/172 - 21s - loss: 0.5820 - accuracy: 0.6938 - val_loss: 0.7188 - val_accuracy: 0.5432
Epoch 6/500
172/172 - 21s - loss: 0.4651 - accuracy: 0.7865 - val_loss: 0.8520 - val_accuracy: 0.5521
Epoch 7/500
172/172 - 21s - loss: 0.3088 - accuracy: 0.8753 - val_loss: 1.4498 - val_accuracy: 0.5193
Epoch 8/500
172/172 - 21s - loss: 0.1936 - accuracy: 0.9231 - val_loss: 1.1291 - val_accuracy: 0.5744
Epoch 9/500
172/172 - 21s - loss: 0.1021 - accuracy: 0.9654 - val_loss: 1.8690 - val_accuracy: 0.5610
Epoch 10/500
172/172 - 21s - loss: 0.0618 - accuracy: 0.9805 - val_loss: 3.1344 - val_accuracy: 0.5238
Epoch 11/500
172/172 - 21s - loss: 0.0400 - accuracy: 0.9872 - val_loss: 3.5653 - val_accuracy: 0.5223
Epoch 12/500
172/172 - 21s - loss: 0.0308 - accuracy: 0.9903 - val_loss: 2.1256 - val_accuracy: 0.5565
Epoch 13/500
172/172 - 22s - loss: 0.0280 - accuracy: 0.9900 - val_loss: 2.0796 - val_accuracy: 0.5789
Epoch 14/500
172/172 - 21s - loss: 0.0207 - accuracy: 0.9933 - val_loss: 3.4301 - val_accuracy: 0.5402
Epoch 15/500
172/172 - 22s - loss: 0.0196 - accuracy: 0.9943 - val_loss: 2.9104 - val_accuracy: 0.5804
Epoch 16/500
172/172 - 21s - loss: 0.0199 - accuracy: 0.9923 - val_loss: 3.0576 - val_accuracy: 0.5699
Epoch 17/500
172/172 - 21s - loss: 0.0232 - accuracy: 0.9913 - val_loss: 9.2805 - val_accuracy: 0.4970
Epoch 18/500
172/172 - 21s - loss: 0.0238 - accuracy: 0.9918 - val_loss: 3.5377 - val_accuracy: 0.5580
Epoch 19/500
172/172 - 21s - loss: 0.0247 - accuracy: 0.9927 - val_loss: 8.7354 - val_accuracy: 0.4970
Epoch 20/500
172/172 - 21s - loss: 0.0225 - accuracy: 0.9911 - val_loss: 4.0533 - val_accuracy: 0.5446
Epoch 21/500
172/172 - 21s - loss: 0.0338 - accuracy: 0.9896 - val_loss: 3.1165 - val_accuracy: 0.5491
Epoch 22/500
172/172 - 21s - loss: 0.0268 - accuracy: 0.9900 - val_loss: 6.9597 - val_accuracy: 0.5060
Epoch 23/500
172/172 - 21s - loss: 0.0240 - accuracy: 0.9918 - val_loss: 4.6115 - val_accuracy: 0.5699
Epoch 24/500
172/172 - 22s - loss: 0.0263 - accuracy: 0.9903 - val_loss: 4.4088 - val_accuracy: 0.5417
Epoch 25/500
172/172 - 21s - loss: 0.0208 - accuracy: 0.9929 - val_loss: 2.6478 - val_accuracy: 0.5833
Epoch 26/500
172/172 - 21s - loss: 0.0277 - accuracy: 0.9911 - val_loss: 3.1582 - val_accuracy: 0.5938
Epoch 27/500
172/172 - 22s - loss: 0.0189 - accuracy: 0.9949 - val_loss: 5.8855 - val_accuracy: 0.5164
Epoch 28/500
172/172 - 21s - loss: 0.0258 - accuracy: 0.9923 - val_loss: 6.7433 - val_accuracy: 0.5015
Epoch 29/500
172/172 - 21s - loss: 0.0256 - accuracy: 0.9913 - val_loss: 2.9729 - val_accuracy: 0.5982
Epoch 30/500
172/172 - 21s - loss: 0.0222 - accuracy: 0.9925 - val_loss: 2.7973 - val_accuracy: 0.5908
Epoch 31/500
172/172 - 21s - loss: 0.0141 - accuracy: 0.9949 - val_loss: 3.3929 - val_accuracy: 0.5655
Epoch 32/500
172/172 - 21s - loss: 0.0187 - accuracy: 0.9942 - val_loss: 2.4993 - val_accuracy: 0.5997
Epoch 33/500
172/172 - 21s - loss: 0.0203 - accuracy: 0.9925 - val_loss: 2.6776 - val_accuracy: 0.5804
Epoch 34/500
172/172 - 21s - loss: 0.0117 - accuracy: 0.9960 - val_loss: 3.6457 - val_accuracy: 0.5536
Epoch 35/500
172/172 - 21s - loss: 0.0231 - accuracy: 0.9922 - val_loss: 6.3418 - val_accuracy: 0.5402
Epoch 36/500
172/172 - 21s - loss: 0.0177 - accuracy: 0.9929 - val_loss: 4.2035 - val_accuracy: 0.5610
Epoch 37/500
172/172 - 21s - loss: 0.0242 - accuracy: 0.9920 - val_loss: 2.8058 - val_accuracy: 0.6042
Epoch 38/500
172/172 - 21s - loss: 0.0169 - accuracy: 0.9953 - val_loss: 3.1000 - val_accuracy: 0.5997
Epoch 39/500
172/172 - 21s - loss: 0.0226 - accuracy: 0.9913 - val_loss: 12.5085 - val_accuracy: 0.5045
Epoch 40/500
172/172 - 21s - loss: 0.0183 - accuracy: 0.9931 - val_loss: 3.8358 - val_accuracy: 0.5729
Epoch 41/500
172/172 - 21s - loss: 0.0105 - accuracy: 0.9958 - val_loss: 2.8835 - val_accuracy: 0.6116
Epoch 42/500
172/172 - 21s - loss: 0.0191 - accuracy: 0.9934 - val_loss: 3.9494 - val_accuracy: 0.5610
Epoch 43/500
172/172 - 21s - loss: 0.0131 - accuracy: 0.9960 - val_loss: 3.2257 - val_accuracy: 0.5759
Epoch 44/500
172/172 - 21s - loss: 0.0142 - accuracy: 0.9953 - val_loss: 2.8305 - val_accuracy: 0.6101
Epoch 45/500
172/172 - 21s - loss: 0.0099 - accuracy: 0.9971 - val_loss: 2.7506 - val_accuracy: 0.6027
Epoch 46/500
172/172 - 21s - loss: 0.0201 - accuracy: 0.9940 - val_loss: 3.7524 - val_accuracy: 0.5744
Epoch 47/500
172/172 - 21s - loss: 0.0121 - accuracy: 0.9956 - val_loss: 3.1121 - val_accuracy: 0.5997
Epoch 48/500
172/172 - 21s - loss: 0.0088 - accuracy: 0.9964 - val_loss: 4.6260 - val_accuracy: 0.5625
Epoch 49/500
172/172 - 21s - loss: 0.0172 - accuracy: 0.9940 - val_loss: 6.0290 - val_accuracy: 0.5283
Epoch 50/500
172/172 - 21s - loss: 0.0157 - accuracy: 0.9947 - val_loss: 3.1146 - val_accuracy: 0.6027
Epoch 51/500
172/172 - 22s - loss: 0.0243 - accuracy: 0.9918 - val_loss: 2.7375 - val_accuracy: 0.6012
========================================
save_weights
h5_weights/LI11.po/onehot_cnn_one_branch.h5
========================================

end time >>> Sun Oct  3 17:51:09 2021

end time >>> Sun Oct  3 17:51:09 2021

end time >>> Sun Oct  3 17:51:09 2021

end time >>> Sun Oct  3 17:51:09 2021

end time >>> Sun Oct  3 17:51:09 2021












args.model = onehot_cnn_one_branch
time used = 1254.4359221458435


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 17:51:10 2021

begin time >>> Sun Oct  3 17:51:10 2021

begin time >>> Sun Oct  3 17:51:10 2021

begin time >>> Sun Oct  3 17:51:10 2021

begin time >>> Sun Oct  3 17:51:10 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_cnn_two_branch
args.type = train
args.name = LI11.po
args.length = 10001
===========================


-> h5_weights/LI11.po folder already exist. pass.
-> result/LI11.po/onehot_cnn_one_branch folder already exist. pass.
-> result/LI11.po/onehot_cnn_two_branch folder already exist. pass.
-> result/LI11.po/onehot_embedding_dense folder already exist. pass.
-> result/LI11.po/onehot_dense folder already exist. pass.
-> result/LI11.po/onehot_resnet18 folder already exist. pass.
-> result/LI11.po/onehot_resnet34 folder already exist. pass.
-> result/LI11.po/embedding_cnn_one_branch folder already exist. pass.
-> result/LI11.po/embedding_cnn_two_branch folder already exist. pass.
-> result/LI11.po/embedding_dense folder already exist. pass.
-> result/LI11.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/LI11.po/onehot_embedding_cnn_two_branch folder already exist. pass.
Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001, 5, 1) 0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001, 5, 1) 0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 2501, 5, 64)  1600        input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 2501, 5, 64)  1600        input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 2501, 5, 64)  256         conv2d[0][0]                     
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 2501, 5, 64)  256         conv2d_6[0][0]                   
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 626, 5, 64)   98368       batch_normalization[0][0]        
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 626, 5, 64)   98368       batch_normalization_8[0][0]      
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 626, 5, 64)   256         conv2d_1[0][0]                   
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 626, 5, 64)   256         conv2d_7[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 157, 5, 64)   98368       batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 157, 5, 64)   98368       batch_normalization_9[0][0]      
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 157, 5, 64)   256         conv2d_2[0][0]                   
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 157, 5, 64)   256         conv2d_8[0][0]                   
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 78, 5, 64)    0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 78, 5, 64)    0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 78, 5, 64)    256         max_pooling2d[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 78, 5, 64)    256         max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 20, 5, 128)   196736      batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 20, 5, 128)   196736      batch_normalization_11[0][0]     
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 20, 5, 128)   512         conv2d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 20, 5, 128)   512         conv2d_9[0][0]                   
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 5, 5, 128)    393344      batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 5, 5, 128)    393344      batch_normalization_12[0][0]     
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 5, 5, 128)    512         conv2d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 5, 5, 128)    512         conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 2, 5, 128)    393344      batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 2, 5, 128)    393344      batch_normalization_13[0][0]     
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 2, 5, 128)    512         conv2d_5[0][0]                   
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 2, 5, 128)    512         conv2d_11[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 1, 5, 128)    0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 1, 5, 128)    0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
flatten (Flatten)               (None, 640)          0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 640)          0           max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 640)          2560        flatten[0][0]                    
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 640)          2560        flatten_1[0][0]                  
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          328192      batch_normalization_7[0][0]      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          328192      batch_normalization_15[0][0]     
__________________________________________________________________________________________________
dropout (Dropout)               (None, 512)          0           dense[0][0]                      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 1024)         0           dropout[0][0]                    
                                                                 dropout_1[0][0]                  
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          524800      concatenate[0][0]                
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 512)          262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 2)            1026        dense_3[0][0]                    
==================================================================================================
Total params: 3,818,626
Trainable params: 3,813,506
Non-trainable params: 5,120
__________________________________________________________________________________________________
Found 5518 images belonging to 2 classes.
Found 5518 images belonging to 2 classes.
Epoch 1/500
Found 680 images belonging to 2 classes.
Found 680 images belonging to 2 classes.
1535/1535 - 524s - loss: 0.4961 - accuracy: 0.7336 - val_loss: 0.9598 - val_accuracy: 0.6012
Epoch 2/500
1535/1535 - 225s - loss: 0.0472 - accuracy: 0.9838 - val_loss: 2.4222 - val_accuracy: 0.5760
Epoch 3/500
1535/1535 - 225s - loss: 0.0291 - accuracy: 0.9912 - val_loss: 5.4530 - val_accuracy: 0.5307
Epoch 4/500
1535/1535 - 229s - loss: 0.0203 - accuracy: 0.9936 - val_loss: 6.0030 - val_accuracy: 0.5596
Epoch 5/500
1535/1535 - 227s - loss: 0.0184 - accuracy: 0.9944 - val_loss: 2.0969 - val_accuracy: 0.6160
Epoch 6/500
1535/1535 - 235s - loss: 0.0125 - accuracy: 0.9962 - val_loss: 2.7405 - val_accuracy: 0.6123
Epoch 7/500
1535/1535 - 230s - loss: 0.0187 - accuracy: 0.9947 - val_loss: 2.0722 - val_accuracy: 0.5794
Epoch 8/500
1535/1535 - 237s - loss: 0.0113 - accuracy: 0.9964 - val_loss: 3.8729 - val_accuracy: 0.5574
Epoch 9/500
1535/1535 - 234s - loss: 0.0134 - accuracy: 0.9961 - val_loss: 3.0782 - val_accuracy: 0.6018
Epoch 10/500
1535/1535 - 230s - loss: 0.0085 - accuracy: 0.9973 - val_loss: 2.2799 - val_accuracy: 0.5945
Epoch 11/500
1535/1535 - 230s - loss: 0.0085 - accuracy: 0.9974 - val_loss: 2.9882 - val_accuracy: 0.5971
Epoch 12/500
1535/1535 - 224s - loss: 0.0113 - accuracy: 0.9968 - val_loss: 2.3233 - val_accuracy: 0.6007
Epoch 13/500
1535/1535 - 229s - loss: 0.0049 - accuracy: 0.9986 - val_loss: 2.9449 - val_accuracy: 0.6108
Epoch 14/500
1535/1535 - 233s - loss: 0.0095 - accuracy: 0.9976 - val_loss: 2.4200 - val_accuracy: 0.6104
Epoch 15/500
1535/1535 - 227s - loss: 0.0062 - accuracy: 0.9979 - val_loss: 3.4686 - val_accuracy: 0.5924
========================================
save_weights
h5_weights/LI11.po/onehot_cnn_two_branch.h5
========================================

end time >>> Sun Oct  3 18:53:45 2021

end time >>> Sun Oct  3 18:53:45 2021

end time >>> Sun Oct  3 18:53:45 2021

end time >>> Sun Oct  3 18:53:45 2021

end time >>> Sun Oct  3 18:53:45 2021












args.model = onehot_cnn_two_branch
time used = 3755.0428006649017


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 18:53:47 2021

begin time >>> Sun Oct  3 18:53:47 2021

begin time >>> Sun Oct  3 18:53:47 2021

begin time >>> Sun Oct  3 18:53:47 2021

begin time >>> Sun Oct  3 18:53:47 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_dense
args.type = train
args.name = LI11.po
args.length = 10001
===========================


-> h5_weights/LI11.po folder already exist. pass.
-> result/LI11.po/onehot_cnn_one_branch folder already exist. pass.
-> result/LI11.po/onehot_cnn_two_branch folder already exist. pass.
-> result/LI11.po/onehot_embedding_dense folder already exist. pass.
-> result/LI11.po/onehot_dense folder already exist. pass.
-> result/LI11.po/onehot_resnet18 folder already exist. pass.
-> result/LI11.po/onehot_resnet34 folder already exist. pass.
-> result/LI11.po/embedding_cnn_one_branch folder already exist. pass.
-> result/LI11.po/embedding_cnn_two_branch folder already exist. pass.
-> result/LI11.po/embedding_dense folder already exist. pass.
-> result/LI11.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/LI11.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
LI11.po
########################################

########################################
model_name
onehot_dense
########################################

Found 5518 images belonging to 2 classes.
Found 680 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 100010)            0         
_________________________________________________________________
batch_normalization (BatchNo (None, 100010)            400040    
_________________________________________________________________
dense (Dense)                (None, 512)               51205632  
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_3 (Batch (None, 512)               2048      
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_3 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 52,402,858
Trainable params: 52,198,742
Non-trainable params: 204,116
_________________________________________________________________
Epoch 1/500
172/172 - 17s - loss: 0.8144 - accuracy: 0.5235 - val_loss: 0.6835 - val_accuracy: 0.5699
Epoch 2/500
172/172 - 16s - loss: 0.7313 - accuracy: 0.5664 - val_loss: 0.6760 - val_accuracy: 0.5729
Epoch 3/500
172/172 - 16s - loss: 0.6533 - accuracy: 0.6367 - val_loss: 0.6583 - val_accuracy: 0.5967
Epoch 4/500
172/172 - 16s - loss: 0.5617 - accuracy: 0.7149 - val_loss: 0.6444 - val_accuracy: 0.6071
Epoch 5/500
172/172 - 16s - loss: 0.4553 - accuracy: 0.7827 - val_loss: 0.6358 - val_accuracy: 0.6071
Epoch 6/500
172/172 - 16s - loss: 0.3394 - accuracy: 0.8531 - val_loss: 0.6496 - val_accuracy: 0.6280
Epoch 7/500
172/172 - 16s - loss: 0.2500 - accuracy: 0.9056 - val_loss: 0.6787 - val_accuracy: 0.6310
Epoch 8/500
172/172 - 15s - loss: 0.1792 - accuracy: 0.9338 - val_loss: 0.7114 - val_accuracy: 0.6280
Epoch 9/500
172/172 - 16s - loss: 0.1365 - accuracy: 0.9506 - val_loss: 0.7511 - val_accuracy: 0.6399
Epoch 10/500
172/172 - 15s - loss: 0.1033 - accuracy: 0.9630 - val_loss: 0.8234 - val_accuracy: 0.6250
Epoch 11/500
172/172 - 16s - loss: 0.0922 - accuracy: 0.9694 - val_loss: 0.8653 - val_accuracy: 0.6324
Epoch 12/500
172/172 - 16s - loss: 0.0701 - accuracy: 0.9769 - val_loss: 0.9131 - val_accuracy: 0.6324
Epoch 13/500
172/172 - 16s - loss: 0.0612 - accuracy: 0.9772 - val_loss: 0.9742 - val_accuracy: 0.6354
Epoch 14/500
172/172 - 16s - loss: 0.0512 - accuracy: 0.9838 - val_loss: 0.9964 - val_accuracy: 0.6235
Epoch 15/500
172/172 - 15s - loss: 0.0525 - accuracy: 0.9816 - val_loss: 1.0534 - val_accuracy: 0.6250
Epoch 16/500
172/172 - 15s - loss: 0.0423 - accuracy: 0.9849 - val_loss: 1.0556 - val_accuracy: 0.6250
Epoch 17/500
172/172 - 16s - loss: 0.0381 - accuracy: 0.9863 - val_loss: 1.1164 - val_accuracy: 0.6220
Epoch 18/500
172/172 - 15s - loss: 0.0337 - accuracy: 0.9880 - val_loss: 1.1291 - val_accuracy: 0.6310
Epoch 19/500
172/172 - 16s - loss: 0.0347 - accuracy: 0.9892 - val_loss: 1.1618 - val_accuracy: 0.6339
========================================
save_weights
h5_weights/LI11.po/onehot_dense.h5
========================================

end time >>> Sun Oct  3 18:58:57 2021

end time >>> Sun Oct  3 18:58:57 2021

end time >>> Sun Oct  3 18:58:57 2021

end time >>> Sun Oct  3 18:58:57 2021

end time >>> Sun Oct  3 18:58:57 2021












args.model = onehot_dense
time used = 310.3865041732788


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 18:58:58 2021

begin time >>> Sun Oct  3 18:58:58 2021

begin time >>> Sun Oct  3 18:58:58 2021

begin time >>> Sun Oct  3 18:58:58 2021

begin time >>> Sun Oct  3 18:58:58 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_resnet18
args.type = train
args.name = LI11.po
args.length = 10001
===========================


-> h5_weights/LI11.po folder already exist. pass.
-> result/LI11.po/onehot_cnn_one_branch folder already exist. pass.
-> result/LI11.po/onehot_cnn_two_branch folder already exist. pass.
-> result/LI11.po/onehot_embedding_dense folder already exist. pass.
-> result/LI11.po/onehot_dense folder already exist. pass.
-> result/LI11.po/onehot_resnet18 folder already exist. pass.
-> result/LI11.po/onehot_resnet34 folder already exist. pass.
-> result/LI11.po/embedding_cnn_one_branch folder already exist. pass.
-> result/LI11.po/embedding_cnn_two_branch folder already exist. pass.
-> result/LI11.po/embedding_dense folder already exist. pass.
-> result/LI11.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/LI11.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
LI11.po
########################################

########################################
model_name
onehot_resnet18
########################################

Found 5518 images belonging to 2 classes.
Found 680 images belonging to 2 classes.
Model: "res_net_type_i"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              multiple                  1600      
_________________________________________________________________
batch_normalization (BatchNo multiple                  256       
_________________________________________________________________
max_pooling2d (MaxPooling2D) multiple                  0         
_________________________________________________________________
sequential (Sequential)      (None, 313, 1, 64)        398912    
_________________________________________________________________
sequential_2 (Sequential)    (None, 20, 1, 64)         398912    
_________________________________________________________________
sequential_4 (Sequential)    (None, 2, 1, 64)          398912    
_________________________________________________________________
sequential_6 (Sequential)    (None, 1, 1, 64)          398912    
_________________________________________________________________
global_average_pooling2d (Gl multiple                  0         
_________________________________________________________________
dense (Dense)                multiple                  130       
=================================================================
Total params: 1,597,634
Trainable params: 1,594,946
Non-trainable params: 2,688
_________________________________________________________________
Epoch 1/500
172/172 - 21s - loss: 0.7962 - accuracy: 0.5026 - val_loss: 0.6979 - val_accuracy: 0.5030
Epoch 2/500
172/172 - 20s - loss: 0.5996 - accuracy: 0.6868 - val_loss: 0.7135 - val_accuracy: 0.4955
Epoch 3/500
172/172 - 21s - loss: 0.4905 - accuracy: 0.7754 - val_loss: 0.7634 - val_accuracy: 0.4985
Epoch 4/500
172/172 - 21s - loss: 0.3937 - accuracy: 0.8451 - val_loss: 0.8340 - val_accuracy: 0.5149
Epoch 5/500
172/172 - 21s - loss: 0.2858 - accuracy: 0.9034 - val_loss: 0.9113 - val_accuracy: 0.4926
Epoch 6/500
172/172 - 21s - loss: 0.2047 - accuracy: 0.9406 - val_loss: 1.0134 - val_accuracy: 0.5045
Epoch 7/500
172/172 - 21s - loss: 0.1559 - accuracy: 0.9563 - val_loss: 1.0031 - val_accuracy: 0.5327
Epoch 8/500
172/172 - 20s - loss: 0.1373 - accuracy: 0.9606 - val_loss: 1.1256 - val_accuracy: 0.5060
Epoch 9/500
172/172 - 20s - loss: 0.1111 - accuracy: 0.9659 - val_loss: 1.2109 - val_accuracy: 0.5387
Epoch 10/500
172/172 - 20s - loss: 0.0926 - accuracy: 0.9732 - val_loss: 1.2394 - val_accuracy: 0.5283
Epoch 11/500
172/172 - 20s - loss: 0.0923 - accuracy: 0.9725 - val_loss: 1.2155 - val_accuracy: 0.5357
Epoch 12/500
172/172 - 20s - loss: 0.1039 - accuracy: 0.9635 - val_loss: 1.3459 - val_accuracy: 0.5298
Epoch 13/500
172/172 - 20s - loss: 0.1278 - accuracy: 0.9522 - val_loss: 1.4085 - val_accuracy: 0.5060
Epoch 14/500
172/172 - 20s - loss: 0.1348 - accuracy: 0.9524 - val_loss: 1.3972 - val_accuracy: 0.5402
Epoch 15/500
172/172 - 20s - loss: 0.0915 - accuracy: 0.9655 - val_loss: 1.3703 - val_accuracy: 0.5372
Epoch 16/500
172/172 - 21s - loss: 0.0743 - accuracy: 0.9738 - val_loss: 1.3204 - val_accuracy: 0.5521
Epoch 17/500
172/172 - 21s - loss: 0.0769 - accuracy: 0.9721 - val_loss: 1.4172 - val_accuracy: 0.5417
Epoch 18/500
172/172 - 21s - loss: 0.0658 - accuracy: 0.9799 - val_loss: 1.4766 - val_accuracy: 0.5268
Epoch 19/500
172/172 - 21s - loss: 0.0497 - accuracy: 0.9856 - val_loss: 1.5009 - val_accuracy: 0.5327
Epoch 20/500
172/172 - 21s - loss: 0.0439 - accuracy: 0.9860 - val_loss: 1.4628 - val_accuracy: 0.5268
Epoch 21/500
172/172 - 21s - loss: 0.0467 - accuracy: 0.9849 - val_loss: 1.5609 - val_accuracy: 0.5193
Epoch 22/500
172/172 - 21s - loss: 0.0453 - accuracy: 0.9860 - val_loss: 1.4955 - val_accuracy: 0.5461
Epoch 23/500
172/172 - 21s - loss: 0.0558 - accuracy: 0.9805 - val_loss: 1.5787 - val_accuracy: 0.5699
Epoch 24/500
172/172 - 21s - loss: 0.0688 - accuracy: 0.9754 - val_loss: 1.6236 - val_accuracy: 0.5268
Epoch 25/500
172/172 - 21s - loss: 0.0807 - accuracy: 0.9697 - val_loss: 1.6302 - val_accuracy: 0.5268
Epoch 26/500
172/172 - 21s - loss: 0.0828 - accuracy: 0.9716 - val_loss: 1.5908 - val_accuracy: 0.5565
Epoch 27/500
172/172 - 21s - loss: 0.0669 - accuracy: 0.9748 - val_loss: 1.6624 - val_accuracy: 0.5685
Epoch 28/500
172/172 - 21s - loss: 0.0630 - accuracy: 0.9790 - val_loss: 1.6306 - val_accuracy: 0.5402
Epoch 29/500
172/172 - 21s - loss: 0.0510 - accuracy: 0.9841 - val_loss: 1.5186 - val_accuracy: 0.5655
Epoch 30/500
172/172 - 21s - loss: 0.0443 - accuracy: 0.9861 - val_loss: 1.6027 - val_accuracy: 0.5521
Epoch 31/500
172/172 - 21s - loss: 0.0299 - accuracy: 0.9907 - val_loss: 1.4922 - val_accuracy: 0.5729
Epoch 32/500
172/172 - 21s - loss: 0.0318 - accuracy: 0.9913 - val_loss: 1.6132 - val_accuracy: 0.5774
Epoch 33/500
172/172 - 21s - loss: 0.0407 - accuracy: 0.9878 - val_loss: 1.6682 - val_accuracy: 0.5789
Epoch 34/500
172/172 - 20s - loss: 0.0374 - accuracy: 0.9861 - val_loss: 1.6167 - val_accuracy: 0.5670
Epoch 35/500
172/172 - 21s - loss: 0.0486 - accuracy: 0.9836 - val_loss: 1.6706 - val_accuracy: 0.5595
Epoch 36/500
172/172 - 21s - loss: 0.0436 - accuracy: 0.9856 - val_loss: 1.7142 - val_accuracy: 0.5670
Epoch 37/500
172/172 - 21s - loss: 0.0467 - accuracy: 0.9845 - val_loss: 1.7852 - val_accuracy: 0.5491
Epoch 38/500
172/172 - 21s - loss: 0.0510 - accuracy: 0.9836 - val_loss: 1.7329 - val_accuracy: 0.5625
Epoch 39/500
172/172 - 21s - loss: 0.0471 - accuracy: 0.9830 - val_loss: 1.7014 - val_accuracy: 0.5580
Epoch 40/500
172/172 - 21s - loss: 0.0481 - accuracy: 0.9832 - val_loss: 1.6576 - val_accuracy: 0.5595
Epoch 41/500
172/172 - 21s - loss: 0.0361 - accuracy: 0.9878 - val_loss: 1.7076 - val_accuracy: 0.5372
Epoch 42/500
172/172 - 21s - loss: 0.0349 - accuracy: 0.9871 - val_loss: 1.6391 - val_accuracy: 0.5402
Epoch 43/500
172/172 - 21s - loss: 0.0279 - accuracy: 0.9902 - val_loss: 1.6050 - val_accuracy: 0.5506
========================================
save_weights
h5_weights/LI11.po/onehot_resnet18.h5
========================================

end time >>> Sun Oct  3 19:14:08 2021

end time >>> Sun Oct  3 19:14:08 2021

end time >>> Sun Oct  3 19:14:08 2021

end time >>> Sun Oct  3 19:14:08 2021

end time >>> Sun Oct  3 19:14:08 2021












args.model = onehot_resnet18
time used = 910.3889625072479


