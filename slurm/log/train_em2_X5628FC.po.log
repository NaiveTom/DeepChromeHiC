************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 18:00:45 2021

begin time >>> Mon Oct  4 18:00:45 2021

begin time >>> Mon Oct  4 18:00:45 2021

begin time >>> Mon Oct  4 18:00:45 2021

begin time >>> Mon Oct  4 18:00:45 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_dense
args.type = train
args.name = X5628FC.po
args.length = 10001
===========================


-> h5_weights/X5628FC.po folder already exist. pass.
-> result/X5628FC.po/onehot_cnn_one_branch folder already exist. pass.
-> result/X5628FC.po/onehot_cnn_two_branch folder already exist. pass.
-> result/X5628FC.po/onehot_embedding_dense folder already exist. pass.
-> result/X5628FC.po/onehot_dense folder already exist. pass.
-> result/X5628FC.po/onehot_resnet18 folder already exist. pass.
-> result/X5628FC.po/onehot_resnet34 folder already exist. pass.
-> result/X5628FC.po/embedding_cnn_one_branch folder already exist. pass.
-> result/X5628FC.po/embedding_cnn_two_branch folder already exist. pass.
-> result/X5628FC.po/embedding_dense folder already exist. pass.
-> result/X5628FC.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/X5628FC.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
X5628FC.po
########################################

########################################
model_name
onehot_embedding_dense
########################################

Found 28654 images belonging to 2 classes.
Found 3540 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 20002, 5, 1, 8)    32776     
_________________________________________________________________
flatten (Flatten)            (None, 800080)            0         
_________________________________________________________________
batch_normalization (BatchNo (None, 800080)            3200320   
_________________________________________________________________
dense (Dense)                (None, 512)               409641472 
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_3 (Batch (None, 512)               2048      
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_3 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 413,671,754
Trainable params: 412,067,498
Non-trainable params: 1,604,256
_________________________________________________________________
Epoch 1/500
895/895 - 879s - loss: 0.6724 - accuracy: 0.6309 - val_loss: 0.7083 - val_accuracy: 0.6017
Epoch 2/500
895/895 - 167s - loss: 0.5161 - accuracy: 0.7558 - val_loss: 1.1357 - val_accuracy: 0.5577
Epoch 3/500
895/895 - 166s - loss: 0.4061 - accuracy: 0.8202 - val_loss: 1.4052 - val_accuracy: 0.5707
Epoch 4/500
895/895 - 167s - loss: 0.3250 - accuracy: 0.8597 - val_loss: 1.6703 - val_accuracy: 0.5741
Epoch 5/500
895/895 - 168s - loss: 0.2756 - accuracy: 0.8853 - val_loss: 1.7627 - val_accuracy: 0.6014
Epoch 6/500
895/895 - 170s - loss: 0.2393 - accuracy: 0.9005 - val_loss: 1.7897 - val_accuracy: 0.6105
Epoch 7/500
895/895 - 169s - loss: 0.2029 - accuracy: 0.9183 - val_loss: 1.8326 - val_accuracy: 0.6165
Epoch 8/500
895/895 - 167s - loss: 0.1749 - accuracy: 0.9294 - val_loss: 1.8694 - val_accuracy: 0.6153
Epoch 9/500
895/895 - 168s - loss: 0.1521 - accuracy: 0.9400 - val_loss: 1.9931 - val_accuracy: 0.6205
Epoch 10/500
895/895 - 171s - loss: 0.1340 - accuracy: 0.9458 - val_loss: 1.9532 - val_accuracy: 0.6301
Epoch 11/500
895/895 - 170s - loss: 0.1228 - accuracy: 0.9530 - val_loss: 1.9519 - val_accuracy: 0.6355
Epoch 12/500
895/895 - 169s - loss: 0.1082 - accuracy: 0.9585 - val_loss: 1.9228 - val_accuracy: 0.6409
Epoch 13/500
895/895 - 168s - loss: 0.0913 - accuracy: 0.9662 - val_loss: 2.0657 - val_accuracy: 0.6307
Epoch 14/500
895/895 - 170s - loss: 0.0912 - accuracy: 0.9655 - val_loss: 1.9458 - val_accuracy: 0.6460
Epoch 15/500
895/895 - 167s - loss: 0.0779 - accuracy: 0.9712 - val_loss: 1.9497 - val_accuracy: 0.6355
Epoch 16/500
895/895 - 169s - loss: 0.0784 - accuracy: 0.9720 - val_loss: 1.8527 - val_accuracy: 0.6423
Epoch 17/500
895/895 - 169s - loss: 0.0694 - accuracy: 0.9749 - val_loss: 1.9796 - val_accuracy: 0.6315
Epoch 18/500
895/895 - 169s - loss: 0.0601 - accuracy: 0.9778 - val_loss: 1.9088 - val_accuracy: 0.6406
Epoch 19/500
895/895 - 170s - loss: 0.0570 - accuracy: 0.9798 - val_loss: 1.9156 - val_accuracy: 0.6398
Epoch 20/500
895/895 - 169s - loss: 0.0545 - accuracy: 0.9799 - val_loss: 1.8818 - val_accuracy: 0.6463
Epoch 21/500
895/895 - 171s - loss: 0.0522 - accuracy: 0.9811 - val_loss: 1.8414 - val_accuracy: 0.6469
Epoch 22/500
895/895 - 170s - loss: 0.0425 - accuracy: 0.9843 - val_loss: 1.9143 - val_accuracy: 0.6503
Epoch 23/500
895/895 - 169s - loss: 0.0420 - accuracy: 0.9853 - val_loss: 1.9400 - val_accuracy: 0.6486
Epoch 24/500
895/895 - 170s - loss: 0.0466 - accuracy: 0.9837 - val_loss: 1.8212 - val_accuracy: 0.6537
Epoch 25/500
895/895 - 169s - loss: 0.0399 - accuracy: 0.9863 - val_loss: 1.8478 - val_accuracy: 0.6605
Epoch 26/500
895/895 - 170s - loss: 0.0346 - accuracy: 0.9879 - val_loss: 1.9023 - val_accuracy: 0.6665
Epoch 27/500
895/895 - 168s - loss: 0.0360 - accuracy: 0.9876 - val_loss: 1.8609 - val_accuracy: 0.6645
Epoch 28/500
895/895 - 166s - loss: 0.0347 - accuracy: 0.9878 - val_loss: 1.9106 - val_accuracy: 0.6651
Epoch 29/500
895/895 - 167s - loss: 0.0292 - accuracy: 0.9901 - val_loss: 1.9256 - val_accuracy: 0.6659
Epoch 30/500
895/895 - 167s - loss: 0.0309 - accuracy: 0.9896 - val_loss: 1.9362 - val_accuracy: 0.6631
Epoch 31/500
895/895 - 166s - loss: 0.0329 - accuracy: 0.9894 - val_loss: 1.8682 - val_accuracy: 0.6628
Epoch 32/500
895/895 - 166s - loss: 0.0268 - accuracy: 0.9909 - val_loss: 1.9608 - val_accuracy: 0.6659
Epoch 33/500
895/895 - 167s - loss: 0.0318 - accuracy: 0.9888 - val_loss: 1.7646 - val_accuracy: 0.6832
Epoch 34/500
895/895 - 167s - loss: 0.0225 - accuracy: 0.9930 - val_loss: 1.9627 - val_accuracy: 0.6645
Epoch 35/500
895/895 - 167s - loss: 0.0241 - accuracy: 0.9920 - val_loss: 1.9218 - val_accuracy: 0.6702
Epoch 36/500
895/895 - 167s - loss: 0.0234 - accuracy: 0.9926 - val_loss: 1.9453 - val_accuracy: 0.6730
Epoch 37/500
895/895 - 166s - loss: 0.0222 - accuracy: 0.9925 - val_loss: 1.9245 - val_accuracy: 0.6733
Epoch 38/500
895/895 - 167s - loss: 0.0250 - accuracy: 0.9918 - val_loss: 1.9525 - val_accuracy: 0.6790
Epoch 39/500
895/895 - 167s - loss: 0.0243 - accuracy: 0.9921 - val_loss: 1.9129 - val_accuracy: 0.6733
Epoch 40/500
895/895 - 168s - loss: 0.0203 - accuracy: 0.9933 - val_loss: 1.8907 - val_accuracy: 0.6759
Epoch 41/500
895/895 - 166s - loss: 0.0216 - accuracy: 0.9933 - val_loss: 1.9548 - val_accuracy: 0.6801
Epoch 42/500
895/895 - 168s - loss: 0.0198 - accuracy: 0.9939 - val_loss: 1.9759 - val_accuracy: 0.6759
Epoch 43/500
895/895 - 168s - loss: 0.0188 - accuracy: 0.9942 - val_loss: 1.9449 - val_accuracy: 0.6736
========================================
save_weights
h5_weights/X5628FC.po/onehot_embedding_dense.h5
========================================

end time >>> Mon Oct  4 20:13:50 2021

end time >>> Mon Oct  4 20:13:50 2021

end time >>> Mon Oct  4 20:13:50 2021

end time >>> Mon Oct  4 20:13:50 2021

end time >>> Mon Oct  4 20:13:50 2021












args.model = onehot_embedding_dense
time used = 7985.496876716614


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 20:13:52 2021

begin time >>> Mon Oct  4 20:13:52 2021

begin time >>> Mon Oct  4 20:13:52 2021

begin time >>> Mon Oct  4 20:13:52 2021

begin time >>> Mon Oct  4 20:13:52 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_one_branch
args.type = train
args.name = X5628FC.po
args.length = 10001
===========================


-> h5_weights/X5628FC.po folder already exist. pass.
-> result/X5628FC.po/onehot_cnn_one_branch folder already exist. pass.
-> result/X5628FC.po/onehot_cnn_two_branch folder already exist. pass.
-> result/X5628FC.po/onehot_embedding_dense folder already exist. pass.
-> result/X5628FC.po/onehot_dense folder already exist. pass.
-> result/X5628FC.po/onehot_resnet18 folder already exist. pass.
-> result/X5628FC.po/onehot_resnet34 folder already exist. pass.
-> result/X5628FC.po/embedding_cnn_one_branch folder already exist. pass.
-> result/X5628FC.po/embedding_cnn_two_branch folder already exist. pass.
-> result/X5628FC.po/embedding_dense folder already exist. pass.
-> result/X5628FC.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/X5628FC.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
X5628FC.po
########################################

########################################
model_name
onehot_embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
896/896 - 124s - loss: 0.8603 - accuracy: 0.4990 - val_loss: 0.7081 - val_accuracy: 0.5093
Epoch 2/500
896/896 - 123s - loss: 0.8410 - accuracy: 0.5088 - val_loss: 0.7022 - val_accuracy: 0.5212
Epoch 3/500
896/896 - 123s - loss: 0.8311 - accuracy: 0.5109 - val_loss: 0.6960 - val_accuracy: 0.5367
Epoch 4/500
896/896 - 123s - loss: 0.8160 - accuracy: 0.5188 - val_loss: 0.6915 - val_accuracy: 0.5486
Epoch 5/500
896/896 - 123s - loss: 0.8041 - accuracy: 0.5302 - val_loss: 0.6876 - val_accuracy: 0.5519
Epoch 6/500
896/896 - 123s - loss: 0.7940 - accuracy: 0.5338 - val_loss: 0.6830 - val_accuracy: 0.5615
Epoch 7/500
896/896 - 123s - loss: 0.7885 - accuracy: 0.5371 - val_loss: 0.6799 - val_accuracy: 0.5686
Epoch 8/500
896/896 - 123s - loss: 0.7749 - accuracy: 0.5460 - val_loss: 0.6761 - val_accuracy: 0.5745
Epoch 9/500
896/896 - 123s - loss: 0.7661 - accuracy: 0.5487 - val_loss: 0.6722 - val_accuracy: 0.5839
Epoch 10/500
896/896 - 123s - loss: 0.7659 - accuracy: 0.5494 - val_loss: 0.6692 - val_accuracy: 0.5920
Epoch 11/500
896/896 - 123s - loss: 0.7572 - accuracy: 0.5584 - val_loss: 0.6659 - val_accuracy: 0.5937
Epoch 12/500
896/896 - 123s - loss: 0.7506 - accuracy: 0.5635 - val_loss: 0.6620 - val_accuracy: 0.5968
Epoch 13/500
896/896 - 123s - loss: 0.7423 - accuracy: 0.5680 - val_loss: 0.6587 - val_accuracy: 0.6047
Epoch 14/500
896/896 - 123s - loss: 0.7358 - accuracy: 0.5712 - val_loss: 0.6555 - val_accuracy: 0.6121
Epoch 15/500
896/896 - 123s - loss: 0.7236 - accuracy: 0.5813 - val_loss: 0.6517 - val_accuracy: 0.6189
Epoch 16/500
896/896 - 123s - loss: 0.7170 - accuracy: 0.5914 - val_loss: 0.6486 - val_accuracy: 0.6234
Epoch 17/500
896/896 - 123s - loss: 0.7100 - accuracy: 0.5935 - val_loss: 0.6443 - val_accuracy: 0.6304
Epoch 18/500
896/896 - 123s - loss: 0.7024 - accuracy: 0.5991 - val_loss: 0.6415 - val_accuracy: 0.6341
Epoch 19/500
896/896 - 123s - loss: 0.6893 - accuracy: 0.6125 - val_loss: 0.6379 - val_accuracy: 0.6409
Epoch 20/500
896/896 - 123s - loss: 0.6801 - accuracy: 0.6190 - val_loss: 0.6342 - val_accuracy: 0.6403
Epoch 21/500
896/896 - 123s - loss: 0.6755 - accuracy: 0.6246 - val_loss: 0.6309 - val_accuracy: 0.6454
Epoch 22/500
896/896 - 123s - loss: 0.6634 - accuracy: 0.6352 - val_loss: 0.6275 - val_accuracy: 0.6510
Epoch 23/500
896/896 - 122s - loss: 0.6612 - accuracy: 0.6362 - val_loss: 0.6241 - val_accuracy: 0.6561
Epoch 24/500
896/896 - 122s - loss: 0.6483 - accuracy: 0.6506 - val_loss: 0.6215 - val_accuracy: 0.6584
Epoch 25/500
896/896 - 122s - loss: 0.6353 - accuracy: 0.6564 - val_loss: 0.6185 - val_accuracy: 0.6629
Epoch 26/500
896/896 - 123s - loss: 0.6204 - accuracy: 0.6683 - val_loss: 0.6158 - val_accuracy: 0.6637
Epoch 27/500
896/896 - 123s - loss: 0.6181 - accuracy: 0.6764 - val_loss: 0.6128 - val_accuracy: 0.6677
Epoch 28/500
896/896 - 122s - loss: 0.6049 - accuracy: 0.6833 - val_loss: 0.6110 - val_accuracy: 0.6705
Epoch 29/500
896/896 - 123s - loss: 0.5945 - accuracy: 0.6902 - val_loss: 0.6091 - val_accuracy: 0.6739
Epoch 30/500
896/896 - 123s - loss: 0.5866 - accuracy: 0.6972 - val_loss: 0.6071 - val_accuracy: 0.6767
Epoch 31/500
896/896 - 123s - loss: 0.5803 - accuracy: 0.7031 - val_loss: 0.6045 - val_accuracy: 0.6787
Epoch 32/500
896/896 - 123s - loss: 0.5712 - accuracy: 0.7087 - val_loss: 0.6031 - val_accuracy: 0.6813
Epoch 33/500
896/896 - 123s - loss: 0.5603 - accuracy: 0.7182 - val_loss: 0.6026 - val_accuracy: 0.6846
Epoch 34/500
896/896 - 123s - loss: 0.5500 - accuracy: 0.7287 - val_loss: 0.6016 - val_accuracy: 0.6872
Epoch 35/500
896/896 - 123s - loss: 0.5404 - accuracy: 0.7339 - val_loss: 0.6003 - val_accuracy: 0.6869
Epoch 36/500
896/896 - 122s - loss: 0.5304 - accuracy: 0.7387 - val_loss: 0.5994 - val_accuracy: 0.6900
Epoch 37/500
896/896 - 123s - loss: 0.5195 - accuracy: 0.7453 - val_loss: 0.5983 - val_accuracy: 0.6945
Epoch 38/500
896/896 - 123s - loss: 0.5141 - accuracy: 0.7500 - val_loss: 0.5991 - val_accuracy: 0.6965
Epoch 39/500
896/896 - 123s - loss: 0.5064 - accuracy: 0.7542 - val_loss: 0.5986 - val_accuracy: 0.6942
Epoch 40/500
896/896 - 123s - loss: 0.4965 - accuracy: 0.7638 - val_loss: 0.5991 - val_accuracy: 0.6979
Epoch 41/500
896/896 - 123s - loss: 0.4876 - accuracy: 0.7666 - val_loss: 0.5997 - val_accuracy: 0.6971
Epoch 42/500
896/896 - 123s - loss: 0.4799 - accuracy: 0.7733 - val_loss: 0.6000 - val_accuracy: 0.7007
Epoch 43/500
896/896 - 122s - loss: 0.4719 - accuracy: 0.7761 - val_loss: 0.5998 - val_accuracy: 0.7007
Epoch 44/500
896/896 - 122s - loss: 0.4633 - accuracy: 0.7836 - val_loss: 0.6005 - val_accuracy: 0.7021
Epoch 45/500
896/896 - 123s - loss: 0.4565 - accuracy: 0.7849 - val_loss: 0.6013 - val_accuracy: 0.7030
Epoch 46/500
896/896 - 123s - loss: 0.4485 - accuracy: 0.7932 - val_loss: 0.6025 - val_accuracy: 0.7064
Epoch 47/500
896/896 - 122s - loss: 0.4395 - accuracy: 0.7947 - val_loss: 0.6041 - val_accuracy: 0.7053
Epoch 48/500
896/896 - 122s - loss: 0.4350 - accuracy: 0.8013 - val_loss: 0.6050 - val_accuracy: 0.7081
Epoch 49/500
896/896 - 122s - loss: 0.4276 - accuracy: 0.8047 - val_loss: 0.6074 - val_accuracy: 0.7078
Epoch 50/500
896/896 - 122s - loss: 0.4201 - accuracy: 0.8096 - val_loss: 0.6079 - val_accuracy: 0.7084
Epoch 51/500
896/896 - 123s - loss: 0.4056 - accuracy: 0.8165 - val_loss: 0.6097 - val_accuracy: 0.7050
Epoch 52/500
896/896 - 122s - loss: 0.3960 - accuracy: 0.8211 - val_loss: 0.6118 - val_accuracy: 0.7089
Epoch 53/500
896/896 - 122s - loss: 0.3973 - accuracy: 0.8224 - val_loss: 0.6144 - val_accuracy: 0.7081
Epoch 54/500
896/896 - 123s - loss: 0.3886 - accuracy: 0.8253 - val_loss: 0.6157 - val_accuracy: 0.7075
Epoch 55/500
896/896 - 122s - loss: 0.3804 - accuracy: 0.8295 - val_loss: 0.6191 - val_accuracy: 0.7092
Epoch 56/500
896/896 - 123s - loss: 0.3743 - accuracy: 0.8336 - val_loss: 0.6212 - val_accuracy: 0.7086
Epoch 57/500
896/896 - 123s - loss: 0.3659 - accuracy: 0.8362 - val_loss: 0.6240 - val_accuracy: 0.7072
Epoch 58/500
896/896 - 122s - loss: 0.3642 - accuracy: 0.8387 - val_loss: 0.6257 - val_accuracy: 0.7112
Epoch 59/500
896/896 - 122s - loss: 0.3532 - accuracy: 0.8457 - val_loss: 0.6300 - val_accuracy: 0.7098
Epoch 60/500
896/896 - 122s - loss: 0.3468 - accuracy: 0.8480 - val_loss: 0.6325 - val_accuracy: 0.7098
Epoch 61/500
896/896 - 122s - loss: 0.3434 - accuracy: 0.8498 - val_loss: 0.6358 - val_accuracy: 0.7115
Epoch 62/500
896/896 - 122s - loss: 0.3364 - accuracy: 0.8534 - val_loss: 0.6357 - val_accuracy: 0.7084
Epoch 63/500
896/896 - 122s - loss: 0.3276 - accuracy: 0.8575 - val_loss: 0.6398 - val_accuracy: 0.7109
Epoch 64/500
896/896 - 123s - loss: 0.3284 - accuracy: 0.8569 - val_loss: 0.6450 - val_accuracy: 0.7154
Epoch 65/500
896/896 - 123s - loss: 0.3181 - accuracy: 0.8662 - val_loss: 0.6466 - val_accuracy: 0.7132
Epoch 66/500
896/896 - 122s - loss: 0.3104 - accuracy: 0.8666 - val_loss: 0.6517 - val_accuracy: 0.7151
Epoch 67/500
896/896 - 122s - loss: 0.3056 - accuracy: 0.8696 - val_loss: 0.6525 - val_accuracy: 0.7146
Epoch 68/500
896/896 - 122s - loss: 0.3029 - accuracy: 0.8700 - val_loss: 0.6548 - val_accuracy: 0.7140
Epoch 69/500
896/896 - 123s - loss: 0.2924 - accuracy: 0.8746 - val_loss: 0.6599 - val_accuracy: 0.7151
Epoch 70/500
896/896 - 123s - loss: 0.2881 - accuracy: 0.8779 - val_loss: 0.6641 - val_accuracy: 0.7151
Epoch 71/500
896/896 - 122s - loss: 0.2840 - accuracy: 0.8785 - val_loss: 0.6676 - val_accuracy: 0.7154
Epoch 72/500
896/896 - 123s - loss: 0.2809 - accuracy: 0.8801 - val_loss: 0.6697 - val_accuracy: 0.7163
Epoch 73/500
896/896 - 123s - loss: 0.2784 - accuracy: 0.8819 - val_loss: 0.6742 - val_accuracy: 0.7171
Epoch 74/500
896/896 - 122s - loss: 0.2698 - accuracy: 0.8848 - val_loss: 0.6782 - val_accuracy: 0.7157
Epoch 75/500
896/896 - 123s - loss: 0.2676 - accuracy: 0.8873 - val_loss: 0.6822 - val_accuracy: 0.7168
Epoch 76/500
896/896 - 122s - loss: 0.2623 - accuracy: 0.8907 - val_loss: 0.6852 - val_accuracy: 0.7165
Epoch 77/500
896/896 - 122s - loss: 0.2571 - accuracy: 0.8919 - val_loss: 0.6893 - val_accuracy: 0.7163
Epoch 78/500
896/896 - 123s - loss: 0.2482 - accuracy: 0.8961 - val_loss: 0.6920 - val_accuracy: 0.7151
Epoch 79/500
896/896 - 122s - loss: 0.2448 - accuracy: 0.8976 - val_loss: 0.6966 - val_accuracy: 0.7160
Epoch 80/500
896/896 - 123s - loss: 0.2406 - accuracy: 0.8991 - val_loss: 0.7005 - val_accuracy: 0.7140
Epoch 81/500
896/896 - 122s - loss: 0.2361 - accuracy: 0.9022 - val_loss: 0.7070 - val_accuracy: 0.7171
Epoch 82/500
896/896 - 122s - loss: 0.2302 - accuracy: 0.9066 - val_loss: 0.7096 - val_accuracy: 0.7171
Epoch 83/500
896/896 - 123s - loss: 0.2246 - accuracy: 0.9072 - val_loss: 0.7154 - val_accuracy: 0.7174
Epoch 84/500
896/896 - 122s - loss: 0.2247 - accuracy: 0.9063 - val_loss: 0.7211 - val_accuracy: 0.7185
Epoch 85/500
896/896 - 122s - loss: 0.2188 - accuracy: 0.9081 - val_loss: 0.7216 - val_accuracy: 0.7140
Epoch 86/500
896/896 - 123s - loss: 0.2133 - accuracy: 0.9119 - val_loss: 0.7273 - val_accuracy: 0.7188
Epoch 87/500
896/896 - 122s - loss: 0.2144 - accuracy: 0.9104 - val_loss: 0.7322 - val_accuracy: 0.7180
Epoch 88/500
896/896 - 123s - loss: 0.2066 - accuracy: 0.9152 - val_loss: 0.7363 - val_accuracy: 0.7182
Epoch 89/500
896/896 - 123s - loss: 0.2097 - accuracy: 0.9157 - val_loss: 0.7401 - val_accuracy: 0.7163
Epoch 90/500
896/896 - 122s - loss: 0.2098 - accuracy: 0.9149 - val_loss: 0.7445 - val_accuracy: 0.7160
Epoch 91/500
896/896 - 123s - loss: 0.1981 - accuracy: 0.9201 - val_loss: 0.7478 - val_accuracy: 0.7157
Epoch 92/500
896/896 - 122s - loss: 0.1974 - accuracy: 0.9194 - val_loss: 0.7509 - val_accuracy: 0.7180
Epoch 93/500
896/896 - 122s - loss: 0.1929 - accuracy: 0.9218 - val_loss: 0.7545 - val_accuracy: 0.7146
Epoch 94/500
896/896 - 122s - loss: 0.1840 - accuracy: 0.9255 - val_loss: 0.7583 - val_accuracy: 0.7168
Epoch 95/500
896/896 - 122s - loss: 0.1854 - accuracy: 0.9250 - val_loss: 0.7616 - val_accuracy: 0.7174
Epoch 96/500
896/896 - 122s - loss: 0.1832 - accuracy: 0.9260 - val_loss: 0.7672 - val_accuracy: 0.7182
Epoch 97/500
896/896 - 123s - loss: 0.1803 - accuracy: 0.9279 - val_loss: 0.7712 - val_accuracy: 0.7143
Epoch 98/500
896/896 - 122s - loss: 0.1809 - accuracy: 0.9273 - val_loss: 0.7742 - val_accuracy: 0.7143
Epoch 99/500
896/896 - 123s - loss: 0.1733 - accuracy: 0.9308 - val_loss: 0.7821 - val_accuracy: 0.7140
Epoch 100/500
896/896 - 122s - loss: 0.1683 - accuracy: 0.9320 - val_loss: 0.7881 - val_accuracy: 0.7154
Epoch 101/500
896/896 - 122s - loss: 0.1663 - accuracy: 0.9335 - val_loss: 0.7903 - val_accuracy: 0.7137
Epoch 102/500
896/896 - 123s - loss: 0.1679 - accuracy: 0.9335 - val_loss: 0.7944 - val_accuracy: 0.7140
Epoch 103/500
896/896 - 122s - loss: 0.1680 - accuracy: 0.9325 - val_loss: 0.7960 - val_accuracy: 0.7137
Epoch 104/500
896/896 - 123s - loss: 0.1640 - accuracy: 0.9355 - val_loss: 0.8015 - val_accuracy: 0.7211
Epoch 105/500
896/896 - 122s - loss: 0.1586 - accuracy: 0.9373 - val_loss: 0.8051 - val_accuracy: 0.7151
Epoch 106/500
896/896 - 122s - loss: 0.1617 - accuracy: 0.9369 - val_loss: 0.8088 - val_accuracy: 0.7134
Epoch 107/500
896/896 - 121s - loss: 0.1609 - accuracy: 0.9358 - val_loss: 0.8142 - val_accuracy: 0.7163
Epoch 108/500
896/896 - 119s - loss: 0.1530 - accuracy: 0.9400 - val_loss: 0.8151 - val_accuracy: 0.7177
Epoch 109/500
896/896 - 119s - loss: 0.1566 - accuracy: 0.9382 - val_loss: 0.8176 - val_accuracy: 0.7182
Epoch 110/500
896/896 - 119s - loss: 0.1470 - accuracy: 0.9412 - val_loss: 0.8219 - val_accuracy: 0.7137
Epoch 111/500
896/896 - 119s - loss: 0.1462 - accuracy: 0.9435 - val_loss: 0.8242 - val_accuracy: 0.7191
Epoch 112/500
896/896 - 119s - loss: 0.1473 - accuracy: 0.9416 - val_loss: 0.8296 - val_accuracy: 0.7168
Epoch 113/500
896/896 - 119s - loss: 0.1434 - accuracy: 0.9436 - val_loss: 0.8328 - val_accuracy: 0.7154
Epoch 114/500
896/896 - 119s - loss: 0.1393 - accuracy: 0.9459 - val_loss: 0.8356 - val_accuracy: 0.7163
Epoch 115/500
896/896 - 119s - loss: 0.1365 - accuracy: 0.9457 - val_loss: 0.8402 - val_accuracy: 0.7208
Epoch 116/500
896/896 - 119s - loss: 0.1376 - accuracy: 0.9467 - val_loss: 0.8440 - val_accuracy: 0.7219
Epoch 117/500
896/896 - 119s - loss: 0.1366 - accuracy: 0.9472 - val_loss: 0.8489 - val_accuracy: 0.7174
Epoch 118/500
896/896 - 119s - loss: 0.1329 - accuracy: 0.9489 - val_loss: 0.8521 - val_accuracy: 0.7191
Epoch 119/500
896/896 - 119s - loss: 0.1284 - accuracy: 0.9497 - val_loss: 0.8540 - val_accuracy: 0.7177
Epoch 120/500
896/896 - 119s - loss: 0.1295 - accuracy: 0.9495 - val_loss: 0.8569 - val_accuracy: 0.7177
Epoch 121/500
896/896 - 119s - loss: 0.1267 - accuracy: 0.9518 - val_loss: 0.8624 - val_accuracy: 0.7174
Epoch 122/500
896/896 - 119s - loss: 0.1265 - accuracy: 0.9515 - val_loss: 0.8642 - val_accuracy: 0.7165
Epoch 123/500
896/896 - 119s - loss: 0.1338 - accuracy: 0.9478 - val_loss: 0.8702 - val_accuracy: 0.7188
Epoch 124/500
896/896 - 119s - loss: 0.1215 - accuracy: 0.9535 - val_loss: 0.8755 - val_accuracy: 0.7180
Epoch 125/500
896/896 - 119s - loss: 0.1218 - accuracy: 0.9534 - val_loss: 0.8761 - val_accuracy: 0.7174
Epoch 126/500
896/896 - 119s - loss: 0.1201 - accuracy: 0.9528 - val_loss: 0.8786 - val_accuracy: 0.7199
Epoch 127/500
896/896 - 119s - loss: 0.1185 - accuracy: 0.9550 - val_loss: 0.8819 - val_accuracy: 0.7182
Epoch 128/500
896/896 - 119s - loss: 0.1187 - accuracy: 0.9556 - val_loss: 0.8856 - val_accuracy: 0.7177
Epoch 129/500
896/896 - 119s - loss: 0.1213 - accuracy: 0.9525 - val_loss: 0.8907 - val_accuracy: 0.7177
Epoch 130/500
896/896 - 119s - loss: 0.1176 - accuracy: 0.9552 - val_loss: 0.8947 - val_accuracy: 0.7194
Epoch 131/500
896/896 - 119s - loss: 0.1123 - accuracy: 0.9570 - val_loss: 0.8928 - val_accuracy: 0.7168
Epoch 132/500
896/896 - 119s - loss: 0.1146 - accuracy: 0.9557 - val_loss: 0.8983 - val_accuracy: 0.7177
Epoch 133/500
896/896 - 119s - loss: 0.1113 - accuracy: 0.9587 - val_loss: 0.9049 - val_accuracy: 0.7185
Epoch 134/500
896/896 - 119s - loss: 0.1118 - accuracy: 0.9576 - val_loss: 0.9032 - val_accuracy: 0.7165
Epoch 135/500
896/896 - 119s - loss: 0.1093 - accuracy: 0.9586 - val_loss: 0.9064 - val_accuracy: 0.7168
Epoch 136/500
896/896 - 119s - loss: 0.1109 - accuracy: 0.9582 - val_loss: 0.9123 - val_accuracy: 0.7180
========================================
save_weights
h5_weights/X5628FC.po/onehot_embedding_cnn_one_branch.h5
========================================

end time >>> Tue Oct  5 00:50:55 2021

end time >>> Tue Oct  5 00:50:55 2021

end time >>> Tue Oct  5 00:50:55 2021

end time >>> Tue Oct  5 00:50:55 2021

end time >>> Tue Oct  5 00:50:55 2021












args.model = onehot_embedding_cnn_one_branch
time used = 16622.891624450684


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Tue Oct  5 00:50:57 2021

begin time >>> Tue Oct  5 00:50:57 2021

begin time >>> Tue Oct  5 00:50:57 2021

begin time >>> Tue Oct  5 00:50:57 2021

begin time >>> Tue Oct  5 00:50:57 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_two_branch
args.type = train
args.name = X5628FC.po
args.length = 10001
===========================


-> h5_weights/X5628FC.po folder already exist. pass.
-> result/X5628FC.po/onehot_cnn_one_branch folder already exist. pass.
-> result/X5628FC.po/onehot_cnn_two_branch folder already exist. pass.
-> result/X5628FC.po/onehot_embedding_dense folder already exist. pass.
-> result/X5628FC.po/onehot_dense folder already exist. pass.
-> result/X5628FC.po/onehot_resnet18 folder already exist. pass.
-> result/X5628FC.po/onehot_resnet34 folder already exist. pass.
-> result/X5628FC.po/embedding_cnn_one_branch folder already exist. pass.
-> result/X5628FC.po/embedding_cnn_two_branch folder already exist. pass.
-> result/X5628FC.po/embedding_dense folder already exist. pass.
-> result/X5628FC.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/X5628FC.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
X5628FC.po
########################################

########################################
model_name
onehot_embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
896/896 - 119s - loss: 0.8711 - accuracy: 0.5022 - val_loss: 0.7043 - val_accuracy: 0.5082
Epoch 2/500
896/896 - 118s - loss: 0.8396 - accuracy: 0.5057 - val_loss: 0.6978 - val_accuracy: 0.5226
Epoch 3/500
896/896 - 118s - loss: 0.8216 - accuracy: 0.5153 - val_loss: 0.6920 - val_accuracy: 0.5381
Epoch 4/500
896/896 - 118s - loss: 0.8109 - accuracy: 0.5185 - val_loss: 0.6875 - val_accuracy: 0.5500
Epoch 5/500
896/896 - 118s - loss: 0.8003 - accuracy: 0.5256 - val_loss: 0.6834 - val_accuracy: 0.5551
Epoch 6/500
896/896 - 118s - loss: 0.7871 - accuracy: 0.5337 - val_loss: 0.6794 - val_accuracy: 0.5683
Epoch 7/500
896/896 - 118s - loss: 0.7744 - accuracy: 0.5417 - val_loss: 0.6758 - val_accuracy: 0.5788
Epoch 8/500
896/896 - 118s - loss: 0.7750 - accuracy: 0.5377 - val_loss: 0.6720 - val_accuracy: 0.5872
Epoch 9/500
896/896 - 118s - loss: 0.7625 - accuracy: 0.5506 - val_loss: 0.6692 - val_accuracy: 0.5949
Epoch 10/500
896/896 - 118s - loss: 0.7639 - accuracy: 0.5484 - val_loss: 0.6656 - val_accuracy: 0.6022
Epoch 11/500
896/896 - 118s - loss: 0.7488 - accuracy: 0.5590 - val_loss: 0.6621 - val_accuracy: 0.6126
Epoch 12/500
896/896 - 118s - loss: 0.7452 - accuracy: 0.5628 - val_loss: 0.6586 - val_accuracy: 0.6177
Epoch 13/500
896/896 - 118s - loss: 0.7307 - accuracy: 0.5725 - val_loss: 0.6550 - val_accuracy: 0.6186
Epoch 14/500
896/896 - 118s - loss: 0.7267 - accuracy: 0.5773 - val_loss: 0.6516 - val_accuracy: 0.6282
Epoch 15/500
896/896 - 118s - loss: 0.7158 - accuracy: 0.5836 - val_loss: 0.6479 - val_accuracy: 0.6350
Epoch 16/500
896/896 - 118s - loss: 0.7030 - accuracy: 0.5971 - val_loss: 0.6441 - val_accuracy: 0.6352
Epoch 17/500
896/896 - 118s - loss: 0.7027 - accuracy: 0.5986 - val_loss: 0.6398 - val_accuracy: 0.6431
Epoch 18/500
896/896 - 118s - loss: 0.6885 - accuracy: 0.6104 - val_loss: 0.6357 - val_accuracy: 0.6443
Epoch 19/500
896/896 - 118s - loss: 0.6824 - accuracy: 0.6155 - val_loss: 0.6314 - val_accuracy: 0.6505
Epoch 20/500
896/896 - 118s - loss: 0.6673 - accuracy: 0.6276 - val_loss: 0.6277 - val_accuracy: 0.6544
Epoch 21/500
896/896 - 118s - loss: 0.6619 - accuracy: 0.6321 - val_loss: 0.6234 - val_accuracy: 0.6575
Epoch 22/500
896/896 - 118s - loss: 0.6500 - accuracy: 0.6446 - val_loss: 0.6198 - val_accuracy: 0.6601
Epoch 23/500
896/896 - 118s - loss: 0.6391 - accuracy: 0.6530 - val_loss: 0.6157 - val_accuracy: 0.6629
Epoch 24/500
896/896 - 118s - loss: 0.6283 - accuracy: 0.6617 - val_loss: 0.6122 - val_accuracy: 0.6660
Epoch 25/500
896/896 - 118s - loss: 0.6188 - accuracy: 0.6707 - val_loss: 0.6085 - val_accuracy: 0.6697
Epoch 26/500
896/896 - 118s - loss: 0.6131 - accuracy: 0.6749 - val_loss: 0.6055 - val_accuracy: 0.6779
Epoch 27/500
896/896 - 118s - loss: 0.5999 - accuracy: 0.6859 - val_loss: 0.6024 - val_accuracy: 0.6810
Epoch 28/500
896/896 - 118s - loss: 0.5820 - accuracy: 0.7002 - val_loss: 0.5996 - val_accuracy: 0.6835
Epoch 29/500
896/896 - 118s - loss: 0.5751 - accuracy: 0.7051 - val_loss: 0.5972 - val_accuracy: 0.6863
Epoch 30/500
896/896 - 118s - loss: 0.5661 - accuracy: 0.7138 - val_loss: 0.5951 - val_accuracy: 0.6892
Epoch 31/500
896/896 - 118s - loss: 0.5548 - accuracy: 0.7185 - val_loss: 0.5939 - val_accuracy: 0.6897
Epoch 32/500
896/896 - 118s - loss: 0.5432 - accuracy: 0.7273 - val_loss: 0.5921 - val_accuracy: 0.6914
Epoch 33/500
896/896 - 118s - loss: 0.5384 - accuracy: 0.7332 - val_loss: 0.5907 - val_accuracy: 0.6931
Epoch 34/500
896/896 - 118s - loss: 0.5231 - accuracy: 0.7430 - val_loss: 0.5889 - val_accuracy: 0.6945
Epoch 35/500
896/896 - 118s - loss: 0.5159 - accuracy: 0.7467 - val_loss: 0.5890 - val_accuracy: 0.6965
Epoch 36/500
896/896 - 118s - loss: 0.5092 - accuracy: 0.7545 - val_loss: 0.5875 - val_accuracy: 0.6976
Epoch 37/500
896/896 - 118s - loss: 0.4934 - accuracy: 0.7650 - val_loss: 0.5872 - val_accuracy: 0.7010
Epoch 38/500
896/896 - 118s - loss: 0.4844 - accuracy: 0.7679 - val_loss: 0.5876 - val_accuracy: 0.6999
Epoch 39/500
896/896 - 118s - loss: 0.4762 - accuracy: 0.7731 - val_loss: 0.5879 - val_accuracy: 0.6993
Epoch 40/500
896/896 - 118s - loss: 0.4683 - accuracy: 0.7809 - val_loss: 0.5876 - val_accuracy: 0.6993
Epoch 41/500
896/896 - 118s - loss: 0.4582 - accuracy: 0.7843 - val_loss: 0.5884 - val_accuracy: 0.7047
Epoch 42/500
896/896 - 118s - loss: 0.4507 - accuracy: 0.7920 - val_loss: 0.5887 - val_accuracy: 0.7055
Epoch 43/500
896/896 - 118s - loss: 0.4403 - accuracy: 0.7981 - val_loss: 0.5899 - val_accuracy: 0.7061
Epoch 44/500
896/896 - 118s - loss: 0.4327 - accuracy: 0.7990 - val_loss: 0.5911 - val_accuracy: 0.7081
Epoch 45/500
896/896 - 118s - loss: 0.4198 - accuracy: 0.8078 - val_loss: 0.5926 - val_accuracy: 0.7095
Epoch 46/500
896/896 - 118s - loss: 0.4168 - accuracy: 0.8084 - val_loss: 0.5934 - val_accuracy: 0.7089
Epoch 47/500
896/896 - 118s - loss: 0.4101 - accuracy: 0.8124 - val_loss: 0.5934 - val_accuracy: 0.7084
Epoch 48/500
896/896 - 118s - loss: 0.4005 - accuracy: 0.8188 - val_loss: 0.5956 - val_accuracy: 0.7098
Epoch 49/500
896/896 - 118s - loss: 0.3905 - accuracy: 0.8259 - val_loss: 0.5974 - val_accuracy: 0.7084
Epoch 50/500
896/896 - 118s - loss: 0.3856 - accuracy: 0.8280 - val_loss: 0.5998 - val_accuracy: 0.7126
Epoch 51/500
896/896 - 118s - loss: 0.3777 - accuracy: 0.8325 - val_loss: 0.6014 - val_accuracy: 0.7101
Epoch 52/500
896/896 - 118s - loss: 0.3680 - accuracy: 0.8364 - val_loss: 0.6031 - val_accuracy: 0.7123
Epoch 53/500
896/896 - 118s - loss: 0.3614 - accuracy: 0.8403 - val_loss: 0.6052 - val_accuracy: 0.7126
Epoch 54/500
896/896 - 118s - loss: 0.3494 - accuracy: 0.8461 - val_loss: 0.6075 - val_accuracy: 0.7151
Epoch 55/500
896/896 - 118s - loss: 0.3472 - accuracy: 0.8482 - val_loss: 0.6090 - val_accuracy: 0.7157
Epoch 56/500
896/896 - 118s - loss: 0.3435 - accuracy: 0.8483 - val_loss: 0.6121 - val_accuracy: 0.7157
Epoch 57/500
896/896 - 118s - loss: 0.3317 - accuracy: 0.8549 - val_loss: 0.6139 - val_accuracy: 0.7165
Epoch 58/500
896/896 - 118s - loss: 0.3251 - accuracy: 0.8583 - val_loss: 0.6189 - val_accuracy: 0.7137
Epoch 59/500
896/896 - 118s - loss: 0.3179 - accuracy: 0.8633 - val_loss: 0.6210 - val_accuracy: 0.7165
Epoch 60/500
896/896 - 118s - loss: 0.3174 - accuracy: 0.8616 - val_loss: 0.6229 - val_accuracy: 0.7174
Epoch 61/500
896/896 - 118s - loss: 0.3033 - accuracy: 0.8704 - val_loss: 0.6279 - val_accuracy: 0.7180
Epoch 62/500
896/896 - 118s - loss: 0.2968 - accuracy: 0.8735 - val_loss: 0.6299 - val_accuracy: 0.7168
Epoch 63/500
896/896 - 118s - loss: 0.2887 - accuracy: 0.8767 - val_loss: 0.6353 - val_accuracy: 0.7151
Epoch 64/500
896/896 - 118s - loss: 0.2797 - accuracy: 0.8818 - val_loss: 0.6377 - val_accuracy: 0.7151
Epoch 65/500
896/896 - 118s - loss: 0.2788 - accuracy: 0.8817 - val_loss: 0.6418 - val_accuracy: 0.7132
Epoch 66/500
896/896 - 118s - loss: 0.2759 - accuracy: 0.8819 - val_loss: 0.6449 - val_accuracy: 0.7202
Epoch 67/500
896/896 - 118s - loss: 0.2686 - accuracy: 0.8852 - val_loss: 0.6484 - val_accuracy: 0.7160
Epoch 68/500
896/896 - 118s - loss: 0.2607 - accuracy: 0.8908 - val_loss: 0.6524 - val_accuracy: 0.7151
Epoch 69/500
896/896 - 118s - loss: 0.2561 - accuracy: 0.8938 - val_loss: 0.6551 - val_accuracy: 0.7180
Epoch 70/500
896/896 - 118s - loss: 0.2473 - accuracy: 0.8989 - val_loss: 0.6598 - val_accuracy: 0.7180
Epoch 71/500
896/896 - 118s - loss: 0.2508 - accuracy: 0.8948 - val_loss: 0.6624 - val_accuracy: 0.7157
Epoch 72/500
896/896 - 118s - loss: 0.2441 - accuracy: 0.8972 - val_loss: 0.6664 - val_accuracy: 0.7151
Epoch 73/500
896/896 - 118s - loss: 0.2364 - accuracy: 0.9040 - val_loss: 0.6711 - val_accuracy: 0.7165
Epoch 74/500
896/896 - 118s - loss: 0.2306 - accuracy: 0.9043 - val_loss: 0.6737 - val_accuracy: 0.7149
Epoch 75/500
896/896 - 118s - loss: 0.2227 - accuracy: 0.9082 - val_loss: 0.6775 - val_accuracy: 0.7182
Epoch 76/500
896/896 - 118s - loss: 0.2229 - accuracy: 0.9064 - val_loss: 0.6817 - val_accuracy: 0.7157
Epoch 77/500
896/896 - 118s - loss: 0.2191 - accuracy: 0.9116 - val_loss: 0.6871 - val_accuracy: 0.7168
Epoch 78/500
896/896 - 118s - loss: 0.2139 - accuracy: 0.9117 - val_loss: 0.6917 - val_accuracy: 0.7134
Epoch 79/500
896/896 - 118s - loss: 0.2097 - accuracy: 0.9144 - val_loss: 0.6935 - val_accuracy: 0.7163
Epoch 80/500
896/896 - 118s - loss: 0.2057 - accuracy: 0.9153 - val_loss: 0.6994 - val_accuracy: 0.7146
Epoch 81/500
896/896 - 118s - loss: 0.2040 - accuracy: 0.9187 - val_loss: 0.7035 - val_accuracy: 0.7171
Epoch 82/500
896/896 - 118s - loss: 0.1954 - accuracy: 0.9206 - val_loss: 0.7095 - val_accuracy: 0.7182
Epoch 83/500
896/896 - 118s - loss: 0.1964 - accuracy: 0.9203 - val_loss: 0.7099 - val_accuracy: 0.7143
Epoch 84/500
896/896 - 118s - loss: 0.1914 - accuracy: 0.9219 - val_loss: 0.7164 - val_accuracy: 0.7160
Epoch 85/500
896/896 - 118s - loss: 0.1842 - accuracy: 0.9253 - val_loss: 0.7199 - val_accuracy: 0.7132
Epoch 86/500
896/896 - 118s - loss: 0.1820 - accuracy: 0.9258 - val_loss: 0.7261 - val_accuracy: 0.7154
========================================
save_weights
h5_weights/X5628FC.po/onehot_embedding_cnn_two_branch.h5
========================================

end time >>> Tue Oct  5 03:41:08 2021

end time >>> Tue Oct  5 03:41:08 2021

end time >>> Tue Oct  5 03:41:08 2021

end time >>> Tue Oct  5 03:41:08 2021

end time >>> Tue Oct  5 03:41:08 2021












args.model = onehot_embedding_cnn_two_branch
time used = 10210.7450735569


