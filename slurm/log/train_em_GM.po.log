************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 02:33:42 2021

begin time >>> Sun Oct  3 02:33:42 2021

begin time >>> Sun Oct  3 02:33:42 2021

begin time >>> Sun Oct  3 02:33:42 2021

begin time >>> Sun Oct  3 02:33:42 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_dense
args.type = train
args.name = GM.po
args.length = 10001
===========================


-> h5_weights/GM.po folder already exist. pass.
-> result/GM.po/onehot_cnn_one_branch folder already exist. pass.
-> result/GM.po/onehot_cnn_two_branch folder already exist. pass.
-> result/GM.po/onehot_embedding_dense folder already exist. pass.
-> result/GM.po/onehot_dense folder already exist. pass.
-> result/GM.po/onehot_resnet18 folder already exist. pass.
-> result/GM.po/onehot_resnet34 folder already exist. pass.
-> result/GM.po/embedding_cnn_one_branch folder already exist. pass.
-> result/GM.po/embedding_cnn_two_branch folder already exist. pass.
-> result/GM.po/embedding_dense folder already exist. pass.
-> result/GM.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/GM.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
GM.po
########################################

########################################
model_name
embedding_dense
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 626, 64)      409664      concatenate[0][0]                
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, 38, 64)       0           conv1d[0][0]                     
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 38, 64)       256         max_pooling1d[0][0]              
__________________________________________________________________________________________________
flatten (Flatten)               (None, 2432)         0           batch_normalization[0][0]        
__________________________________________________________________________________________________
dropout (Dropout)               (None, 2432)         0           flatten[0][0]                    
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          1245696     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation (Activation)         (None, 512)          0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation[0][0]                 
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 512)          0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_1[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 512)          2048        dense_2[0][0]                    
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 512)          0           activation_2[0][0]               
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            513         dropout_3[0][0]                  
==================================================================================================
Total params: 3,006,985
Trainable params: 3,003,785
Non-trainable params: 3,200
__________________________________________________________________________________________________
Epoch 1/500
133/133 - 17s - loss: 0.8964 - accuracy: 0.5051 - val_loss: 0.6890 - val_accuracy: 0.5458
Epoch 2/500
133/133 - 17s - loss: 0.8973 - accuracy: 0.4949 - val_loss: 0.6915 - val_accuracy: 0.5401
Epoch 3/500
133/133 - 17s - loss: 0.8899 - accuracy: 0.5060 - val_loss: 0.6960 - val_accuracy: 0.5248
Epoch 4/500
133/133 - 17s - loss: 0.8982 - accuracy: 0.4878 - val_loss: 0.6970 - val_accuracy: 0.5344
Epoch 5/500
133/133 - 17s - loss: 0.8671 - accuracy: 0.5011 - val_loss: 0.6973 - val_accuracy: 0.5286
Epoch 6/500
133/133 - 17s - loss: 0.8733 - accuracy: 0.4992 - val_loss: 0.6975 - val_accuracy: 0.5363
Epoch 7/500
133/133 - 17s - loss: 0.8580 - accuracy: 0.5013 - val_loss: 0.6970 - val_accuracy: 0.5344
Epoch 8/500
133/133 - 17s - loss: 0.8526 - accuracy: 0.5143 - val_loss: 0.6961 - val_accuracy: 0.5363
Epoch 9/500
133/133 - 17s - loss: 0.8568 - accuracy: 0.5143 - val_loss: 0.6963 - val_accuracy: 0.5401
Epoch 10/500
133/133 - 17s - loss: 0.8440 - accuracy: 0.5145 - val_loss: 0.6960 - val_accuracy: 0.5305
Epoch 11/500
133/133 - 17s - loss: 0.8637 - accuracy: 0.5022 - val_loss: 0.6948 - val_accuracy: 0.5382
Epoch 12/500
133/133 - 17s - loss: 0.8435 - accuracy: 0.5119 - val_loss: 0.6942 - val_accuracy: 0.5344
Epoch 13/500
133/133 - 17s - loss: 0.8410 - accuracy: 0.5100 - val_loss: 0.6943 - val_accuracy: 0.5382
Epoch 14/500
133/133 - 17s - loss: 0.8642 - accuracy: 0.5020 - val_loss: 0.6945 - val_accuracy: 0.5248
Epoch 15/500
133/133 - 17s - loss: 0.8403 - accuracy: 0.5046 - val_loss: 0.6938 - val_accuracy: 0.5305
Epoch 16/500
133/133 - 17s - loss: 0.8379 - accuracy: 0.5122 - val_loss: 0.6932 - val_accuracy: 0.5286
Epoch 17/500
133/133 - 17s - loss: 0.8361 - accuracy: 0.5245 - val_loss: 0.6932 - val_accuracy: 0.5229
Epoch 18/500
133/133 - 17s - loss: 0.8322 - accuracy: 0.5155 - val_loss: 0.6925 - val_accuracy: 0.5344
Epoch 19/500
133/133 - 17s - loss: 0.8246 - accuracy: 0.5171 - val_loss: 0.6919 - val_accuracy: 0.5324
Epoch 20/500
133/133 - 17s - loss: 0.8461 - accuracy: 0.5100 - val_loss: 0.6918 - val_accuracy: 0.5267
Epoch 21/500
133/133 - 17s - loss: 0.8326 - accuracy: 0.5219 - val_loss: 0.6915 - val_accuracy: 0.5344
========================================
save_weights
h5_weights/GM.po/embedding_dense.h5
========================================

end time >>> Sun Oct  3 02:39:51 2021

end time >>> Sun Oct  3 02:39:51 2021

end time >>> Sun Oct  3 02:39:51 2021

end time >>> Sun Oct  3 02:39:51 2021

end time >>> Sun Oct  3 02:39:51 2021












args.model = embedding_dense
time used = 369.139532327652


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 02:39:52 2021

begin time >>> Sun Oct  3 02:39:52 2021

begin time >>> Sun Oct  3 02:39:52 2021

begin time >>> Sun Oct  3 02:39:52 2021

begin time >>> Sun Oct  3 02:39:52 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_cnn_one_branch
args.type = train
args.name = GM.po
args.length = 10001
===========================


-> h5_weights/GM.po folder already exist. pass.
-> result/GM.po/onehot_cnn_one_branch folder already exist. pass.
-> result/GM.po/onehot_cnn_two_branch folder already exist. pass.
-> result/GM.po/onehot_embedding_dense folder already exist. pass.
-> result/GM.po/onehot_dense folder already exist. pass.
-> result/GM.po/onehot_resnet18 folder already exist. pass.
-> result/GM.po/onehot_resnet34 folder already exist. pass.
-> result/GM.po/embedding_cnn_one_branch folder already exist. pass.
-> result/GM.po/embedding_cnn_two_branch folder already exist. pass.
-> result/GM.po/embedding_dense folder already exist. pass.
-> result/GM.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/GM.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
GM.po
########################################

########################################
model_name
embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
133/133 - 18s - loss: 0.9071 - accuracy: 0.4864 - val_loss: 0.7060 - val_accuracy: 0.4561
Epoch 2/500
133/133 - 18s - loss: 0.8869 - accuracy: 0.4940 - val_loss: 0.7115 - val_accuracy: 0.4523
Epoch 3/500
133/133 - 18s - loss: 0.8670 - accuracy: 0.4952 - val_loss: 0.7174 - val_accuracy: 0.4656
Epoch 4/500
133/133 - 18s - loss: 0.8739 - accuracy: 0.4956 - val_loss: 0.7187 - val_accuracy: 0.4733
Epoch 5/500
133/133 - 18s - loss: 0.8556 - accuracy: 0.5134 - val_loss: 0.7185 - val_accuracy: 0.4771
Epoch 6/500
133/133 - 18s - loss: 0.8468 - accuracy: 0.5245 - val_loss: 0.7159 - val_accuracy: 0.4828
Epoch 7/500
133/133 - 18s - loss: 0.8531 - accuracy: 0.5162 - val_loss: 0.7133 - val_accuracy: 0.4885
Epoch 8/500
133/133 - 18s - loss: 0.8286 - accuracy: 0.5297 - val_loss: 0.7113 - val_accuracy: 0.4981
Epoch 9/500
133/133 - 18s - loss: 0.8177 - accuracy: 0.5261 - val_loss: 0.7090 - val_accuracy: 0.5019
Epoch 10/500
133/133 - 18s - loss: 0.8275 - accuracy: 0.5223 - val_loss: 0.7061 - val_accuracy: 0.4962
Epoch 11/500
133/133 - 18s - loss: 0.8254 - accuracy: 0.5297 - val_loss: 0.7044 - val_accuracy: 0.5076
Epoch 12/500
133/133 - 18s - loss: 0.8270 - accuracy: 0.5318 - val_loss: 0.7021 - val_accuracy: 0.5134
Epoch 13/500
133/133 - 18s - loss: 0.8092 - accuracy: 0.5368 - val_loss: 0.7000 - val_accuracy: 0.5191
Epoch 14/500
133/133 - 18s - loss: 0.7954 - accuracy: 0.5420 - val_loss: 0.6981 - val_accuracy: 0.5191
Epoch 15/500
133/133 - 18s - loss: 0.7990 - accuracy: 0.5462 - val_loss: 0.6958 - val_accuracy: 0.5267
Epoch 16/500
133/133 - 18s - loss: 0.7894 - accuracy: 0.5571 - val_loss: 0.6942 - val_accuracy: 0.5363
Epoch 17/500
133/133 - 18s - loss: 0.7975 - accuracy: 0.5495 - val_loss: 0.6914 - val_accuracy: 0.5477
Epoch 18/500
133/133 - 18s - loss: 0.7813 - accuracy: 0.5613 - val_loss: 0.6903 - val_accuracy: 0.5477
Epoch 19/500
133/133 - 18s - loss: 0.8046 - accuracy: 0.5412 - val_loss: 0.6891 - val_accuracy: 0.5515
Epoch 20/500
133/133 - 18s - loss: 0.7626 - accuracy: 0.5755 - val_loss: 0.6861 - val_accuracy: 0.5553
Epoch 21/500
133/133 - 18s - loss: 0.7641 - accuracy: 0.5656 - val_loss: 0.6843 - val_accuracy: 0.5611
Epoch 22/500
133/133 - 18s - loss: 0.7524 - accuracy: 0.5736 - val_loss: 0.6837 - val_accuracy: 0.5611
Epoch 23/500
133/133 - 18s - loss: 0.7625 - accuracy: 0.5691 - val_loss: 0.6819 - val_accuracy: 0.5592
Epoch 24/500
133/133 - 18s - loss: 0.7687 - accuracy: 0.5701 - val_loss: 0.6808 - val_accuracy: 0.5668
Epoch 25/500
133/133 - 18s - loss: 0.7639 - accuracy: 0.5762 - val_loss: 0.6790 - val_accuracy: 0.5706
Epoch 26/500
133/133 - 18s - loss: 0.7639 - accuracy: 0.5717 - val_loss: 0.6769 - val_accuracy: 0.5840
Epoch 27/500
133/133 - 18s - loss: 0.7388 - accuracy: 0.5779 - val_loss: 0.6760 - val_accuracy: 0.5840
Epoch 28/500
133/133 - 18s - loss: 0.7202 - accuracy: 0.5932 - val_loss: 0.6741 - val_accuracy: 0.5859
Epoch 29/500
133/133 - 18s - loss: 0.7319 - accuracy: 0.5866 - val_loss: 0.6728 - val_accuracy: 0.5897
Epoch 30/500
133/133 - 18s - loss: 0.7191 - accuracy: 0.5987 - val_loss: 0.6713 - val_accuracy: 0.5878
Epoch 31/500
133/133 - 18s - loss: 0.7151 - accuracy: 0.6008 - val_loss: 0.6699 - val_accuracy: 0.5954
Epoch 32/500
133/133 - 18s - loss: 0.7294 - accuracy: 0.5949 - val_loss: 0.6683 - val_accuracy: 0.5954
Epoch 33/500
133/133 - 18s - loss: 0.7185 - accuracy: 0.6043 - val_loss: 0.6662 - val_accuracy: 0.5992
Epoch 34/500
133/133 - 18s - loss: 0.7023 - accuracy: 0.6225 - val_loss: 0.6653 - val_accuracy: 0.5973
Epoch 35/500
133/133 - 18s - loss: 0.7030 - accuracy: 0.6051 - val_loss: 0.6638 - val_accuracy: 0.5973
Epoch 36/500
133/133 - 18s - loss: 0.6943 - accuracy: 0.6107 - val_loss: 0.6618 - val_accuracy: 0.6031
Epoch 37/500
133/133 - 18s - loss: 0.6965 - accuracy: 0.6230 - val_loss: 0.6612 - val_accuracy: 0.5992
Epoch 38/500
133/133 - 18s - loss: 0.6868 - accuracy: 0.6256 - val_loss: 0.6601 - val_accuracy: 0.5992
Epoch 39/500
133/133 - 18s - loss: 0.6815 - accuracy: 0.6254 - val_loss: 0.6586 - val_accuracy: 0.5992
Epoch 40/500
133/133 - 18s - loss: 0.6702 - accuracy: 0.6301 - val_loss: 0.6569 - val_accuracy: 0.5954
Epoch 41/500
133/133 - 18s - loss: 0.6821 - accuracy: 0.6223 - val_loss: 0.6550 - val_accuracy: 0.6107
Epoch 42/500
133/133 - 18s - loss: 0.6778 - accuracy: 0.6235 - val_loss: 0.6532 - val_accuracy: 0.6050
Epoch 43/500
133/133 - 18s - loss: 0.6642 - accuracy: 0.6389 - val_loss: 0.6529 - val_accuracy: 0.6031
Epoch 44/500
133/133 - 18s - loss: 0.6518 - accuracy: 0.6474 - val_loss: 0.6508 - val_accuracy: 0.6069
Epoch 45/500
133/133 - 18s - loss: 0.6460 - accuracy: 0.6578 - val_loss: 0.6504 - val_accuracy: 0.6050
Epoch 46/500
133/133 - 18s - loss: 0.6414 - accuracy: 0.6592 - val_loss: 0.6498 - val_accuracy: 0.6126
Epoch 47/500
133/133 - 18s - loss: 0.6484 - accuracy: 0.6594 - val_loss: 0.6478 - val_accuracy: 0.6126
Epoch 48/500
133/133 - 18s - loss: 0.6389 - accuracy: 0.6476 - val_loss: 0.6461 - val_accuracy: 0.6164
Epoch 49/500
133/133 - 18s - loss: 0.6341 - accuracy: 0.6578 - val_loss: 0.6456 - val_accuracy: 0.6088
Epoch 50/500
133/133 - 18s - loss: 0.6387 - accuracy: 0.6563 - val_loss: 0.6445 - val_accuracy: 0.6107
Epoch 51/500
133/133 - 18s - loss: 0.6109 - accuracy: 0.6802 - val_loss: 0.6419 - val_accuracy: 0.6145
Epoch 52/500
133/133 - 18s - loss: 0.6193 - accuracy: 0.6712 - val_loss: 0.6403 - val_accuracy: 0.6221
Epoch 53/500
133/133 - 18s - loss: 0.6053 - accuracy: 0.6831 - val_loss: 0.6406 - val_accuracy: 0.6202
Epoch 54/500
133/133 - 18s - loss: 0.6136 - accuracy: 0.6774 - val_loss: 0.6391 - val_accuracy: 0.6240
Epoch 55/500
133/133 - 18s - loss: 0.5910 - accuracy: 0.7008 - val_loss: 0.6380 - val_accuracy: 0.6202
Epoch 56/500
133/133 - 18s - loss: 0.5982 - accuracy: 0.6899 - val_loss: 0.6363 - val_accuracy: 0.6298
Epoch 57/500
133/133 - 18s - loss: 0.5871 - accuracy: 0.7027 - val_loss: 0.6342 - val_accuracy: 0.6355
Epoch 58/500
133/133 - 18s - loss: 0.5726 - accuracy: 0.7013 - val_loss: 0.6332 - val_accuracy: 0.6298
Epoch 59/500
133/133 - 18s - loss: 0.5757 - accuracy: 0.7031 - val_loss: 0.6315 - val_accuracy: 0.6336
Epoch 60/500
133/133 - 18s - loss: 0.5740 - accuracy: 0.7119 - val_loss: 0.6315 - val_accuracy: 0.6374
Epoch 61/500
133/133 - 18s - loss: 0.5737 - accuracy: 0.7050 - val_loss: 0.6302 - val_accuracy: 0.6393
Epoch 62/500
133/133 - 18s - loss: 0.5496 - accuracy: 0.7232 - val_loss: 0.6289 - val_accuracy: 0.6450
Epoch 63/500
133/133 - 18s - loss: 0.5571 - accuracy: 0.7187 - val_loss: 0.6277 - val_accuracy: 0.6489
Epoch 64/500
133/133 - 18s - loss: 0.5470 - accuracy: 0.7296 - val_loss: 0.6256 - val_accuracy: 0.6546
Epoch 65/500
133/133 - 18s - loss: 0.5482 - accuracy: 0.7225 - val_loss: 0.6242 - val_accuracy: 0.6546
Epoch 66/500
133/133 - 18s - loss: 0.5501 - accuracy: 0.7202 - val_loss: 0.6232 - val_accuracy: 0.6546
Epoch 67/500
133/133 - 18s - loss: 0.5404 - accuracy: 0.7268 - val_loss: 0.6223 - val_accuracy: 0.6489
Epoch 68/500
133/133 - 18s - loss: 0.5305 - accuracy: 0.7379 - val_loss: 0.6218 - val_accuracy: 0.6546
Epoch 69/500
133/133 - 18s - loss: 0.5139 - accuracy: 0.7374 - val_loss: 0.6208 - val_accuracy: 0.6603
Epoch 70/500
133/133 - 18s - loss: 0.5194 - accuracy: 0.7421 - val_loss: 0.6204 - val_accuracy: 0.6641
Epoch 71/500
133/133 - 18s - loss: 0.5241 - accuracy: 0.7351 - val_loss: 0.6206 - val_accuracy: 0.6584
Epoch 72/500
133/133 - 18s - loss: 0.5095 - accuracy: 0.7509 - val_loss: 0.6185 - val_accuracy: 0.6660
Epoch 73/500
133/133 - 18s - loss: 0.4813 - accuracy: 0.7660 - val_loss: 0.6180 - val_accuracy: 0.6660
Epoch 74/500
133/133 - 18s - loss: 0.4956 - accuracy: 0.7584 - val_loss: 0.6162 - val_accuracy: 0.6698
Epoch 75/500
133/133 - 18s - loss: 0.4879 - accuracy: 0.7660 - val_loss: 0.6155 - val_accuracy: 0.6660
Epoch 76/500
133/133 - 18s - loss: 0.4871 - accuracy: 0.7610 - val_loss: 0.6138 - val_accuracy: 0.6679
Epoch 77/500
133/133 - 18s - loss: 0.4688 - accuracy: 0.7783 - val_loss: 0.6129 - val_accuracy: 0.6660
Epoch 78/500
133/133 - 18s - loss: 0.4734 - accuracy: 0.7714 - val_loss: 0.6134 - val_accuracy: 0.6737
Epoch 79/500
133/133 - 18s - loss: 0.4679 - accuracy: 0.7710 - val_loss: 0.6123 - val_accuracy: 0.6756
Epoch 80/500
133/133 - 18s - loss: 0.4625 - accuracy: 0.7731 - val_loss: 0.6129 - val_accuracy: 0.6813
Epoch 81/500
133/133 - 18s - loss: 0.4439 - accuracy: 0.7904 - val_loss: 0.6109 - val_accuracy: 0.6794
Epoch 82/500
133/133 - 18s - loss: 0.4488 - accuracy: 0.7842 - val_loss: 0.6112 - val_accuracy: 0.6794
Epoch 83/500
133/133 - 18s - loss: 0.4437 - accuracy: 0.7977 - val_loss: 0.6102 - val_accuracy: 0.6851
Epoch 84/500
133/133 - 18s - loss: 0.4309 - accuracy: 0.8022 - val_loss: 0.6095 - val_accuracy: 0.6832
Epoch 85/500
133/133 - 18s - loss: 0.4324 - accuracy: 0.7927 - val_loss: 0.6104 - val_accuracy: 0.6832
Epoch 86/500
133/133 - 18s - loss: 0.4153 - accuracy: 0.8052 - val_loss: 0.6100 - val_accuracy: 0.6889
Epoch 87/500
133/133 - 18s - loss: 0.4091 - accuracy: 0.8102 - val_loss: 0.6100 - val_accuracy: 0.6851
Epoch 88/500
133/133 - 18s - loss: 0.4116 - accuracy: 0.8147 - val_loss: 0.6098 - val_accuracy: 0.6889
Epoch 89/500
133/133 - 18s - loss: 0.4104 - accuracy: 0.8095 - val_loss: 0.6079 - val_accuracy: 0.6870
Epoch 90/500
133/133 - 18s - loss: 0.4137 - accuracy: 0.8116 - val_loss: 0.6077 - val_accuracy: 0.6851
Epoch 91/500
133/133 - 18s - loss: 0.4063 - accuracy: 0.8159 - val_loss: 0.6064 - val_accuracy: 0.6889
Epoch 92/500
133/133 - 18s - loss: 0.3970 - accuracy: 0.8173 - val_loss: 0.6078 - val_accuracy: 0.6889
Epoch 93/500
133/133 - 18s - loss: 0.3988 - accuracy: 0.8149 - val_loss: 0.6066 - val_accuracy: 0.6889
Epoch 94/500
133/133 - 18s - loss: 0.3963 - accuracy: 0.8178 - val_loss: 0.6071 - val_accuracy: 0.6889
Epoch 95/500
133/133 - 18s - loss: 0.3867 - accuracy: 0.8213 - val_loss: 0.6067 - val_accuracy: 0.6870
Epoch 96/500
133/133 - 18s - loss: 0.3844 - accuracy: 0.8227 - val_loss: 0.6068 - val_accuracy: 0.6870
Epoch 97/500
133/133 - 18s - loss: 0.3715 - accuracy: 0.8346 - val_loss: 0.6063 - val_accuracy: 0.6889
Epoch 98/500
133/133 - 18s - loss: 0.3530 - accuracy: 0.8414 - val_loss: 0.6056 - val_accuracy: 0.6889
Epoch 99/500
133/133 - 18s - loss: 0.3662 - accuracy: 0.8369 - val_loss: 0.6065 - val_accuracy: 0.6908
Epoch 100/500
133/133 - 18s - loss: 0.3602 - accuracy: 0.8412 - val_loss: 0.6071 - val_accuracy: 0.6889
Epoch 101/500
133/133 - 18s - loss: 0.3595 - accuracy: 0.8355 - val_loss: 0.6077 - val_accuracy: 0.6870
Epoch 102/500
133/133 - 18s - loss: 0.3504 - accuracy: 0.8440 - val_loss: 0.6085 - val_accuracy: 0.6927
Epoch 103/500
133/133 - 18s - loss: 0.3426 - accuracy: 0.8565 - val_loss: 0.6083 - val_accuracy: 0.6947
Epoch 104/500
133/133 - 18s - loss: 0.3376 - accuracy: 0.8591 - val_loss: 0.6067 - val_accuracy: 0.6908
Epoch 105/500
133/133 - 18s - loss: 0.3305 - accuracy: 0.8561 - val_loss: 0.6085 - val_accuracy: 0.6966
Epoch 106/500
133/133 - 18s - loss: 0.3178 - accuracy: 0.8601 - val_loss: 0.6072 - val_accuracy: 0.6927
Epoch 107/500
133/133 - 18s - loss: 0.3216 - accuracy: 0.8634 - val_loss: 0.6075 - val_accuracy: 0.6947
Epoch 108/500
133/133 - 18s - loss: 0.3099 - accuracy: 0.8695 - val_loss: 0.6109 - val_accuracy: 0.6985
Epoch 109/500
133/133 - 18s - loss: 0.2970 - accuracy: 0.8747 - val_loss: 0.6130 - val_accuracy: 0.6985
Epoch 110/500
133/133 - 18s - loss: 0.3143 - accuracy: 0.8646 - val_loss: 0.6133 - val_accuracy: 0.7023
Epoch 111/500
133/133 - 18s - loss: 0.3056 - accuracy: 0.8710 - val_loss: 0.6132 - val_accuracy: 0.7023
Epoch 112/500
133/133 - 18s - loss: 0.3087 - accuracy: 0.8693 - val_loss: 0.6153 - val_accuracy: 0.7080
Epoch 113/500
133/133 - 18s - loss: 0.3029 - accuracy: 0.8764 - val_loss: 0.6133 - val_accuracy: 0.7061
Epoch 114/500
133/133 - 18s - loss: 0.3012 - accuracy: 0.8698 - val_loss: 0.6149 - val_accuracy: 0.7099
Epoch 115/500
133/133 - 18s - loss: 0.2788 - accuracy: 0.8875 - val_loss: 0.6137 - val_accuracy: 0.7004
Epoch 116/500
133/133 - 18s - loss: 0.2868 - accuracy: 0.8835 - val_loss: 0.6159 - val_accuracy: 0.7004
Epoch 117/500
133/133 - 18s - loss: 0.2849 - accuracy: 0.8837 - val_loss: 0.6165 - val_accuracy: 0.7042
Epoch 118/500
133/133 - 18s - loss: 0.2783 - accuracy: 0.8856 - val_loss: 0.6170 - val_accuracy: 0.7023
Epoch 119/500
133/133 - 18s - loss: 0.2762 - accuracy: 0.8851 - val_loss: 0.6181 - val_accuracy: 0.7061
Epoch 120/500
133/133 - 18s - loss: 0.2589 - accuracy: 0.8936 - val_loss: 0.6203 - val_accuracy: 0.7042
Epoch 121/500
133/133 - 18s - loss: 0.2599 - accuracy: 0.8880 - val_loss: 0.6218 - val_accuracy: 0.7061
Epoch 122/500
133/133 - 18s - loss: 0.2600 - accuracy: 0.8974 - val_loss: 0.6236 - val_accuracy: 0.7061
Epoch 123/500
133/133 - 18s - loss: 0.2424 - accuracy: 0.9000 - val_loss: 0.6237 - val_accuracy: 0.7080
Epoch 124/500
133/133 - 18s - loss: 0.2496 - accuracy: 0.8920 - val_loss: 0.6259 - val_accuracy: 0.7080
Epoch 125/500
133/133 - 18s - loss: 0.2594 - accuracy: 0.8974 - val_loss: 0.6255 - val_accuracy: 0.7099
Epoch 126/500
133/133 - 18s - loss: 0.2334 - accuracy: 0.9043 - val_loss: 0.6266 - val_accuracy: 0.7099
Epoch 127/500
133/133 - 18s - loss: 0.2517 - accuracy: 0.8953 - val_loss: 0.6267 - val_accuracy: 0.7042
Epoch 128/500
133/133 - 18s - loss: 0.2290 - accuracy: 0.9095 - val_loss: 0.6267 - val_accuracy: 0.7061
Epoch 129/500
133/133 - 18s - loss: 0.2375 - accuracy: 0.9066 - val_loss: 0.6289 - val_accuracy: 0.7118
Epoch 130/500
133/133 - 18s - loss: 0.2233 - accuracy: 0.9116 - val_loss: 0.6313 - val_accuracy: 0.7099
Epoch 131/500
133/133 - 18s - loss: 0.2198 - accuracy: 0.9128 - val_loss: 0.6331 - val_accuracy: 0.7080
Epoch 132/500
133/133 - 18s - loss: 0.2229 - accuracy: 0.9109 - val_loss: 0.6350 - val_accuracy: 0.7099
Epoch 133/500
133/133 - 18s - loss: 0.2168 - accuracy: 0.9111 - val_loss: 0.6339 - val_accuracy: 0.7099
Epoch 134/500
133/133 - 18s - loss: 0.2268 - accuracy: 0.9059 - val_loss: 0.6351 - val_accuracy: 0.7176
Epoch 135/500
133/133 - 18s - loss: 0.2280 - accuracy: 0.9022 - val_loss: 0.6389 - val_accuracy: 0.7156
Epoch 136/500
133/133 - 18s - loss: 0.2063 - accuracy: 0.9187 - val_loss: 0.6406 - val_accuracy: 0.7080
Epoch 137/500
133/133 - 18s - loss: 0.1916 - accuracy: 0.9296 - val_loss: 0.6437 - val_accuracy: 0.7099
Epoch 138/500
133/133 - 18s - loss: 0.2066 - accuracy: 0.9140 - val_loss: 0.6423 - val_accuracy: 0.7195
Epoch 139/500
133/133 - 18s - loss: 0.2082 - accuracy: 0.9187 - val_loss: 0.6439 - val_accuracy: 0.7137
Epoch 140/500
133/133 - 18s - loss: 0.2033 - accuracy: 0.9239 - val_loss: 0.6442 - val_accuracy: 0.7214
Epoch 141/500
133/133 - 18s - loss: 0.1988 - accuracy: 0.9206 - val_loss: 0.6431 - val_accuracy: 0.7214
Epoch 142/500
133/133 - 18s - loss: 0.1941 - accuracy: 0.9253 - val_loss: 0.6501 - val_accuracy: 0.7080
Epoch 143/500
133/133 - 18s - loss: 0.1905 - accuracy: 0.9274 - val_loss: 0.6485 - val_accuracy: 0.7118
Epoch 144/500
133/133 - 18s - loss: 0.1913 - accuracy: 0.9291 - val_loss: 0.6515 - val_accuracy: 0.7080
Epoch 145/500
133/133 - 18s - loss: 0.1897 - accuracy: 0.9263 - val_loss: 0.6511 - val_accuracy: 0.7176
Epoch 146/500
133/133 - 18s - loss: 0.1753 - accuracy: 0.9298 - val_loss: 0.6527 - val_accuracy: 0.7137
Epoch 147/500
133/133 - 18s - loss: 0.1757 - accuracy: 0.9300 - val_loss: 0.6536 - val_accuracy: 0.7156
Epoch 148/500
133/133 - 18s - loss: 0.1738 - accuracy: 0.9345 - val_loss: 0.6556 - val_accuracy: 0.7137
Epoch 149/500
133/133 - 18s - loss: 0.1835 - accuracy: 0.9331 - val_loss: 0.6555 - val_accuracy: 0.7137
Epoch 150/500
133/133 - 18s - loss: 0.1707 - accuracy: 0.9359 - val_loss: 0.6569 - val_accuracy: 0.7176
Epoch 151/500
133/133 - 18s - loss: 0.1721 - accuracy: 0.9319 - val_loss: 0.6573 - val_accuracy: 0.7176
Epoch 152/500
133/133 - 18s - loss: 0.1597 - accuracy: 0.9390 - val_loss: 0.6564 - val_accuracy: 0.7214
Epoch 153/500
133/133 - 18s - loss: 0.1739 - accuracy: 0.9329 - val_loss: 0.6585 - val_accuracy: 0.7233
Epoch 154/500
133/133 - 18s - loss: 0.1558 - accuracy: 0.9419 - val_loss: 0.6567 - val_accuracy: 0.7156
Epoch 155/500
133/133 - 18s - loss: 0.1631 - accuracy: 0.9383 - val_loss: 0.6620 - val_accuracy: 0.7195
Epoch 156/500
133/133 - 19s - loss: 0.1597 - accuracy: 0.9371 - val_loss: 0.6667 - val_accuracy: 0.7195
Epoch 157/500
133/133 - 18s - loss: 0.1632 - accuracy: 0.9359 - val_loss: 0.6655 - val_accuracy: 0.7176
Epoch 158/500
133/133 - 18s - loss: 0.1569 - accuracy: 0.9397 - val_loss: 0.6654 - val_accuracy: 0.7156
Epoch 159/500
133/133 - 18s - loss: 0.1482 - accuracy: 0.9404 - val_loss: 0.6687 - val_accuracy: 0.7195
Epoch 160/500
133/133 - 18s - loss: 0.1465 - accuracy: 0.9456 - val_loss: 0.6698 - val_accuracy: 0.7176
Epoch 161/500
133/133 - 18s - loss: 0.1393 - accuracy: 0.9489 - val_loss: 0.6712 - val_accuracy: 0.7176
Epoch 162/500
133/133 - 18s - loss: 0.1361 - accuracy: 0.9523 - val_loss: 0.6718 - val_accuracy: 0.7214
Epoch 163/500
133/133 - 18s - loss: 0.1433 - accuracy: 0.9478 - val_loss: 0.6773 - val_accuracy: 0.7195
Epoch 164/500
133/133 - 18s - loss: 0.1286 - accuracy: 0.9551 - val_loss: 0.6784 - val_accuracy: 0.7233
Epoch 165/500
133/133 - 18s - loss: 0.1452 - accuracy: 0.9466 - val_loss: 0.6789 - val_accuracy: 0.7233
Epoch 166/500
133/133 - 18s - loss: 0.1416 - accuracy: 0.9454 - val_loss: 0.6823 - val_accuracy: 0.7156
Epoch 167/500
133/133 - 18s - loss: 0.1349 - accuracy: 0.9492 - val_loss: 0.6812 - val_accuracy: 0.7176
Epoch 168/500
133/133 - 18s - loss: 0.1262 - accuracy: 0.9539 - val_loss: 0.6815 - val_accuracy: 0.7195
Epoch 169/500
133/133 - 18s - loss: 0.1391 - accuracy: 0.9466 - val_loss: 0.6835 - val_accuracy: 0.7214
Epoch 170/500
133/133 - 18s - loss: 0.1300 - accuracy: 0.9508 - val_loss: 0.6853 - val_accuracy: 0.7233
Epoch 171/500
133/133 - 18s - loss: 0.1342 - accuracy: 0.9499 - val_loss: 0.6866 - val_accuracy: 0.7233
Epoch 172/500
133/133 - 18s - loss: 0.1224 - accuracy: 0.9541 - val_loss: 0.6896 - val_accuracy: 0.7252
Epoch 173/500
133/133 - 18s - loss: 0.1291 - accuracy: 0.9515 - val_loss: 0.6897 - val_accuracy: 0.7252
Epoch 174/500
133/133 - 18s - loss: 0.1211 - accuracy: 0.9544 - val_loss: 0.6950 - val_accuracy: 0.7233
Epoch 175/500
133/133 - 18s - loss: 0.1252 - accuracy: 0.9541 - val_loss: 0.6933 - val_accuracy: 0.7252
Epoch 176/500
133/133 - 19s - loss: 0.1219 - accuracy: 0.9558 - val_loss: 0.7007 - val_accuracy: 0.7195
Epoch 177/500
133/133 - 18s - loss: 0.1201 - accuracy: 0.9534 - val_loss: 0.6983 - val_accuracy: 0.7214
Epoch 178/500
133/133 - 18s - loss: 0.1116 - accuracy: 0.9586 - val_loss: 0.7044 - val_accuracy: 0.7176
Epoch 179/500
133/133 - 18s - loss: 0.1181 - accuracy: 0.9553 - val_loss: 0.7043 - val_accuracy: 0.7195
Epoch 180/500
133/133 - 18s - loss: 0.1127 - accuracy: 0.9593 - val_loss: 0.7011 - val_accuracy: 0.7252
Epoch 181/500
133/133 - 18s - loss: 0.1095 - accuracy: 0.9631 - val_loss: 0.7034 - val_accuracy: 0.7271
Epoch 182/500
133/133 - 18s - loss: 0.1194 - accuracy: 0.9570 - val_loss: 0.7089 - val_accuracy: 0.7233
Epoch 183/500
133/133 - 18s - loss: 0.1053 - accuracy: 0.9643 - val_loss: 0.7112 - val_accuracy: 0.7176
Epoch 184/500
133/133 - 18s - loss: 0.1053 - accuracy: 0.9641 - val_loss: 0.7126 - val_accuracy: 0.7233
Epoch 185/500
133/133 - 18s - loss: 0.1067 - accuracy: 0.9622 - val_loss: 0.7155 - val_accuracy: 0.7195
Epoch 186/500
133/133 - 18s - loss: 0.1116 - accuracy: 0.9582 - val_loss: 0.7190 - val_accuracy: 0.7195
Epoch 187/500
133/133 - 18s - loss: 0.1095 - accuracy: 0.9638 - val_loss: 0.7175 - val_accuracy: 0.7252
Epoch 188/500
133/133 - 18s - loss: 0.1091 - accuracy: 0.9598 - val_loss: 0.7214 - val_accuracy: 0.7233
Epoch 189/500
133/133 - 18s - loss: 0.1031 - accuracy: 0.9598 - val_loss: 0.7221 - val_accuracy: 0.7252
Epoch 190/500
133/133 - 18s - loss: 0.1044 - accuracy: 0.9634 - val_loss: 0.7222 - val_accuracy: 0.7195
Epoch 191/500
133/133 - 18s - loss: 0.0997 - accuracy: 0.9662 - val_loss: 0.7241 - val_accuracy: 0.7214
Epoch 192/500
133/133 - 18s - loss: 0.1048 - accuracy: 0.9636 - val_loss: 0.7301 - val_accuracy: 0.7176
Epoch 193/500
133/133 - 18s - loss: 0.0953 - accuracy: 0.9674 - val_loss: 0.7372 - val_accuracy: 0.7176
Epoch 194/500
133/133 - 18s - loss: 0.1042 - accuracy: 0.9631 - val_loss: 0.7383 - val_accuracy: 0.7176
Epoch 195/500
133/133 - 18s - loss: 0.0976 - accuracy: 0.9631 - val_loss: 0.7419 - val_accuracy: 0.7156
Epoch 196/500
133/133 - 18s - loss: 0.1016 - accuracy: 0.9627 - val_loss: 0.7406 - val_accuracy: 0.7195
Epoch 197/500
133/133 - 18s - loss: 0.0896 - accuracy: 0.9716 - val_loss: 0.7409 - val_accuracy: 0.7195
Epoch 198/500
133/133 - 19s - loss: 0.1016 - accuracy: 0.9634 - val_loss: 0.7418 - val_accuracy: 0.7233
Epoch 199/500
133/133 - 18s - loss: 0.0813 - accuracy: 0.9731 - val_loss: 0.7433 - val_accuracy: 0.7195
Epoch 200/500
133/133 - 18s - loss: 0.0938 - accuracy: 0.9662 - val_loss: 0.7450 - val_accuracy: 0.7214
Epoch 201/500
133/133 - 18s - loss: 0.0901 - accuracy: 0.9667 - val_loss: 0.7479 - val_accuracy: 0.7214
========================================
save_weights
h5_weights/GM.po/embedding_cnn_one_branch.h5
========================================

end time >>> Sun Oct  3 03:41:18 2021

end time >>> Sun Oct  3 03:41:18 2021

end time >>> Sun Oct  3 03:41:18 2021

end time >>> Sun Oct  3 03:41:18 2021

end time >>> Sun Oct  3 03:41:18 2021












args.model = embedding_cnn_one_branch
time used = 3686.510414838791


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 03:41:20 2021

begin time >>> Sun Oct  3 03:41:20 2021

begin time >>> Sun Oct  3 03:41:20 2021

begin time >>> Sun Oct  3 03:41:20 2021

begin time >>> Sun Oct  3 03:41:20 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_cnn_two_branch
args.type = train
args.name = GM.po
args.length = 10001
===========================


-> h5_weights/GM.po folder already exist. pass.
-> result/GM.po/onehot_cnn_one_branch folder already exist. pass.
-> result/GM.po/onehot_cnn_two_branch folder already exist. pass.
-> result/GM.po/onehot_embedding_dense folder already exist. pass.
-> result/GM.po/onehot_dense folder already exist. pass.
-> result/GM.po/onehot_resnet18 folder already exist. pass.
-> result/GM.po/onehot_resnet34 folder already exist. pass.
-> result/GM.po/embedding_cnn_one_branch folder already exist. pass.
-> result/GM.po/embedding_cnn_two_branch folder already exist. pass.
-> result/GM.po/embedding_dense folder already exist. pass.
-> result/GM.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/GM.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
GM.po
########################################

########################################
model_name
embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
133/133 - 19s - loss: 0.8761 - accuracy: 0.4940 - val_loss: 0.6881 - val_accuracy: 0.5382
Epoch 2/500
133/133 - 18s - loss: 0.8510 - accuracy: 0.5058 - val_loss: 0.6876 - val_accuracy: 0.5420
Epoch 3/500
133/133 - 19s - loss: 0.8533 - accuracy: 0.5108 - val_loss: 0.6878 - val_accuracy: 0.5496
Epoch 4/500
133/133 - 18s - loss: 0.8356 - accuracy: 0.5200 - val_loss: 0.6866 - val_accuracy: 0.5592
Epoch 5/500
133/133 - 18s - loss: 0.8295 - accuracy: 0.5233 - val_loss: 0.6844 - val_accuracy: 0.5534
Epoch 6/500
133/133 - 18s - loss: 0.8257 - accuracy: 0.5245 - val_loss: 0.6815 - val_accuracy: 0.5630
Epoch 7/500
133/133 - 19s - loss: 0.8211 - accuracy: 0.5160 - val_loss: 0.6792 - val_accuracy: 0.5630
Epoch 8/500
133/133 - 18s - loss: 0.8019 - accuracy: 0.5422 - val_loss: 0.6765 - val_accuracy: 0.5668
Epoch 9/500
133/133 - 18s - loss: 0.8140 - accuracy: 0.5285 - val_loss: 0.6738 - val_accuracy: 0.5725
Epoch 10/500
133/133 - 18s - loss: 0.8091 - accuracy: 0.5287 - val_loss: 0.6716 - val_accuracy: 0.5763
Epoch 11/500
133/133 - 18s - loss: 0.7899 - accuracy: 0.5460 - val_loss: 0.6696 - val_accuracy: 0.5744
Epoch 12/500
133/133 - 18s - loss: 0.7616 - accuracy: 0.5540 - val_loss: 0.6674 - val_accuracy: 0.5744
Epoch 13/500
133/133 - 18s - loss: 0.7796 - accuracy: 0.5524 - val_loss: 0.6656 - val_accuracy: 0.5763
Epoch 14/500
133/133 - 18s - loss: 0.7823 - accuracy: 0.5502 - val_loss: 0.6637 - val_accuracy: 0.5782
Epoch 15/500
133/133 - 18s - loss: 0.7681 - accuracy: 0.5559 - val_loss: 0.6615 - val_accuracy: 0.5878
Epoch 16/500
133/133 - 18s - loss: 0.7761 - accuracy: 0.5557 - val_loss: 0.6599 - val_accuracy: 0.5897
Epoch 17/500
133/133 - 18s - loss: 0.7795 - accuracy: 0.5554 - val_loss: 0.6576 - val_accuracy: 0.5916
Epoch 18/500
133/133 - 19s - loss: 0.7495 - accuracy: 0.5696 - val_loss: 0.6554 - val_accuracy: 0.6011
Epoch 19/500
133/133 - 18s - loss: 0.7662 - accuracy: 0.5620 - val_loss: 0.6540 - val_accuracy: 0.5973
Epoch 20/500
133/133 - 18s - loss: 0.7658 - accuracy: 0.5628 - val_loss: 0.6523 - val_accuracy: 0.6050
Epoch 21/500
133/133 - 18s - loss: 0.7537 - accuracy: 0.5656 - val_loss: 0.6504 - val_accuracy: 0.6069
Epoch 22/500
133/133 - 18s - loss: 0.7432 - accuracy: 0.5722 - val_loss: 0.6477 - val_accuracy: 0.6050
Epoch 23/500
133/133 - 18s - loss: 0.7393 - accuracy: 0.5814 - val_loss: 0.6461 - val_accuracy: 0.6088
Epoch 24/500
133/133 - 18s - loss: 0.7305 - accuracy: 0.5788 - val_loss: 0.6443 - val_accuracy: 0.6164
Epoch 25/500
133/133 - 18s - loss: 0.7360 - accuracy: 0.5786 - val_loss: 0.6424 - val_accuracy: 0.6164
Epoch 26/500
133/133 - 18s - loss: 0.7172 - accuracy: 0.5961 - val_loss: 0.6410 - val_accuracy: 0.6145
Epoch 27/500
133/133 - 18s - loss: 0.7187 - accuracy: 0.5925 - val_loss: 0.6393 - val_accuracy: 0.6221
Epoch 28/500
133/133 - 18s - loss: 0.7123 - accuracy: 0.5991 - val_loss: 0.6379 - val_accuracy: 0.6279
Epoch 29/500
133/133 - 18s - loss: 0.6921 - accuracy: 0.6067 - val_loss: 0.6358 - val_accuracy: 0.6336
Epoch 30/500
133/133 - 18s - loss: 0.7017 - accuracy: 0.6036 - val_loss: 0.6337 - val_accuracy: 0.6336
Epoch 31/500
133/133 - 18s - loss: 0.7066 - accuracy: 0.6020 - val_loss: 0.6317 - val_accuracy: 0.6355
Epoch 32/500
133/133 - 18s - loss: 0.6901 - accuracy: 0.6112 - val_loss: 0.6304 - val_accuracy: 0.6317
Epoch 33/500
133/133 - 18s - loss: 0.6719 - accuracy: 0.6339 - val_loss: 0.6279 - val_accuracy: 0.6374
Epoch 34/500
133/133 - 18s - loss: 0.6887 - accuracy: 0.6114 - val_loss: 0.6265 - val_accuracy: 0.6450
Epoch 35/500
133/133 - 18s - loss: 0.6871 - accuracy: 0.6173 - val_loss: 0.6244 - val_accuracy: 0.6412
Epoch 36/500
133/133 - 18s - loss: 0.6796 - accuracy: 0.6367 - val_loss: 0.6222 - val_accuracy: 0.6527
Epoch 37/500
133/133 - 18s - loss: 0.6621 - accuracy: 0.6285 - val_loss: 0.6198 - val_accuracy: 0.6584
Epoch 38/500
133/133 - 18s - loss: 0.6546 - accuracy: 0.6412 - val_loss: 0.6183 - val_accuracy: 0.6584
Epoch 39/500
133/133 - 18s - loss: 0.6425 - accuracy: 0.6535 - val_loss: 0.6158 - val_accuracy: 0.6641
Epoch 40/500
133/133 - 18s - loss: 0.6338 - accuracy: 0.6528 - val_loss: 0.6143 - val_accuracy: 0.6718
Epoch 41/500
133/133 - 18s - loss: 0.6445 - accuracy: 0.6502 - val_loss: 0.6127 - val_accuracy: 0.6679
Epoch 42/500
133/133 - 18s - loss: 0.6273 - accuracy: 0.6615 - val_loss: 0.6110 - val_accuracy: 0.6698
Epoch 43/500
133/133 - 18s - loss: 0.6312 - accuracy: 0.6547 - val_loss: 0.6086 - val_accuracy: 0.6737
Epoch 44/500
133/133 - 18s - loss: 0.6391 - accuracy: 0.6561 - val_loss: 0.6063 - val_accuracy: 0.6718
Epoch 45/500
133/133 - 18s - loss: 0.6161 - accuracy: 0.6675 - val_loss: 0.6050 - val_accuracy: 0.6813
Epoch 46/500
133/133 - 18s - loss: 0.6200 - accuracy: 0.6684 - val_loss: 0.6028 - val_accuracy: 0.6794
Epoch 47/500
133/133 - 18s - loss: 0.6032 - accuracy: 0.6871 - val_loss: 0.6012 - val_accuracy: 0.6851
Epoch 48/500
133/133 - 18s - loss: 0.5951 - accuracy: 0.6984 - val_loss: 0.5995 - val_accuracy: 0.6870
Epoch 49/500
133/133 - 18s - loss: 0.6027 - accuracy: 0.6762 - val_loss: 0.5979 - val_accuracy: 0.6908
Epoch 50/500
133/133 - 18s - loss: 0.6030 - accuracy: 0.6800 - val_loss: 0.5958 - val_accuracy: 0.6870
Epoch 51/500
133/133 - 18s - loss: 0.5957 - accuracy: 0.6821 - val_loss: 0.5944 - val_accuracy: 0.6870
Epoch 52/500
133/133 - 18s - loss: 0.5791 - accuracy: 0.7034 - val_loss: 0.5926 - val_accuracy: 0.6927
Epoch 53/500
133/133 - 18s - loss: 0.5740 - accuracy: 0.7060 - val_loss: 0.5906 - val_accuracy: 0.6927
Epoch 54/500
133/133 - 18s - loss: 0.5897 - accuracy: 0.6904 - val_loss: 0.5886 - val_accuracy: 0.6966
Epoch 55/500
133/133 - 18s - loss: 0.5754 - accuracy: 0.7015 - val_loss: 0.5874 - val_accuracy: 0.6947
Epoch 56/500
133/133 - 18s - loss: 0.5582 - accuracy: 0.7166 - val_loss: 0.5859 - val_accuracy: 0.6908
Epoch 57/500
133/133 - 18s - loss: 0.5476 - accuracy: 0.7268 - val_loss: 0.5844 - val_accuracy: 0.6908
Epoch 58/500
133/133 - 18s - loss: 0.5423 - accuracy: 0.7268 - val_loss: 0.5817 - val_accuracy: 0.6908
Epoch 59/500
133/133 - 18s - loss: 0.5535 - accuracy: 0.7143 - val_loss: 0.5796 - val_accuracy: 0.6966
Epoch 60/500
133/133 - 18s - loss: 0.5373 - accuracy: 0.7244 - val_loss: 0.5775 - val_accuracy: 0.6966
Epoch 61/500
133/133 - 18s - loss: 0.5335 - accuracy: 0.7296 - val_loss: 0.5756 - val_accuracy: 0.7061
Epoch 62/500
133/133 - 18s - loss: 0.5152 - accuracy: 0.7421 - val_loss: 0.5742 - val_accuracy: 0.7118
Epoch 63/500
133/133 - 18s - loss: 0.5234 - accuracy: 0.7386 - val_loss: 0.5725 - val_accuracy: 0.7061
Epoch 64/500
133/133 - 19s - loss: 0.5037 - accuracy: 0.7502 - val_loss: 0.5711 - val_accuracy: 0.7042
Epoch 65/500
133/133 - 18s - loss: 0.5065 - accuracy: 0.7462 - val_loss: 0.5697 - val_accuracy: 0.7099
Epoch 66/500
133/133 - 18s - loss: 0.4943 - accuracy: 0.7592 - val_loss: 0.5680 - val_accuracy: 0.7137
Epoch 67/500
133/133 - 18s - loss: 0.5023 - accuracy: 0.7608 - val_loss: 0.5664 - val_accuracy: 0.7099
Epoch 68/500
133/133 - 18s - loss: 0.4774 - accuracy: 0.7755 - val_loss: 0.5648 - val_accuracy: 0.7099
Epoch 69/500
133/133 - 18s - loss: 0.4876 - accuracy: 0.7613 - val_loss: 0.5636 - val_accuracy: 0.7080
Epoch 70/500
133/133 - 18s - loss: 0.4721 - accuracy: 0.7781 - val_loss: 0.5622 - val_accuracy: 0.7137
Epoch 71/500
133/133 - 18s - loss: 0.4726 - accuracy: 0.7648 - val_loss: 0.5603 - val_accuracy: 0.7137
Epoch 72/500
133/133 - 18s - loss: 0.4561 - accuracy: 0.7849 - val_loss: 0.5594 - val_accuracy: 0.7176
Epoch 73/500
133/133 - 18s - loss: 0.4569 - accuracy: 0.7842 - val_loss: 0.5578 - val_accuracy: 0.7176
Epoch 74/500
133/133 - 18s - loss: 0.4484 - accuracy: 0.7859 - val_loss: 0.5562 - val_accuracy: 0.7156
Epoch 75/500
133/133 - 18s - loss: 0.4356 - accuracy: 0.7974 - val_loss: 0.5553 - val_accuracy: 0.7137
Epoch 76/500
133/133 - 18s - loss: 0.4356 - accuracy: 0.8015 - val_loss: 0.5539 - val_accuracy: 0.7156
Epoch 77/500
133/133 - 18s - loss: 0.4390 - accuracy: 0.7948 - val_loss: 0.5531 - val_accuracy: 0.7214
Epoch 78/500
133/133 - 18s - loss: 0.4364 - accuracy: 0.7972 - val_loss: 0.5520 - val_accuracy: 0.7271
Epoch 79/500
133/133 - 18s - loss: 0.4073 - accuracy: 0.8100 - val_loss: 0.5512 - val_accuracy: 0.7271
Epoch 80/500
133/133 - 18s - loss: 0.4156 - accuracy: 0.8133 - val_loss: 0.5504 - val_accuracy: 0.7271
Epoch 81/500
133/133 - 18s - loss: 0.3918 - accuracy: 0.8268 - val_loss: 0.5495 - val_accuracy: 0.7290
Epoch 82/500
133/133 - 18s - loss: 0.4093 - accuracy: 0.8130 - val_loss: 0.5484 - val_accuracy: 0.7271
Epoch 83/500
133/133 - 18s - loss: 0.3967 - accuracy: 0.8171 - val_loss: 0.5474 - val_accuracy: 0.7290
Epoch 84/500
133/133 - 18s - loss: 0.3910 - accuracy: 0.8185 - val_loss: 0.5471 - val_accuracy: 0.7290
Epoch 85/500
133/133 - 18s - loss: 0.3853 - accuracy: 0.8305 - val_loss: 0.5460 - val_accuracy: 0.7233
Epoch 86/500
133/133 - 18s - loss: 0.3736 - accuracy: 0.8374 - val_loss: 0.5453 - val_accuracy: 0.7309
Epoch 87/500
133/133 - 19s - loss: 0.3803 - accuracy: 0.8268 - val_loss: 0.5445 - val_accuracy: 0.7328
Epoch 88/500
133/133 - 18s - loss: 0.3684 - accuracy: 0.8350 - val_loss: 0.5442 - val_accuracy: 0.7290
Epoch 89/500
133/133 - 18s - loss: 0.3564 - accuracy: 0.8424 - val_loss: 0.5433 - val_accuracy: 0.7309
Epoch 90/500
133/133 - 18s - loss: 0.3568 - accuracy: 0.8393 - val_loss: 0.5432 - val_accuracy: 0.7385
Epoch 91/500
133/133 - 18s - loss: 0.3434 - accuracy: 0.8476 - val_loss: 0.5426 - val_accuracy: 0.7309
Epoch 92/500
133/133 - 18s - loss: 0.3376 - accuracy: 0.8570 - val_loss: 0.5414 - val_accuracy: 0.7252
Epoch 93/500
133/133 - 18s - loss: 0.3358 - accuracy: 0.8516 - val_loss: 0.5409 - val_accuracy: 0.7290
Epoch 94/500
133/133 - 18s - loss: 0.3283 - accuracy: 0.8650 - val_loss: 0.5410 - val_accuracy: 0.7328
Epoch 95/500
133/133 - 18s - loss: 0.3246 - accuracy: 0.8591 - val_loss: 0.5407 - val_accuracy: 0.7405
Epoch 96/500
133/133 - 18s - loss: 0.3247 - accuracy: 0.8603 - val_loss: 0.5406 - val_accuracy: 0.7366
Epoch 97/500
133/133 - 18s - loss: 0.3170 - accuracy: 0.8636 - val_loss: 0.5406 - val_accuracy: 0.7405
Epoch 98/500
133/133 - 18s - loss: 0.3094 - accuracy: 0.8736 - val_loss: 0.5397 - val_accuracy: 0.7405
Epoch 99/500
133/133 - 18s - loss: 0.3065 - accuracy: 0.8705 - val_loss: 0.5397 - val_accuracy: 0.7405
Epoch 100/500
133/133 - 18s - loss: 0.2954 - accuracy: 0.8785 - val_loss: 0.5392 - val_accuracy: 0.7424
Epoch 101/500
133/133 - 18s - loss: 0.2936 - accuracy: 0.8762 - val_loss: 0.5394 - val_accuracy: 0.7462
Epoch 102/500
133/133 - 18s - loss: 0.3010 - accuracy: 0.8707 - val_loss: 0.5403 - val_accuracy: 0.7424
Epoch 103/500
133/133 - 18s - loss: 0.2946 - accuracy: 0.8785 - val_loss: 0.5397 - val_accuracy: 0.7347
Epoch 104/500
133/133 - 18s - loss: 0.2783 - accuracy: 0.8927 - val_loss: 0.5401 - val_accuracy: 0.7405
Epoch 105/500
133/133 - 18s - loss: 0.2757 - accuracy: 0.8844 - val_loss: 0.5393 - val_accuracy: 0.7500
Epoch 106/500
133/133 - 18s - loss: 0.2691 - accuracy: 0.8892 - val_loss: 0.5399 - val_accuracy: 0.7424
Epoch 107/500
133/133 - 18s - loss: 0.2612 - accuracy: 0.8998 - val_loss: 0.5400 - val_accuracy: 0.7405
Epoch 108/500
133/133 - 18s - loss: 0.2608 - accuracy: 0.8944 - val_loss: 0.5408 - val_accuracy: 0.7385
Epoch 109/500
133/133 - 18s - loss: 0.2464 - accuracy: 0.9003 - val_loss: 0.5416 - val_accuracy: 0.7424
Epoch 110/500
133/133 - 18s - loss: 0.2610 - accuracy: 0.8927 - val_loss: 0.5410 - val_accuracy: 0.7424
Epoch 111/500
133/133 - 18s - loss: 0.2581 - accuracy: 0.8981 - val_loss: 0.5420 - val_accuracy: 0.7443
Epoch 112/500
133/133 - 18s - loss: 0.2369 - accuracy: 0.9059 - val_loss: 0.5421 - val_accuracy: 0.7443
Epoch 113/500
133/133 - 18s - loss: 0.2210 - accuracy: 0.9180 - val_loss: 0.5427 - val_accuracy: 0.7481
Epoch 114/500
133/133 - 18s - loss: 0.2367 - accuracy: 0.9085 - val_loss: 0.5433 - val_accuracy: 0.7519
Epoch 115/500
133/133 - 18s - loss: 0.2305 - accuracy: 0.9114 - val_loss: 0.5447 - val_accuracy: 0.7481
Epoch 116/500
133/133 - 18s - loss: 0.2391 - accuracy: 0.9052 - val_loss: 0.5444 - val_accuracy: 0.7424
Epoch 117/500
133/133 - 18s - loss: 0.2266 - accuracy: 0.9052 - val_loss: 0.5453 - val_accuracy: 0.7424
Epoch 118/500
133/133 - 18s - loss: 0.2193 - accuracy: 0.9173 - val_loss: 0.5460 - val_accuracy: 0.7443
Epoch 119/500
133/133 - 18s - loss: 0.2103 - accuracy: 0.9177 - val_loss: 0.5469 - val_accuracy: 0.7443
Epoch 120/500
133/133 - 18s - loss: 0.2147 - accuracy: 0.9152 - val_loss: 0.5478 - val_accuracy: 0.7385
Epoch 121/500
133/133 - 18s - loss: 0.2140 - accuracy: 0.9163 - val_loss: 0.5481 - val_accuracy: 0.7424
Epoch 122/500
133/133 - 18s - loss: 0.2056 - accuracy: 0.9177 - val_loss: 0.5492 - val_accuracy: 0.7481
Epoch 123/500
133/133 - 18s - loss: 0.1932 - accuracy: 0.9265 - val_loss: 0.5502 - val_accuracy: 0.7385
Epoch 124/500
133/133 - 18s - loss: 0.2000 - accuracy: 0.9258 - val_loss: 0.5518 - val_accuracy: 0.7443
Epoch 125/500
133/133 - 18s - loss: 0.1893 - accuracy: 0.9300 - val_loss: 0.5523 - val_accuracy: 0.7462
Epoch 126/500
133/133 - 18s - loss: 0.1940 - accuracy: 0.9232 - val_loss: 0.5547 - val_accuracy: 0.7462
Epoch 127/500
133/133 - 18s - loss: 0.1871 - accuracy: 0.9298 - val_loss: 0.5552 - val_accuracy: 0.7385
Epoch 128/500
133/133 - 18s - loss: 0.1756 - accuracy: 0.9310 - val_loss: 0.5564 - val_accuracy: 0.7385
Epoch 129/500
133/133 - 18s - loss: 0.1757 - accuracy: 0.9369 - val_loss: 0.5562 - val_accuracy: 0.7443
Epoch 130/500
133/133 - 18s - loss: 0.1859 - accuracy: 0.9319 - val_loss: 0.5565 - val_accuracy: 0.7462
Epoch 131/500
133/133 - 18s - loss: 0.1721 - accuracy: 0.9367 - val_loss: 0.5582 - val_accuracy: 0.7443
Epoch 132/500
133/133 - 18s - loss: 0.1739 - accuracy: 0.9393 - val_loss: 0.5587 - val_accuracy: 0.7443
Epoch 133/500
133/133 - 18s - loss: 0.1673 - accuracy: 0.9371 - val_loss: 0.5603 - val_accuracy: 0.7462
Epoch 134/500
133/133 - 18s - loss: 0.1565 - accuracy: 0.9414 - val_loss: 0.5612 - val_accuracy: 0.7424
========================================
save_weights
h5_weights/GM.po/embedding_cnn_two_branch.h5
========================================

end time >>> Sun Oct  3 04:22:38 2021

end time >>> Sun Oct  3 04:22:38 2021

end time >>> Sun Oct  3 04:22:38 2021

end time >>> Sun Oct  3 04:22:38 2021

end time >>> Sun Oct  3 04:22:38 2021












args.model = embedding_cnn_two_branch
time used = 2478.6732816696167


