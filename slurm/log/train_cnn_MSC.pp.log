************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 21:11:42 2021

begin time >>> Sun Oct  3 21:11:42 2021

begin time >>> Sun Oct  3 21:11:42 2021

begin time >>> Sun Oct  3 21:11:42 2021

begin time >>> Sun Oct  3 21:11:42 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_cnn_one_branch
args.type = train
args.name = MSC.pp
args.length = 10001
===========================


-> make new folder: h5_weights/MSC.pp
-> make new folder: result/MSC.pp/onehot_cnn_one_branch
-> make new folder: result/MSC.pp/onehot_cnn_two_branch
-> make new folder: result/MSC.pp/onehot_embedding_dense
-> make new folder: result/MSC.pp/onehot_dense
-> make new folder: result/MSC.pp/onehot_resnet18
-> make new folder: result/MSC.pp/onehot_resnet34
-> make new folder: result/MSC.pp/embedding_cnn_one_branch
-> make new folder: result/MSC.pp/embedding_cnn_two_branch
-> make new folder: result/MSC.pp/embedding_dense
-> make new folder: result/MSC.pp/onehot_embedding_cnn_one_branch
-> make new folder: result/MSC.pp/onehot_embedding_cnn_two_branch
########################################
gen_name
MSC.pp
########################################

########################################
model_name
onehot_cnn_one_branch
########################################

Found 3950 images belonging to 2 classes.
Found 488 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 5001, 5, 64)       1600      
_________________________________________________________________
batch_normalization (BatchNo (None, 5001, 5, 64)       256       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1251, 5, 64)       98368     
_________________________________________________________________
batch_normalization_1 (Batch (None, 1251, 5, 64)       256       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 313, 5, 64)        98368     
_________________________________________________________________
batch_normalization_2 (Batch (None, 313, 5, 64)        256       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 156, 5, 64)        0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 156, 5, 64)        256       
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 39, 5, 128)        196736    
_________________________________________________________________
batch_normalization_4 (Batch (None, 39, 5, 128)        512       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 10, 5, 128)        393344    
_________________________________________________________________
batch_normalization_5 (Batch (None, 10, 5, 128)        512       
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 3, 5, 128)         393344    
_________________________________________________________________
batch_normalization_6 (Batch (None, 3, 5, 128)         512       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 1, 5, 128)         0         
_________________________________________________________________
flatten (Flatten)            (None, 640)               0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 640)               2560      
_________________________________________________________________
dense (Dense)                (None, 512)               328192    
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
dense_3 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 2,041,410
Trainable params: 2,038,850
Non-trainable params: 2,560
_________________________________________________________________
Epoch 1/500
123/123 - 124s - loss: 0.7919 - accuracy: 0.5046 - val_loss: 0.7109 - val_accuracy: 0.4979
Epoch 2/500
123/123 - 16s - loss: 0.7180 - accuracy: 0.5584 - val_loss: 0.6916 - val_accuracy: 0.5417
Epoch 3/500
123/123 - 16s - loss: 0.6897 - accuracy: 0.5768 - val_loss: 0.6876 - val_accuracy: 0.5333
Epoch 4/500
123/123 - 15s - loss: 0.6401 - accuracy: 0.6399 - val_loss: 0.6968 - val_accuracy: 0.5562
Epoch 5/500
123/123 - 15s - loss: 0.5852 - accuracy: 0.6835 - val_loss: 0.7309 - val_accuracy: 0.5292
Epoch 6/500
123/123 - 16s - loss: 0.4980 - accuracy: 0.7677 - val_loss: 0.7730 - val_accuracy: 0.5562
Epoch 7/500
123/123 - 16s - loss: 0.4005 - accuracy: 0.8249 - val_loss: 1.4557 - val_accuracy: 0.5188
Epoch 8/500
123/123 - 15s - loss: 0.2773 - accuracy: 0.8933 - val_loss: 0.8113 - val_accuracy: 0.5646
Epoch 9/500
123/123 - 16s - loss: 0.1811 - accuracy: 0.9364 - val_loss: 1.0372 - val_accuracy: 0.5688
Epoch 10/500
123/123 - 15s - loss: 0.1110 - accuracy: 0.9622 - val_loss: 1.3075 - val_accuracy: 0.5688
Epoch 11/500
123/123 - 16s - loss: 0.0644 - accuracy: 0.9786 - val_loss: 2.0859 - val_accuracy: 0.5521
Epoch 12/500
123/123 - 16s - loss: 0.0470 - accuracy: 0.9860 - val_loss: 6.1466 - val_accuracy: 0.4979
Epoch 13/500
123/123 - 16s - loss: 0.0385 - accuracy: 0.9872 - val_loss: 1.7987 - val_accuracy: 0.6104
Epoch 14/500
123/123 - 15s - loss: 0.0289 - accuracy: 0.9916 - val_loss: 1.5665 - val_accuracy: 0.5771
Epoch 15/500
123/123 - 16s - loss: 0.0250 - accuracy: 0.9934 - val_loss: 1.4938 - val_accuracy: 0.5938
Epoch 16/500
123/123 - 15s - loss: 0.0194 - accuracy: 0.9941 - val_loss: 1.4968 - val_accuracy: 0.5771
Epoch 17/500
123/123 - 15s - loss: 0.0139 - accuracy: 0.9969 - val_loss: 5.0246 - val_accuracy: 0.5208
Epoch 18/500
123/123 - 16s - loss: 0.0233 - accuracy: 0.9916 - val_loss: 2.5882 - val_accuracy: 0.5667
Epoch 19/500
123/123 - 15s - loss: 0.0174 - accuracy: 0.9939 - val_loss: 13.7435 - val_accuracy: 0.5021
Epoch 20/500
123/123 - 15s - loss: 0.0212 - accuracy: 0.9936 - val_loss: 1.8367 - val_accuracy: 0.5979
Epoch 21/500
123/123 - 15s - loss: 0.0175 - accuracy: 0.9946 - val_loss: 1.8973 - val_accuracy: 0.6062
Epoch 22/500
123/123 - 16s - loss: 0.0183 - accuracy: 0.9957 - val_loss: 5.6643 - val_accuracy: 0.5292
Epoch 23/500
123/123 - 15s - loss: 0.0177 - accuracy: 0.9946 - val_loss: 2.7928 - val_accuracy: 0.6021
========================================
save_weights
h5_weights/MSC.pp/onehot_cnn_one_branch.h5
========================================

end time >>> Sun Oct  3 21:19:45 2021

end time >>> Sun Oct  3 21:19:45 2021

end time >>> Sun Oct  3 21:19:45 2021

end time >>> Sun Oct  3 21:19:45 2021

end time >>> Sun Oct  3 21:19:45 2021












args.model = onehot_cnn_one_branch
time used = 482.68850111961365


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 21:19:46 2021

begin time >>> Sun Oct  3 21:19:46 2021

begin time >>> Sun Oct  3 21:19:46 2021

begin time >>> Sun Oct  3 21:19:46 2021

begin time >>> Sun Oct  3 21:19:46 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_cnn_two_branch
args.type = train
args.name = MSC.pp
args.length = 10001
===========================


-> h5_weights/MSC.pp folder already exist. pass.
-> result/MSC.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/MSC.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/MSC.pp/onehot_embedding_dense folder already exist. pass.
-> result/MSC.pp/onehot_dense folder already exist. pass.
-> result/MSC.pp/onehot_resnet18 folder already exist. pass.
-> result/MSC.pp/onehot_resnet34 folder already exist. pass.
-> result/MSC.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/MSC.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/MSC.pp/embedding_dense folder already exist. pass.
-> result/MSC.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/MSC.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001, 5, 1) 0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001, 5, 1) 0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 2501, 5, 64)  1600        input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 2501, 5, 64)  1600        input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 2501, 5, 64)  256         conv2d[0][0]                     
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 2501, 5, 64)  256         conv2d_6[0][0]                   
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 626, 5, 64)   98368       batch_normalization[0][0]        
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 626, 5, 64)   98368       batch_normalization_8[0][0]      
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 626, 5, 64)   256         conv2d_1[0][0]                   
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 626, 5, 64)   256         conv2d_7[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 157, 5, 64)   98368       batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 157, 5, 64)   98368       batch_normalization_9[0][0]      
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 157, 5, 64)   256         conv2d_2[0][0]                   
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 157, 5, 64)   256         conv2d_8[0][0]                   
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 78, 5, 64)    0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 78, 5, 64)    0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 78, 5, 64)    256         max_pooling2d[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 78, 5, 64)    256         max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 20, 5, 128)   196736      batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 20, 5, 128)   196736      batch_normalization_11[0][0]     
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 20, 5, 128)   512         conv2d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 20, 5, 128)   512         conv2d_9[0][0]                   
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 5, 5, 128)    393344      batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 5, 5, 128)    393344      batch_normalization_12[0][0]     
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 5, 5, 128)    512         conv2d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 5, 5, 128)    512         conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 2, 5, 128)    393344      batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 2, 5, 128)    393344      batch_normalization_13[0][0]     
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 2, 5, 128)    512         conv2d_5[0][0]                   
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 2, 5, 128)    512         conv2d_11[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 1, 5, 128)    0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 1, 5, 128)    0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
flatten (Flatten)               (None, 640)          0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 640)          0           max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 640)          2560        flatten[0][0]                    
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 640)          2560        flatten_1[0][0]                  
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          328192      batch_normalization_7[0][0]      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          328192      batch_normalization_15[0][0]     
__________________________________________________________________________________________________
dropout (Dropout)               (None, 512)          0           dense[0][0]                      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 1024)         0           dropout[0][0]                    
                                                                 dropout_1[0][0]                  
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          524800      concatenate[0][0]                
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 512)          262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 2)            1026        dense_3[0][0]                    
==================================================================================================
Total params: 3,818,626
Trainable params: 3,813,506
Non-trainable params: 5,120
__________________________________________________________________________________________________
Found 3950 images belonging to 2 classes.
Found 3950 images belonging to 2 classes.
Epoch 1/500
Found 488 images belonging to 2 classes.
Found 488 images belonging to 2 classes.
1535/1535 - 469s - loss: 0.3951 - accuracy: 0.7982 - val_loss: 3.3879 - val_accuracy: 0.5306
Epoch 2/500
1535/1535 - 232s - loss: 0.0601 - accuracy: 0.9802 - val_loss: 1.9972 - val_accuracy: 0.6296
Epoch 3/500
1535/1535 - 230s - loss: 0.0327 - accuracy: 0.9901 - val_loss: 2.6272 - val_accuracy: 0.6084
Epoch 4/500
1535/1535 - 235s - loss: 0.0228 - accuracy: 0.9930 - val_loss: 8.3553 - val_accuracy: 0.5205
Epoch 5/500
1535/1535 - 234s - loss: 0.0239 - accuracy: 0.9933 - val_loss: 2.5293 - val_accuracy: 0.5906
Epoch 6/500
1535/1535 - 229s - loss: 0.0148 - accuracy: 0.9958 - val_loss: 3.7782 - val_accuracy: 0.6252
Epoch 7/500
1535/1535 - 233s - loss: 0.0143 - accuracy: 0.9960 - val_loss: 2.8087 - val_accuracy: 0.5800
Epoch 8/500
1535/1535 - 245s - loss: 0.0136 - accuracy: 0.9965 - val_loss: 3.0217 - val_accuracy: 0.6026
Epoch 9/500
1535/1535 - 236s - loss: 0.0101 - accuracy: 0.9970 - val_loss: 3.3580 - val_accuracy: 0.6294
Epoch 10/500
1535/1535 - 228s - loss: 0.0101 - accuracy: 0.9973 - val_loss: 3.2075 - val_accuracy: 0.6293
Epoch 11/500
1535/1535 - 223s - loss: 0.0104 - accuracy: 0.9968 - val_loss: 3.3342 - val_accuracy: 0.6151
Epoch 12/500
1535/1535 - 230s - loss: 0.0092 - accuracy: 0.9974 - val_loss: 3.5630 - val_accuracy: 0.6515
Epoch 13/500
1535/1535 - 230s - loss: 0.0092 - accuracy: 0.9975 - val_loss: 3.1086 - val_accuracy: 0.6522
Epoch 14/500
1535/1535 - 230s - loss: 0.0066 - accuracy: 0.9981 - val_loss: 3.6739 - val_accuracy: 0.6537
Epoch 15/500
1535/1535 - 228s - loss: 0.0082 - accuracy: 0.9978 - val_loss: 3.2323 - val_accuracy: 0.6435
Epoch 16/500
1535/1535 - 227s - loss: 0.0062 - accuracy: 0.9983 - val_loss: 3.6578 - val_accuracy: 0.6276
Epoch 17/500
1535/1535 - 239s - loss: 0.0054 - accuracy: 0.9985 - val_loss: 3.9459 - val_accuracy: 0.6271
Epoch 18/500
1535/1535 - 233s - loss: 0.0085 - accuracy: 0.9976 - val_loss: 3.2274 - val_accuracy: 0.6149
Epoch 19/500
1535/1535 - 239s - loss: 0.0072 - accuracy: 0.9977 - val_loss: 3.4012 - val_accuracy: 0.6166
Epoch 20/500
1535/1535 - 226s - loss: 0.0026 - accuracy: 0.9992 - val_loss: 3.6752 - val_accuracy: 0.6255
Epoch 21/500
1535/1535 - 224s - loss: 0.0056 - accuracy: 0.9986 - val_loss: 3.2807 - val_accuracy: 0.6414
Epoch 22/500
1535/1535 - 226s - loss: 0.0053 - accuracy: 0.9983 - val_loss: 3.6554 - val_accuracy: 0.6310
Epoch 23/500
1535/1535 - 224s - loss: 0.0037 - accuracy: 0.9990 - val_loss: 3.4377 - val_accuracy: 0.6171
Epoch 24/500
1535/1535 - 223s - loss: 0.0048 - accuracy: 0.9987 - val_loss: 4.8064 - val_accuracy: 0.6065
========================================
save_weights
h5_weights/MSC.pp/onehot_cnn_two_branch.h5
========================================

end time >>> Sun Oct  3 22:56:15 2021

end time >>> Sun Oct  3 22:56:15 2021

end time >>> Sun Oct  3 22:56:15 2021

end time >>> Sun Oct  3 22:56:15 2021

end time >>> Sun Oct  3 22:56:15 2021












args.model = onehot_cnn_two_branch
time used = 5789.123849153519


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 22:56:16 2021

begin time >>> Sun Oct  3 22:56:16 2021

begin time >>> Sun Oct  3 22:56:16 2021

begin time >>> Sun Oct  3 22:56:16 2021

begin time >>> Sun Oct  3 22:56:16 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_dense
args.type = train
args.name = MSC.pp
args.length = 10001
===========================


-> h5_weights/MSC.pp folder already exist. pass.
-> result/MSC.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/MSC.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/MSC.pp/onehot_embedding_dense folder already exist. pass.
-> result/MSC.pp/onehot_dense folder already exist. pass.
-> result/MSC.pp/onehot_resnet18 folder already exist. pass.
-> result/MSC.pp/onehot_resnet34 folder already exist. pass.
-> result/MSC.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/MSC.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/MSC.pp/embedding_dense folder already exist. pass.
-> result/MSC.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/MSC.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
MSC.pp
########################################

########################################
model_name
onehot_dense
########################################

Found 3950 images belonging to 2 classes.
Found 488 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 100010)            0         
_________________________________________________________________
batch_normalization (BatchNo (None, 100010)            400040    
_________________________________________________________________
dense (Dense)                (None, 512)               51205632  
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_3 (Batch (None, 512)               2048      
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_3 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 52,402,858
Trainable params: 52,198,742
Non-trainable params: 204,116
_________________________________________________________________
Epoch 1/500
123/123 - 18s - loss: 0.8030 - accuracy: 0.5219 - val_loss: 0.6739 - val_accuracy: 0.5771
Epoch 2/500
123/123 - 12s - loss: 0.7039 - accuracy: 0.5944 - val_loss: 0.6632 - val_accuracy: 0.5979
Epoch 3/500
123/123 - 12s - loss: 0.6304 - accuracy: 0.6603 - val_loss: 0.6751 - val_accuracy: 0.6042
Epoch 4/500
123/123 - 11s - loss: 0.5544 - accuracy: 0.7159 - val_loss: 0.7175 - val_accuracy: 0.6042
Epoch 5/500
123/123 - 12s - loss: 0.4484 - accuracy: 0.7953 - val_loss: 0.8171 - val_accuracy: 0.6062
Epoch 6/500
123/123 - 11s - loss: 0.3666 - accuracy: 0.8369 - val_loss: 0.9680 - val_accuracy: 0.5979
Epoch 7/500
123/123 - 11s - loss: 0.2936 - accuracy: 0.8775 - val_loss: 1.0858 - val_accuracy: 0.6021
Epoch 8/500
123/123 - 11s - loss: 0.2289 - accuracy: 0.9150 - val_loss: 1.1869 - val_accuracy: 0.5938
Epoch 9/500
123/123 - 11s - loss: 0.1789 - accuracy: 0.9352 - val_loss: 1.3115 - val_accuracy: 0.5958
Epoch 10/500
123/123 - 11s - loss: 0.1440 - accuracy: 0.9472 - val_loss: 1.4042 - val_accuracy: 0.5979
Epoch 11/500
123/123 - 12s - loss: 0.1318 - accuracy: 0.9518 - val_loss: 1.4951 - val_accuracy: 0.5958
Epoch 12/500
123/123 - 12s - loss: 0.1194 - accuracy: 0.9566 - val_loss: 1.4440 - val_accuracy: 0.6104
Epoch 13/500
123/123 - 11s - loss: 0.0943 - accuracy: 0.9681 - val_loss: 1.5262 - val_accuracy: 0.6083
Epoch 14/500
123/123 - 11s - loss: 0.0744 - accuracy: 0.9727 - val_loss: 1.6003 - val_accuracy: 0.6083
Epoch 15/500
123/123 - 11s - loss: 0.0790 - accuracy: 0.9706 - val_loss: 1.6414 - val_accuracy: 0.6083
Epoch 16/500
123/123 - 11s - loss: 0.0649 - accuracy: 0.9763 - val_loss: 1.7481 - val_accuracy: 0.6000
Epoch 17/500
123/123 - 12s - loss: 0.0571 - accuracy: 0.9788 - val_loss: 1.7484 - val_accuracy: 0.6125
Epoch 18/500
123/123 - 12s - loss: 0.0580 - accuracy: 0.9798 - val_loss: 1.7469 - val_accuracy: 0.6187
Epoch 19/500
123/123 - 11s - loss: 0.0510 - accuracy: 0.9837 - val_loss: 1.7701 - val_accuracy: 0.6042
Epoch 20/500
123/123 - 11s - loss: 0.0512 - accuracy: 0.9826 - val_loss: 1.8247 - val_accuracy: 0.6125
Epoch 21/500
123/123 - 11s - loss: 0.0502 - accuracy: 0.9821 - val_loss: 1.8672 - val_accuracy: 0.6146
Epoch 22/500
123/123 - 11s - loss: 0.0459 - accuracy: 0.9847 - val_loss: 1.8986 - val_accuracy: 0.6062
Epoch 23/500
123/123 - 12s - loss: 0.0402 - accuracy: 0.9857 - val_loss: 1.8668 - val_accuracy: 0.6208
Epoch 24/500
123/123 - 12s - loss: 0.0404 - accuracy: 0.9870 - val_loss: 1.8645 - val_accuracy: 0.6292
Epoch 25/500
123/123 - 11s - loss: 0.0419 - accuracy: 0.9849 - val_loss: 1.8812 - val_accuracy: 0.6146
Epoch 26/500
123/123 - 11s - loss: 0.0371 - accuracy: 0.9880 - val_loss: 1.9377 - val_accuracy: 0.6062
Epoch 27/500
123/123 - 11s - loss: 0.0419 - accuracy: 0.9847 - val_loss: 1.8731 - val_accuracy: 0.6208
Epoch 28/500
123/123 - 11s - loss: 0.0479 - accuracy: 0.9837 - val_loss: 1.9470 - val_accuracy: 0.6167
Epoch 29/500
123/123 - 12s - loss: 0.0378 - accuracy: 0.9872 - val_loss: 1.9676 - val_accuracy: 0.6354
Epoch 30/500
123/123 - 11s - loss: 0.0406 - accuracy: 0.9862 - val_loss: 1.9829 - val_accuracy: 0.6292
Epoch 31/500
123/123 - 11s - loss: 0.0433 - accuracy: 0.9849 - val_loss: 2.0143 - val_accuracy: 0.6333
Epoch 32/500
123/123 - 12s - loss: 0.0394 - accuracy: 0.9865 - val_loss: 2.0031 - val_accuracy: 0.6229
Epoch 33/500
123/123 - 11s - loss: 0.0305 - accuracy: 0.9900 - val_loss: 2.0155 - val_accuracy: 0.6271
Epoch 34/500
123/123 - 11s - loss: 0.0320 - accuracy: 0.9898 - val_loss: 2.0293 - val_accuracy: 0.6313
Epoch 35/500
123/123 - 12s - loss: 0.0343 - accuracy: 0.9880 - val_loss: 2.0488 - val_accuracy: 0.6417
Epoch 36/500
123/123 - 12s - loss: 0.0336 - accuracy: 0.9906 - val_loss: 2.1205 - val_accuracy: 0.6438
Epoch 37/500
123/123 - 11s - loss: 0.0374 - accuracy: 0.9867 - val_loss: 2.0581 - val_accuracy: 0.6438
Epoch 38/500
123/123 - 12s - loss: 0.0322 - accuracy: 0.9890 - val_loss: 2.0337 - val_accuracy: 0.6521
Epoch 39/500
123/123 - 11s - loss: 0.0306 - accuracy: 0.9888 - val_loss: 2.0499 - val_accuracy: 0.6500
Epoch 40/500
123/123 - 11s - loss: 0.0219 - accuracy: 0.9918 - val_loss: 2.0404 - val_accuracy: 0.6438
Epoch 41/500
123/123 - 11s - loss: 0.0201 - accuracy: 0.9941 - val_loss: 2.0193 - val_accuracy: 0.6521
Epoch 42/500
123/123 - 12s - loss: 0.0300 - accuracy: 0.9913 - val_loss: 1.9735 - val_accuracy: 0.6562
Epoch 43/500
123/123 - 11s - loss: 0.0476 - accuracy: 0.9865 - val_loss: 2.0953 - val_accuracy: 0.6458
Epoch 44/500
123/123 - 11s - loss: 0.0427 - accuracy: 0.9847 - val_loss: 2.1091 - val_accuracy: 0.6333
Epoch 45/500
123/123 - 11s - loss: 0.0306 - accuracy: 0.9880 - val_loss: 2.0448 - val_accuracy: 0.6396
Epoch 46/500
123/123 - 11s - loss: 0.0254 - accuracy: 0.9918 - val_loss: 2.0144 - val_accuracy: 0.6458
Epoch 47/500
123/123 - 12s - loss: 0.0261 - accuracy: 0.9893 - val_loss: 2.0152 - val_accuracy: 0.6500
Epoch 48/500
123/123 - 12s - loss: 0.0219 - accuracy: 0.9936 - val_loss: 2.0649 - val_accuracy: 0.6500
Epoch 49/500
123/123 - 12s - loss: 0.0224 - accuracy: 0.9926 - val_loss: 2.0256 - val_accuracy: 0.6479
Epoch 50/500
123/123 - 12s - loss: 0.0194 - accuracy: 0.9923 - val_loss: 2.1147 - val_accuracy: 0.6458
Epoch 51/500
123/123 - 12s - loss: 0.0185 - accuracy: 0.9934 - val_loss: 2.1191 - val_accuracy: 0.6500
Epoch 52/500
123/123 - 12s - loss: 0.0196 - accuracy: 0.9931 - val_loss: 2.1497 - val_accuracy: 0.6417
========================================
save_weights
h5_weights/MSC.pp/onehot_dense.h5
========================================

end time >>> Sun Oct  3 23:06:40 2021

end time >>> Sun Oct  3 23:06:40 2021

end time >>> Sun Oct  3 23:06:40 2021

end time >>> Sun Oct  3 23:06:40 2021

end time >>> Sun Oct  3 23:06:40 2021












args.model = onehot_dense
time used = 623.8174645900726


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 23:06:41 2021

begin time >>> Sun Oct  3 23:06:41 2021

begin time >>> Sun Oct  3 23:06:41 2021

begin time >>> Sun Oct  3 23:06:41 2021

begin time >>> Sun Oct  3 23:06:41 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_resnet18
args.type = train
args.name = MSC.pp
args.length = 10001
===========================


-> h5_weights/MSC.pp folder already exist. pass.
-> result/MSC.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/MSC.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/MSC.pp/onehot_embedding_dense folder already exist. pass.
-> result/MSC.pp/onehot_dense folder already exist. pass.
-> result/MSC.pp/onehot_resnet18 folder already exist. pass.
-> result/MSC.pp/onehot_resnet34 folder already exist. pass.
-> result/MSC.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/MSC.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/MSC.pp/embedding_dense folder already exist. pass.
-> result/MSC.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/MSC.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
MSC.pp
########################################

########################################
model_name
onehot_resnet18
########################################

Found 3950 images belonging to 2 classes.
Found 488 images belonging to 2 classes.
Model: "res_net_type_i"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              multiple                  1600      
_________________________________________________________________
batch_normalization (BatchNo multiple                  256       
_________________________________________________________________
max_pooling2d (MaxPooling2D) multiple                  0         
_________________________________________________________________
sequential (Sequential)      (None, 313, 1, 64)        398912    
_________________________________________________________________
sequential_2 (Sequential)    (None, 20, 1, 64)         398912    
_________________________________________________________________
sequential_4 (Sequential)    (None, 2, 1, 64)          398912    
_________________________________________________________________
sequential_6 (Sequential)    (None, 1, 1, 64)          398912    
_________________________________________________________________
global_average_pooling2d (Gl multiple                  0         
_________________________________________________________________
dense (Dense)                multiple                  130       
=================================================================
Total params: 1,597,634
Trainable params: 1,594,946
Non-trainable params: 2,688
_________________________________________________________________
Epoch 1/500
123/123 - 15s - loss: 0.7939 - accuracy: 0.5069 - val_loss: 0.6947 - val_accuracy: 0.5021
Epoch 2/500
123/123 - 15s - loss: 0.6200 - accuracy: 0.6539 - val_loss: 0.7009 - val_accuracy: 0.5000
Epoch 3/500
123/123 - 15s - loss: 0.5189 - accuracy: 0.7534 - val_loss: 0.7101 - val_accuracy: 0.5021
Epoch 4/500
123/123 - 15s - loss: 0.4161 - accuracy: 0.8344 - val_loss: 0.7783 - val_accuracy: 0.4979
Epoch 5/500
123/123 - 15s - loss: 0.3190 - accuracy: 0.8992 - val_loss: 0.8253 - val_accuracy: 0.5063
Epoch 6/500
123/123 - 15s - loss: 0.2280 - accuracy: 0.9416 - val_loss: 0.8609 - val_accuracy: 0.5292
Epoch 7/500
123/123 - 15s - loss: 0.1739 - accuracy: 0.9584 - val_loss: 0.9022 - val_accuracy: 0.5229
Epoch 8/500
123/123 - 15s - loss: 0.1240 - accuracy: 0.9758 - val_loss: 0.9543 - val_accuracy: 0.5396
Epoch 9/500
123/123 - 15s - loss: 0.1017 - accuracy: 0.9765 - val_loss: 1.0476 - val_accuracy: 0.5229
Epoch 10/500
123/123 - 15s - loss: 0.0832 - accuracy: 0.9834 - val_loss: 1.0923 - val_accuracy: 0.5229
Epoch 11/500
123/123 - 15s - loss: 0.0725 - accuracy: 0.9847 - val_loss: 1.1147 - val_accuracy: 0.4979
Epoch 12/500
123/123 - 15s - loss: 0.0760 - accuracy: 0.9814 - val_loss: 1.1354 - val_accuracy: 0.5188
Epoch 13/500
123/123 - 15s - loss: 0.0631 - accuracy: 0.9872 - val_loss: 1.1142 - val_accuracy: 0.5396
Epoch 14/500
123/123 - 15s - loss: 0.0648 - accuracy: 0.9849 - val_loss: 1.1394 - val_accuracy: 0.5396
Epoch 15/500
123/123 - 15s - loss: 0.0790 - accuracy: 0.9752 - val_loss: 1.1649 - val_accuracy: 0.5604
Epoch 16/500
123/123 - 15s - loss: 0.0941 - accuracy: 0.9689 - val_loss: 1.3600 - val_accuracy: 0.5250
Epoch 17/500
123/123 - 15s - loss: 0.1130 - accuracy: 0.9607 - val_loss: 1.3143 - val_accuracy: 0.5292
Epoch 18/500
123/123 - 15s - loss: 0.1187 - accuracy: 0.9538 - val_loss: 1.1859 - val_accuracy: 0.5792
Epoch 19/500
123/123 - 15s - loss: 0.0915 - accuracy: 0.9712 - val_loss: 1.3142 - val_accuracy: 0.5333
Epoch 20/500
123/123 - 15s - loss: 0.0724 - accuracy: 0.9752 - val_loss: 1.3859 - val_accuracy: 0.5250
Epoch 21/500
123/123 - 15s - loss: 0.0667 - accuracy: 0.9781 - val_loss: 1.2439 - val_accuracy: 0.5542
Epoch 22/500
123/123 - 15s - loss: 0.0621 - accuracy: 0.9793 - val_loss: 1.2722 - val_accuracy: 0.5396
Epoch 23/500
123/123 - 15s - loss: 0.0486 - accuracy: 0.9872 - val_loss: 1.2469 - val_accuracy: 0.5604
Epoch 24/500
123/123 - 15s - loss: 0.0392 - accuracy: 0.9872 - val_loss: 1.3600 - val_accuracy: 0.5396
Epoch 25/500
123/123 - 15s - loss: 0.0418 - accuracy: 0.9890 - val_loss: 1.4011 - val_accuracy: 0.5521
Epoch 26/500
123/123 - 15s - loss: 0.0369 - accuracy: 0.9898 - val_loss: 1.3503 - val_accuracy: 0.5521
Epoch 27/500
123/123 - 15s - loss: 0.0411 - accuracy: 0.9855 - val_loss: 1.4082 - val_accuracy: 0.5542
Epoch 28/500
123/123 - 15s - loss: 0.0481 - accuracy: 0.9834 - val_loss: 1.4596 - val_accuracy: 0.5479
========================================
save_weights
h5_weights/MSC.pp/onehot_resnet18.h5
========================================

end time >>> Sun Oct  3 23:13:57 2021

end time >>> Sun Oct  3 23:13:57 2021

end time >>> Sun Oct  3 23:13:57 2021

end time >>> Sun Oct  3 23:13:57 2021

end time >>> Sun Oct  3 23:13:57 2021












args.model = onehot_resnet18
time used = 436.6979088783264


