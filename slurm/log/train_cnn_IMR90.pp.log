************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 15:07:53 2021

begin time >>> Sun Oct  3 15:07:53 2021

begin time >>> Sun Oct  3 15:07:53 2021

begin time >>> Sun Oct  3 15:07:53 2021

begin time >>> Sun Oct  3 15:07:53 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_cnn_one_branch
args.type = train
args.name = IMR90.pp
args.length = 10001
===========================


-> make new folder: h5_weights/IMR90.pp
-> make new folder: result/IMR90.pp/onehot_cnn_one_branch
-> make new folder: result/IMR90.pp/onehot_cnn_two_branch
-> make new folder: result/IMR90.pp/onehot_embedding_dense
-> make new folder: result/IMR90.pp/onehot_dense
-> make new folder: result/IMR90.pp/onehot_resnet18
-> make new folder: result/IMR90.pp/onehot_resnet34
-> make new folder: result/IMR90.pp/embedding_cnn_one_branch
-> make new folder: result/IMR90.pp/embedding_cnn_two_branch
-> make new folder: result/IMR90.pp/embedding_dense
-> make new folder: result/IMR90.pp/onehot_embedding_cnn_one_branch
-> make new folder: result/IMR90.pp/onehot_embedding_cnn_two_branch
########################################
gen_name
IMR90.pp
########################################

########################################
model_name
onehot_cnn_one_branch
########################################

Found 3890 images belonging to 2 classes.
Found 480 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 5001, 5, 64)       1600      
_________________________________________________________________
batch_normalization (BatchNo (None, 5001, 5, 64)       256       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1251, 5, 64)       98368     
_________________________________________________________________
batch_normalization_1 (Batch (None, 1251, 5, 64)       256       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 313, 5, 64)        98368     
_________________________________________________________________
batch_normalization_2 (Batch (None, 313, 5, 64)        256       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 156, 5, 64)        0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 156, 5, 64)        256       
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 39, 5, 128)        196736    
_________________________________________________________________
batch_normalization_4 (Batch (None, 39, 5, 128)        512       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 10, 5, 128)        393344    
_________________________________________________________________
batch_normalization_5 (Batch (None, 10, 5, 128)        512       
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 3, 5, 128)         393344    
_________________________________________________________________
batch_normalization_6 (Batch (None, 3, 5, 128)         512       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 1, 5, 128)         0         
_________________________________________________________________
flatten (Flatten)            (None, 640)               0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 640)               2560      
_________________________________________________________________
dense (Dense)                (None, 512)               328192    
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
dense_3 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 2,041,410
Trainable params: 2,038,850
Non-trainable params: 2,560
_________________________________________________________________
Epoch 1/500
121/121 - 114s - loss: 0.7941 - accuracy: 0.4938 - val_loss: 0.6930 - val_accuracy: 0.4917
Epoch 2/500
121/121 - 15s - loss: 0.7190 - accuracy: 0.5539 - val_loss: 0.6948 - val_accuracy: 0.5312
Epoch 3/500
121/121 - 15s - loss: 0.6786 - accuracy: 0.5778 - val_loss: 0.7034 - val_accuracy: 0.5000
Epoch 4/500
121/121 - 15s - loss: 0.6330 - accuracy: 0.6402 - val_loss: 0.7726 - val_accuracy: 0.5021
Epoch 5/500
121/121 - 15s - loss: 0.5737 - accuracy: 0.6985 - val_loss: 0.7360 - val_accuracy: 0.5375
Epoch 6/500
121/121 - 15s - loss: 0.4835 - accuracy: 0.7693 - val_loss: 0.8656 - val_accuracy: 0.5146
Epoch 7/500
121/121 - 15s - loss: 0.3573 - accuracy: 0.8541 - val_loss: 2.7789 - val_accuracy: 0.5000
Epoch 8/500
121/121 - 15s - loss: 0.2431 - accuracy: 0.9080 - val_loss: 0.9644 - val_accuracy: 0.5875
Epoch 9/500
121/121 - 15s - loss: 0.1437 - accuracy: 0.9554 - val_loss: 0.9902 - val_accuracy: 0.5562
Epoch 10/500
121/121 - 15s - loss: 0.0981 - accuracy: 0.9692 - val_loss: 1.5797 - val_accuracy: 0.5375
Epoch 11/500
121/121 - 15s - loss: 0.0600 - accuracy: 0.9795 - val_loss: 3.2062 - val_accuracy: 0.5125
Epoch 12/500
121/121 - 15s - loss: 0.0451 - accuracy: 0.9855 - val_loss: 2.2242 - val_accuracy: 0.5437
Epoch 13/500
121/121 - 15s - loss: 0.0314 - accuracy: 0.9904 - val_loss: 6.0220 - val_accuracy: 0.5042
Epoch 14/500
121/121 - 15s - loss: 0.0273 - accuracy: 0.9904 - val_loss: 1.3767 - val_accuracy: 0.5625
Epoch 15/500
121/121 - 15s - loss: 0.0225 - accuracy: 0.9925 - val_loss: 1.6040 - val_accuracy: 0.5750
Epoch 16/500
121/121 - 15s - loss: 0.0209 - accuracy: 0.9933 - val_loss: 2.4351 - val_accuracy: 0.5292
Epoch 17/500
121/121 - 15s - loss: 0.0154 - accuracy: 0.9953 - val_loss: 3.6017 - val_accuracy: 0.5208
Epoch 18/500
121/121 - 15s - loss: 0.0131 - accuracy: 0.9964 - val_loss: 6.0697 - val_accuracy: 0.5146
========================================
save_weights
h5_weights/IMR90.pp/onehot_cnn_one_branch.h5
========================================

end time >>> Sun Oct  3 15:14:20 2021

end time >>> Sun Oct  3 15:14:20 2021

end time >>> Sun Oct  3 15:14:20 2021

end time >>> Sun Oct  3 15:14:20 2021

end time >>> Sun Oct  3 15:14:20 2021












args.model = onehot_cnn_one_branch
time used = 387.75384736061096


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 15:14:22 2021

begin time >>> Sun Oct  3 15:14:22 2021

begin time >>> Sun Oct  3 15:14:22 2021

begin time >>> Sun Oct  3 15:14:22 2021

begin time >>> Sun Oct  3 15:14:22 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_cnn_two_branch
args.type = train
args.name = IMR90.pp
args.length = 10001
===========================


-> h5_weights/IMR90.pp folder already exist. pass.
-> result/IMR90.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/IMR90.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/IMR90.pp/onehot_embedding_dense folder already exist. pass.
-> result/IMR90.pp/onehot_dense folder already exist. pass.
-> result/IMR90.pp/onehot_resnet18 folder already exist. pass.
-> result/IMR90.pp/onehot_resnet34 folder already exist. pass.
-> result/IMR90.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/IMR90.pp/embedding_dense folder already exist. pass.
-> result/IMR90.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001, 5, 1) 0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001, 5, 1) 0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 2501, 5, 64)  1600        input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 2501, 5, 64)  1600        input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 2501, 5, 64)  256         conv2d[0][0]                     
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 2501, 5, 64)  256         conv2d_6[0][0]                   
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 626, 5, 64)   98368       batch_normalization[0][0]        
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 626, 5, 64)   98368       batch_normalization_8[0][0]      
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 626, 5, 64)   256         conv2d_1[0][0]                   
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 626, 5, 64)   256         conv2d_7[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 157, 5, 64)   98368       batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 157, 5, 64)   98368       batch_normalization_9[0][0]      
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 157, 5, 64)   256         conv2d_2[0][0]                   
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 157, 5, 64)   256         conv2d_8[0][0]                   
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 78, 5, 64)    0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 78, 5, 64)    0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 78, 5, 64)    256         max_pooling2d[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 78, 5, 64)    256         max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 20, 5, 128)   196736      batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 20, 5, 128)   196736      batch_normalization_11[0][0]     
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 20, 5, 128)   512         conv2d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 20, 5, 128)   512         conv2d_9[0][0]                   
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 5, 5, 128)    393344      batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 5, 5, 128)    393344      batch_normalization_12[0][0]     
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 5, 5, 128)    512         conv2d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 5, 5, 128)    512         conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 2, 5, 128)    393344      batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 2, 5, 128)    393344      batch_normalization_13[0][0]     
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 2, 5, 128)    512         conv2d_5[0][0]                   
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 2, 5, 128)    512         conv2d_11[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 1, 5, 128)    0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 1, 5, 128)    0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
flatten (Flatten)               (None, 640)          0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 640)          0           max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 640)          2560        flatten[0][0]                    
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 640)          2560        flatten_1[0][0]                  
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          328192      batch_normalization_7[0][0]      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          328192      batch_normalization_15[0][0]     
__________________________________________________________________________________________________
dropout (Dropout)               (None, 512)          0           dense[0][0]                      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 1024)         0           dropout[0][0]                    
                                                                 dropout_1[0][0]                  
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          524800      concatenate[0][0]                
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 512)          262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 2)            1026        dense_3[0][0]                    
==================================================================================================
Total params: 3,818,626
Trainable params: 3,813,506
Non-trainable params: 5,120
__________________________________________________________________________________________________
Found 3890 images belonging to 2 classes.
Found 3890 images belonging to 2 classes.
Epoch 1/500
Found 480 images belonging to 2 classes.
Found 480 images belonging to 2 classes.
1535/1535 - 389s - loss: 0.3924 - accuracy: 0.7963 - val_loss: 7.1776 - val_accuracy: 0.4998
Epoch 2/500
1535/1535 - 235s - loss: 0.0468 - accuracy: 0.9850 - val_loss: 2.4069 - val_accuracy: 0.5995
Epoch 3/500
1535/1535 - 240s - loss: 0.0243 - accuracy: 0.9926 - val_loss: 3.2297 - val_accuracy: 0.5581
Epoch 4/500
1535/1535 - 239s - loss: 0.0230 - accuracy: 0.9937 - val_loss: 3.6840 - val_accuracy: 0.5774
Epoch 5/500
1535/1535 - 236s - loss: 0.0167 - accuracy: 0.9959 - val_loss: 5.0989 - val_accuracy: 0.5309
Epoch 6/500
1535/1535 - 233s - loss: 0.0127 - accuracy: 0.9964 - val_loss: 3.5118 - val_accuracy: 0.5741
Epoch 7/500
1535/1535 - 235s - loss: 0.0137 - accuracy: 0.9968 - val_loss: 4.0273 - val_accuracy: 0.5884
Epoch 8/500
1535/1535 - 238s - loss: 0.0098 - accuracy: 0.9977 - val_loss: 4.1154 - val_accuracy: 0.6101
Epoch 9/500
1535/1535 - 231s - loss: 0.0104 - accuracy: 0.9972 - val_loss: 4.0642 - val_accuracy: 0.5769
Epoch 10/500
1535/1535 - 235s - loss: 0.0093 - accuracy: 0.9974 - val_loss: 5.2661 - val_accuracy: 0.5407
Epoch 11/500
1535/1535 - 241s - loss: 0.0104 - accuracy: 0.9967 - val_loss: 4.3373 - val_accuracy: 0.5977
Epoch 12/500
1535/1535 - 236s - loss: 0.0065 - accuracy: 0.9982 - val_loss: 4.3426 - val_accuracy: 0.5514
Epoch 13/500
1535/1535 - 235s - loss: 0.0069 - accuracy: 0.9980 - val_loss: 4.6217 - val_accuracy: 0.5910
Epoch 14/500
1535/1535 - 232s - loss: 0.0066 - accuracy: 0.9984 - val_loss: 4.6258 - val_accuracy: 0.5805
Epoch 15/500
1535/1535 - 241s - loss: 0.0048 - accuracy: 0.9986 - val_loss: 6.9237 - val_accuracy: 0.5065
Epoch 16/500
1535/1535 - 241s - loss: 0.0080 - accuracy: 0.9983 - val_loss: 5.1044 - val_accuracy: 0.5928
Epoch 17/500
1535/1535 - 234s - loss: 0.0048 - accuracy: 0.9988 - val_loss: 7.4522 - val_accuracy: 0.5517
Epoch 18/500
1535/1535 - 229s - loss: 0.0053 - accuracy: 0.9982 - val_loss: 5.2624 - val_accuracy: 0.5955
========================================
save_weights
h5_weights/IMR90.pp/onehot_cnn_two_branch.h5
========================================

end time >>> Sun Oct  3 16:27:59 2021

end time >>> Sun Oct  3 16:27:59 2021

end time >>> Sun Oct  3 16:27:59 2021

end time >>> Sun Oct  3 16:27:59 2021

end time >>> Sun Oct  3 16:27:59 2021












args.model = onehot_cnn_two_branch
time used = 4417.388737201691


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 16:28:00 2021

begin time >>> Sun Oct  3 16:28:00 2021

begin time >>> Sun Oct  3 16:28:00 2021

begin time >>> Sun Oct  3 16:28:00 2021

begin time >>> Sun Oct  3 16:28:00 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_dense
args.type = train
args.name = IMR90.pp
args.length = 10001
===========================


-> h5_weights/IMR90.pp folder already exist. pass.
-> result/IMR90.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/IMR90.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/IMR90.pp/onehot_embedding_dense folder already exist. pass.
-> result/IMR90.pp/onehot_dense folder already exist. pass.
-> result/IMR90.pp/onehot_resnet18 folder already exist. pass.
-> result/IMR90.pp/onehot_resnet34 folder already exist. pass.
-> result/IMR90.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/IMR90.pp/embedding_dense folder already exist. pass.
-> result/IMR90.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
IMR90.pp
########################################

########################################
model_name
onehot_dense
########################################

Found 3890 images belonging to 2 classes.
Found 480 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 100010)            0         
_________________________________________________________________
batch_normalization (BatchNo (None, 100010)            400040    
_________________________________________________________________
dense (Dense)                (None, 512)               51205632  
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_3 (Batch (None, 512)               2048      
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_3 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 52,402,858
Trainable params: 52,198,742
Non-trainable params: 204,116
_________________________________________________________________
Epoch 1/500
121/121 - 54s - loss: 0.8163 - accuracy: 0.5194 - val_loss: 0.6826 - val_accuracy: 0.5688
Epoch 2/500
121/121 - 12s - loss: 0.7008 - accuracy: 0.5967 - val_loss: 0.6744 - val_accuracy: 0.6083
Epoch 3/500
121/121 - 12s - loss: 0.6215 - accuracy: 0.6615 - val_loss: 0.6822 - val_accuracy: 0.5958
Epoch 4/500
121/121 - 12s - loss: 0.5543 - accuracy: 0.7154 - val_loss: 0.6923 - val_accuracy: 0.6000
Epoch 5/500
121/121 - 11s - loss: 0.4561 - accuracy: 0.7818 - val_loss: 0.7215 - val_accuracy: 0.5917
Epoch 6/500
121/121 - 12s - loss: 0.3695 - accuracy: 0.8404 - val_loss: 0.7602 - val_accuracy: 0.5604
Epoch 7/500
121/121 - 11s - loss: 0.2828 - accuracy: 0.8841 - val_loss: 0.8058 - val_accuracy: 0.5729
Epoch 8/500
121/121 - 11s - loss: 0.2172 - accuracy: 0.9189 - val_loss: 0.8874 - val_accuracy: 0.5688
Epoch 9/500
121/121 - 11s - loss: 0.1769 - accuracy: 0.9339 - val_loss: 0.9650 - val_accuracy: 0.5625
Epoch 10/500
121/121 - 11s - loss: 0.1451 - accuracy: 0.9492 - val_loss: 1.0544 - val_accuracy: 0.5667
Epoch 11/500
121/121 - 12s - loss: 0.1292 - accuracy: 0.9549 - val_loss: 1.1195 - val_accuracy: 0.5729
Epoch 12/500
121/121 - 11s - loss: 0.0954 - accuracy: 0.9697 - val_loss: 1.1829 - val_accuracy: 0.5604
========================================
save_weights
h5_weights/IMR90.pp/onehot_dense.h5
========================================

end time >>> Sun Oct  3 16:31:15 2021

end time >>> Sun Oct  3 16:31:15 2021

end time >>> Sun Oct  3 16:31:15 2021

end time >>> Sun Oct  3 16:31:15 2021

end time >>> Sun Oct  3 16:31:15 2021












args.model = onehot_dense
time used = 194.58167672157288


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 16:31:15 2021

begin time >>> Sun Oct  3 16:31:15 2021

begin time >>> Sun Oct  3 16:31:15 2021

begin time >>> Sun Oct  3 16:31:15 2021

begin time >>> Sun Oct  3 16:31:15 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_resnet18
args.type = train
args.name = IMR90.pp
args.length = 10001
===========================


-> h5_weights/IMR90.pp folder already exist. pass.
-> result/IMR90.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/IMR90.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/IMR90.pp/onehot_embedding_dense folder already exist. pass.
-> result/IMR90.pp/onehot_dense folder already exist. pass.
-> result/IMR90.pp/onehot_resnet18 folder already exist. pass.
-> result/IMR90.pp/onehot_resnet34 folder already exist. pass.
-> result/IMR90.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/IMR90.pp/embedding_dense folder already exist. pass.
-> result/IMR90.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
IMR90.pp
########################################

########################################
model_name
onehot_resnet18
########################################

Found 3890 images belonging to 2 classes.
Found 480 images belonging to 2 classes.
Model: "res_net_type_i"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              multiple                  1600      
_________________________________________________________________
batch_normalization (BatchNo multiple                  256       
_________________________________________________________________
max_pooling2d (MaxPooling2D) multiple                  0         
_________________________________________________________________
sequential (Sequential)      (None, 313, 1, 64)        398912    
_________________________________________________________________
sequential_2 (Sequential)    (None, 20, 1, 64)         398912    
_________________________________________________________________
sequential_4 (Sequential)    (None, 2, 1, 64)          398912    
_________________________________________________________________
sequential_6 (Sequential)    (None, 1, 1, 64)          398912    
_________________________________________________________________
global_average_pooling2d (Gl multiple                  0         
_________________________________________________________________
dense (Dense)                multiple                  130       
=================================================================
Total params: 1,597,634
Trainable params: 1,594,946
Non-trainable params: 2,688
_________________________________________________________________
Epoch 1/500
121/121 - 15s - loss: 0.7999 - accuracy: 0.4922 - val_loss: 0.6941 - val_accuracy: 0.5000
Epoch 2/500
121/121 - 16s - loss: 0.5938 - accuracy: 0.6825 - val_loss: 0.6990 - val_accuracy: 0.5000
Epoch 3/500
121/121 - 15s - loss: 0.4935 - accuracy: 0.7815 - val_loss: 0.7301 - val_accuracy: 0.4917
Epoch 4/500
121/121 - 15s - loss: 0.4015 - accuracy: 0.8409 - val_loss: 0.7720 - val_accuracy: 0.5125
Epoch 5/500
121/121 - 15s - loss: 0.2964 - accuracy: 0.9051 - val_loss: 0.8392 - val_accuracy: 0.5083
Epoch 6/500
121/121 - 15s - loss: 0.2127 - accuracy: 0.9414 - val_loss: 0.8827 - val_accuracy: 0.5375
Epoch 7/500
121/121 - 15s - loss: 0.1564 - accuracy: 0.9603 - val_loss: 0.9199 - val_accuracy: 0.5208
Epoch 8/500
121/121 - 15s - loss: 0.1265 - accuracy: 0.9681 - val_loss: 0.9863 - val_accuracy: 0.5354
Epoch 9/500
121/121 - 15s - loss: 0.0951 - accuracy: 0.9819 - val_loss: 0.9581 - val_accuracy: 0.5396
Epoch 10/500
121/121 - 15s - loss: 0.0823 - accuracy: 0.9832 - val_loss: 1.1059 - val_accuracy: 0.5208
Epoch 11/500
121/121 - 15s - loss: 0.0693 - accuracy: 0.9852 - val_loss: 1.1424 - val_accuracy: 0.5708
Epoch 12/500
121/121 - 15s - loss: 0.0610 - accuracy: 0.9865 - val_loss: 1.2357 - val_accuracy: 0.5646
Epoch 13/500
121/121 - 15s - loss: 0.0679 - accuracy: 0.9806 - val_loss: 1.2867 - val_accuracy: 0.5479
Epoch 14/500
121/121 - 15s - loss: 0.0820 - accuracy: 0.9712 - val_loss: 1.3589 - val_accuracy: 0.5437
Epoch 15/500
121/121 - 15s - loss: 0.0963 - accuracy: 0.9679 - val_loss: 1.4108 - val_accuracy: 0.5188
Epoch 16/500
121/121 - 15s - loss: 0.0954 - accuracy: 0.9715 - val_loss: 1.3826 - val_accuracy: 0.5354
Epoch 17/500
121/121 - 15s - loss: 0.0929 - accuracy: 0.9686 - val_loss: 1.3829 - val_accuracy: 0.5250
Epoch 18/500
121/121 - 15s - loss: 0.1071 - accuracy: 0.9603 - val_loss: 1.4647 - val_accuracy: 0.5146
Epoch 19/500
121/121 - 15s - loss: 0.0944 - accuracy: 0.9655 - val_loss: 1.3461 - val_accuracy: 0.5500
Epoch 20/500
121/121 - 15s - loss: 0.0644 - accuracy: 0.9764 - val_loss: 1.3299 - val_accuracy: 0.5771
Epoch 21/500
121/121 - 15s - loss: 0.0562 - accuracy: 0.9824 - val_loss: 1.3675 - val_accuracy: 0.5708
Epoch 22/500
121/121 - 15s - loss: 0.0393 - accuracy: 0.9878 - val_loss: 1.3965 - val_accuracy: 0.5688
Epoch 23/500
121/121 - 15s - loss: 0.0393 - accuracy: 0.9889 - val_loss: 1.4307 - val_accuracy: 0.5375
Epoch 24/500
121/121 - 15s - loss: 0.0394 - accuracy: 0.9876 - val_loss: 1.3293 - val_accuracy: 0.5417
Epoch 25/500
121/121 - 15s - loss: 0.0319 - accuracy: 0.9917 - val_loss: 1.4208 - val_accuracy: 0.5625
Epoch 26/500
121/121 - 15s - loss: 0.0282 - accuracy: 0.9927 - val_loss: 1.4310 - val_accuracy: 0.5562
Epoch 27/500
121/121 - 15s - loss: 0.0324 - accuracy: 0.9909 - val_loss: 1.3583 - val_accuracy: 0.5771
Epoch 28/500
121/121 - 15s - loss: 0.0359 - accuracy: 0.9870 - val_loss: 1.5883 - val_accuracy: 0.5562
Epoch 29/500
121/121 - 15s - loss: 0.0498 - accuracy: 0.9816 - val_loss: 1.4390 - val_accuracy: 0.5896
Epoch 30/500
121/121 - 15s - loss: 0.0443 - accuracy: 0.9850 - val_loss: 1.4974 - val_accuracy: 0.5667
Epoch 31/500
121/121 - 15s - loss: 0.0532 - accuracy: 0.9824 - val_loss: 1.5489 - val_accuracy: 0.5688
Epoch 32/500
121/121 - 15s - loss: 0.0529 - accuracy: 0.9787 - val_loss: 1.5215 - val_accuracy: 0.5646
Epoch 33/500
121/121 - 15s - loss: 0.0564 - accuracy: 0.9772 - val_loss: 1.4850 - val_accuracy: 0.5854
Epoch 34/500
121/121 - 15s - loss: 0.0522 - accuracy: 0.9800 - val_loss: 1.4697 - val_accuracy: 0.5979
Epoch 35/500
121/121 - 15s - loss: 0.0430 - accuracy: 0.9865 - val_loss: 1.5407 - val_accuracy: 0.5792
Epoch 36/500
121/121 - 17s - loss: 0.0342 - accuracy: 0.9876 - val_loss: 1.5186 - val_accuracy: 0.5875
Epoch 37/500
121/121 - 16s - loss: 0.0240 - accuracy: 0.9938 - val_loss: 1.5231 - val_accuracy: 0.5979
Epoch 38/500
121/121 - 15s - loss: 0.0291 - accuracy: 0.9914 - val_loss: 1.4883 - val_accuracy: 0.5917
Epoch 39/500
121/121 - 15s - loss: 0.0273 - accuracy: 0.9907 - val_loss: 1.5423 - val_accuracy: 0.5479
Epoch 40/500
121/121 - 15s - loss: 0.0293 - accuracy: 0.9902 - val_loss: 1.5707 - val_accuracy: 0.6000
Epoch 41/500
121/121 - 15s - loss: 0.0276 - accuracy: 0.9899 - val_loss: 1.4546 - val_accuracy: 0.6021
Epoch 42/500
121/121 - 15s - loss: 0.0395 - accuracy: 0.9863 - val_loss: 1.5676 - val_accuracy: 0.5917
Epoch 43/500
121/121 - 15s - loss: 0.0520 - accuracy: 0.9824 - val_loss: 1.5130 - val_accuracy: 0.6042
Epoch 44/500
121/121 - 15s - loss: 0.0489 - accuracy: 0.9847 - val_loss: 1.5091 - val_accuracy: 0.5875
Epoch 45/500
121/121 - 15s - loss: 0.0387 - accuracy: 0.9860 - val_loss: 1.5142 - val_accuracy: 0.5875
Epoch 46/500
121/121 - 15s - loss: 0.0316 - accuracy: 0.9891 - val_loss: 1.5505 - val_accuracy: 0.5854
Epoch 47/500
121/121 - 15s - loss: 0.0446 - accuracy: 0.9855 - val_loss: 1.5379 - val_accuracy: 0.6187
Epoch 48/500
121/121 - 15s - loss: 0.0354 - accuracy: 0.9889 - val_loss: 1.5503 - val_accuracy: 0.5958
Epoch 49/500
121/121 - 15s - loss: 0.0284 - accuracy: 0.9909 - val_loss: 1.6298 - val_accuracy: 0.5938
Epoch 50/500
121/121 - 15s - loss: 0.0272 - accuracy: 0.9904 - val_loss: 1.7263 - val_accuracy: 0.5667
Epoch 51/500
121/121 - 15s - loss: 0.0319 - accuracy: 0.9891 - val_loss: 1.6140 - val_accuracy: 0.5792
Epoch 52/500
121/121 - 15s - loss: 0.0288 - accuracy: 0.9909 - val_loss: 1.6509 - val_accuracy: 0.5813
Epoch 53/500
121/121 - 15s - loss: 0.0278 - accuracy: 0.9917 - val_loss: 1.6298 - val_accuracy: 0.5958
Epoch 54/500
121/121 - 15s - loss: 0.0215 - accuracy: 0.9930 - val_loss: 1.6604 - val_accuracy: 0.5854
Epoch 55/500
121/121 - 15s - loss: 0.0276 - accuracy: 0.9899 - val_loss: 1.5411 - val_accuracy: 0.5979
Epoch 56/500
121/121 - 15s - loss: 0.0330 - accuracy: 0.9896 - val_loss: 1.6137 - val_accuracy: 0.5896
Epoch 57/500
121/121 - 15s - loss: 0.0290 - accuracy: 0.9907 - val_loss: 1.6137 - val_accuracy: 0.5979
========================================
save_weights
h5_weights/IMR90.pp/onehot_resnet18.h5
========================================

end time >>> Sun Oct  3 16:45:55 2021

end time >>> Sun Oct  3 16:45:55 2021

end time >>> Sun Oct  3 16:45:55 2021

end time >>> Sun Oct  3 16:45:55 2021

end time >>> Sun Oct  3 16:45:55 2021












args.model = onehot_resnet18
time used = 879.780446767807


