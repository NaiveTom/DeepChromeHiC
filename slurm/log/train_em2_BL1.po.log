************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 00:01:08 2021

begin time >>> Sun Oct  3 00:01:08 2021

begin time >>> Sun Oct  3 00:01:08 2021

begin time >>> Sun Oct  3 00:01:08 2021

begin time >>> Sun Oct  3 00:01:08 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_dense
args.type = train
args.name = BL1.po
args.length = 10001
===========================


-> h5_weights/BL1.po folder already exist. pass.
-> result/BL1.po/onehot_cnn_one_branch folder already exist. pass.
-> result/BL1.po/onehot_cnn_two_branch folder already exist. pass.
-> result/BL1.po/onehot_embedding_dense folder already exist. pass.
-> result/BL1.po/onehot_dense folder already exist. pass.
-> result/BL1.po/onehot_resnet18 folder already exist. pass.
-> result/BL1.po/onehot_resnet34 folder already exist. pass.
-> result/BL1.po/embedding_cnn_one_branch folder already exist. pass.
-> result/BL1.po/embedding_cnn_two_branch folder already exist. pass.
-> result/BL1.po/embedding_dense folder already exist. pass.
-> result/BL1.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/BL1.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
BL1.po
########################################

########################################
model_name
onehot_embedding_dense
########################################

Found 4942 images belonging to 2 classes.
Found 610 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 20002, 5, 1, 8)    32776     
_________________________________________________________________
flatten (Flatten)            (None, 800080)            0         
_________________________________________________________________
batch_normalization (BatchNo (None, 800080)            3200320   
_________________________________________________________________
dense (Dense)                (None, 512)               409641472 
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_3 (Batch (None, 512)               2048      
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_3 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 413,671,754
Trainable params: 412,067,498
Non-trainable params: 1,604,256
_________________________________________________________________
Epoch 1/500
154/154 - 74s - loss: 0.7653 - accuracy: 0.5540 - val_loss: 0.7207 - val_accuracy: 0.5000
Epoch 2/500
154/154 - 28s - loss: 0.6685 - accuracy: 0.6293 - val_loss: 0.8423 - val_accuracy: 0.5000
Epoch 3/500
154/154 - 28s - loss: 0.5868 - accuracy: 0.6978 - val_loss: 1.0873 - val_accuracy: 0.5000
Epoch 4/500
154/154 - 29s - loss: 0.4714 - accuracy: 0.7833 - val_loss: 1.3322 - val_accuracy: 0.5115
Epoch 5/500
154/154 - 29s - loss: 0.3753 - accuracy: 0.8399 - val_loss: 1.5044 - val_accuracy: 0.5247
Epoch 6/500
154/154 - 29s - loss: 0.2725 - accuracy: 0.8919 - val_loss: 1.7995 - val_accuracy: 0.5411
Epoch 7/500
154/154 - 28s - loss: 0.2131 - accuracy: 0.9159 - val_loss: 2.0346 - val_accuracy: 0.5378
Epoch 8/500
154/154 - 29s - loss: 0.1563 - accuracy: 0.9401 - val_loss: 2.1539 - val_accuracy: 0.5510
Epoch 9/500
154/154 - 28s - loss: 0.1245 - accuracy: 0.9580 - val_loss: 2.3418 - val_accuracy: 0.5461
Epoch 10/500
154/154 - 29s - loss: 0.0949 - accuracy: 0.9660 - val_loss: 2.4336 - val_accuracy: 0.5592
Epoch 11/500
154/154 - 29s - loss: 0.0789 - accuracy: 0.9739 - val_loss: 2.4991 - val_accuracy: 0.5625
Epoch 12/500
154/154 - 28s - loss: 0.0711 - accuracy: 0.9760 - val_loss: 2.5804 - val_accuracy: 0.5526
Epoch 13/500
154/154 - 29s - loss: 0.0620 - accuracy: 0.9780 - val_loss: 2.6227 - val_accuracy: 0.5641
Epoch 14/500
154/154 - 29s - loss: 0.0547 - accuracy: 0.9819 - val_loss: 2.6444 - val_accuracy: 0.5658
Epoch 15/500
154/154 - 29s - loss: 0.0511 - accuracy: 0.9819 - val_loss: 2.7014 - val_accuracy: 0.5691
Epoch 16/500
154/154 - 28s - loss: 0.0450 - accuracy: 0.9847 - val_loss: 2.7337 - val_accuracy: 0.5641
Epoch 17/500
154/154 - 28s - loss: 0.0405 - accuracy: 0.9880 - val_loss: 2.8463 - val_accuracy: 0.5609
Epoch 18/500
154/154 - 28s - loss: 0.0405 - accuracy: 0.9851 - val_loss: 2.8956 - val_accuracy: 0.5592
Epoch 19/500
154/154 - 28s - loss: 0.0501 - accuracy: 0.9833 - val_loss: 2.8794 - val_accuracy: 0.5625
Epoch 20/500
154/154 - 28s - loss: 0.0537 - accuracy: 0.9827 - val_loss: 2.9534 - val_accuracy: 0.5477
Epoch 21/500
154/154 - 28s - loss: 0.0498 - accuracy: 0.9817 - val_loss: 2.8831 - val_accuracy: 0.5477
Epoch 22/500
154/154 - 28s - loss: 0.0460 - accuracy: 0.9853 - val_loss: 2.8290 - val_accuracy: 0.5641
Epoch 23/500
154/154 - 28s - loss: 0.0446 - accuracy: 0.9864 - val_loss: 2.8641 - val_accuracy: 0.5543
Epoch 24/500
154/154 - 28s - loss: 0.0506 - accuracy: 0.9841 - val_loss: 2.8489 - val_accuracy: 0.5576
Epoch 25/500
154/154 - 29s - loss: 0.0443 - accuracy: 0.9853 - val_loss: 2.9539 - val_accuracy: 0.5559
========================================
save_weights
h5_weights/BL1.po/onehot_embedding_dense.h5
========================================

end time >>> Sun Oct  3 00:14:05 2021

end time >>> Sun Oct  3 00:14:05 2021

end time >>> Sun Oct  3 00:14:05 2021

end time >>> Sun Oct  3 00:14:05 2021

end time >>> Sun Oct  3 00:14:05 2021












args.model = onehot_embedding_dense
time used = 776.7861618995667


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 00:14:06 2021

begin time >>> Sun Oct  3 00:14:06 2021

begin time >>> Sun Oct  3 00:14:06 2021

begin time >>> Sun Oct  3 00:14:06 2021

begin time >>> Sun Oct  3 00:14:06 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_one_branch
args.type = train
args.name = BL1.po
args.length = 10001
===========================


-> h5_weights/BL1.po folder already exist. pass.
-> result/BL1.po/onehot_cnn_one_branch folder already exist. pass.
-> result/BL1.po/onehot_cnn_two_branch folder already exist. pass.
-> result/BL1.po/onehot_embedding_dense folder already exist. pass.
-> result/BL1.po/onehot_dense folder already exist. pass.
-> result/BL1.po/onehot_resnet18 folder already exist. pass.
-> result/BL1.po/onehot_resnet34 folder already exist. pass.
-> result/BL1.po/embedding_cnn_one_branch folder already exist. pass.
-> result/BL1.po/embedding_cnn_two_branch folder already exist. pass.
-> result/BL1.po/embedding_dense folder already exist. pass.
-> result/BL1.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/BL1.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
BL1.po
########################################

########################################
model_name
onehot_embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
155/155 - 22s - loss: 0.8759 - accuracy: 0.5012 - val_loss: 0.6919 - val_accuracy: 0.5254
Epoch 2/500
155/155 - 21s - loss: 0.8712 - accuracy: 0.5067 - val_loss: 0.6918 - val_accuracy: 0.5254
Epoch 3/500
155/155 - 21s - loss: 0.8472 - accuracy: 0.5109 - val_loss: 0.6937 - val_accuracy: 0.5237
Epoch 4/500
155/155 - 21s - loss: 0.8456 - accuracy: 0.5164 - val_loss: 0.6993 - val_accuracy: 0.5041
Epoch 5/500
155/155 - 21s - loss: 0.8275 - accuracy: 0.5301 - val_loss: 0.7051 - val_accuracy: 0.4943
Epoch 6/500
155/155 - 21s - loss: 0.8296 - accuracy: 0.5134 - val_loss: 0.7066 - val_accuracy: 0.4828
Epoch 7/500
155/155 - 21s - loss: 0.8187 - accuracy: 0.5202 - val_loss: 0.7060 - val_accuracy: 0.4910
Epoch 8/500
155/155 - 21s - loss: 0.8064 - accuracy: 0.5439 - val_loss: 0.7051 - val_accuracy: 0.4910
Epoch 9/500
155/155 - 22s - loss: 0.8258 - accuracy: 0.5192 - val_loss: 0.7038 - val_accuracy: 0.5008
Epoch 10/500
155/155 - 21s - loss: 0.8223 - accuracy: 0.5202 - val_loss: 0.7027 - val_accuracy: 0.4992
Epoch 11/500
155/155 - 21s - loss: 0.8007 - accuracy: 0.5358 - val_loss: 0.7018 - val_accuracy: 0.4975
Epoch 12/500
155/155 - 21s - loss: 0.7825 - accuracy: 0.5500 - val_loss: 0.7006 - val_accuracy: 0.5074
Epoch 13/500
155/155 - 21s - loss: 0.7970 - accuracy: 0.5471 - val_loss: 0.6999 - val_accuracy: 0.5057
Epoch 14/500
155/155 - 21s - loss: 0.7755 - accuracy: 0.5536 - val_loss: 0.6991 - val_accuracy: 0.5123
Epoch 15/500
155/155 - 21s - loss: 0.7947 - accuracy: 0.5433 - val_loss: 0.6986 - val_accuracy: 0.5090
Epoch 16/500
155/155 - 21s - loss: 0.7969 - accuracy: 0.5455 - val_loss: 0.6978 - val_accuracy: 0.5106
Epoch 17/500
155/155 - 22s - loss: 0.7730 - accuracy: 0.5627 - val_loss: 0.6965 - val_accuracy: 0.5139
Epoch 18/500
155/155 - 22s - loss: 0.7630 - accuracy: 0.5591 - val_loss: 0.6958 - val_accuracy: 0.5123
Epoch 19/500
155/155 - 21s - loss: 0.7509 - accuracy: 0.5759 - val_loss: 0.6951 - val_accuracy: 0.5205
Epoch 20/500
155/155 - 22s - loss: 0.7451 - accuracy: 0.5728 - val_loss: 0.6945 - val_accuracy: 0.5221
Epoch 21/500
155/155 - 22s - loss: 0.7510 - accuracy: 0.5735 - val_loss: 0.6941 - val_accuracy: 0.5188
========================================
save_weights
h5_weights/BL1.po/onehot_embedding_cnn_one_branch.h5
========================================

end time >>> Sun Oct  3 00:21:51 2021

end time >>> Sun Oct  3 00:21:51 2021

end time >>> Sun Oct  3 00:21:51 2021

end time >>> Sun Oct  3 00:21:51 2021

end time >>> Sun Oct  3 00:21:51 2021












args.model = onehot_embedding_cnn_one_branch
time used = 465.0852737426758


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 00:21:52 2021

begin time >>> Sun Oct  3 00:21:52 2021

begin time >>> Sun Oct  3 00:21:52 2021

begin time >>> Sun Oct  3 00:21:52 2021

begin time >>> Sun Oct  3 00:21:52 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_two_branch
args.type = train
args.name = BL1.po
args.length = 10001
===========================


-> h5_weights/BL1.po folder already exist. pass.
-> result/BL1.po/onehot_cnn_one_branch folder already exist. pass.
-> result/BL1.po/onehot_cnn_two_branch folder already exist. pass.
-> result/BL1.po/onehot_embedding_dense folder already exist. pass.
-> result/BL1.po/onehot_dense folder already exist. pass.
-> result/BL1.po/onehot_resnet18 folder already exist. pass.
-> result/BL1.po/onehot_resnet34 folder already exist. pass.
-> result/BL1.po/embedding_cnn_one_branch folder already exist. pass.
-> result/BL1.po/embedding_cnn_two_branch folder already exist. pass.
-> result/BL1.po/embedding_dense folder already exist. pass.
-> result/BL1.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/BL1.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
BL1.po
########################################

########################################
model_name
onehot_embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
155/155 - 22s - loss: 0.9018 - accuracy: 0.4883 - val_loss: 0.6927 - val_accuracy: 0.5254
Epoch 2/500
155/155 - 21s - loss: 0.8928 - accuracy: 0.5026 - val_loss: 0.6923 - val_accuracy: 0.5254
Epoch 3/500
155/155 - 21s - loss: 0.8831 - accuracy: 0.5067 - val_loss: 0.6934 - val_accuracy: 0.5270
Epoch 4/500
155/155 - 21s - loss: 0.8705 - accuracy: 0.5101 - val_loss: 0.7018 - val_accuracy: 0.4992
Epoch 5/500
155/155 - 21s - loss: 0.8443 - accuracy: 0.5202 - val_loss: 0.7071 - val_accuracy: 0.4894
Epoch 6/500
155/155 - 21s - loss: 0.8252 - accuracy: 0.5285 - val_loss: 0.7093 - val_accuracy: 0.4763
Epoch 7/500
155/155 - 21s - loss: 0.8372 - accuracy: 0.5249 - val_loss: 0.7092 - val_accuracy: 0.4861
Epoch 8/500
155/155 - 21s - loss: 0.8260 - accuracy: 0.5372 - val_loss: 0.7075 - val_accuracy: 0.4877
Epoch 9/500
155/155 - 21s - loss: 0.8130 - accuracy: 0.5366 - val_loss: 0.7072 - val_accuracy: 0.4926
Epoch 10/500
155/155 - 21s - loss: 0.8010 - accuracy: 0.5496 - val_loss: 0.7054 - val_accuracy: 0.4877
Epoch 11/500
155/155 - 21s - loss: 0.8183 - accuracy: 0.5441 - val_loss: 0.7040 - val_accuracy: 0.4975
Epoch 12/500
155/155 - 21s - loss: 0.7924 - accuracy: 0.5538 - val_loss: 0.7025 - val_accuracy: 0.5074
Epoch 13/500
155/155 - 21s - loss: 0.7858 - accuracy: 0.5643 - val_loss: 0.7013 - val_accuracy: 0.5074
Epoch 14/500
155/155 - 21s - loss: 0.7731 - accuracy: 0.5637 - val_loss: 0.7006 - val_accuracy: 0.5074
Epoch 15/500
155/155 - 21s - loss: 0.7841 - accuracy: 0.5504 - val_loss: 0.6996 - val_accuracy: 0.5172
Epoch 16/500
155/155 - 21s - loss: 0.7768 - accuracy: 0.5595 - val_loss: 0.6977 - val_accuracy: 0.5237
Epoch 17/500
155/155 - 21s - loss: 0.7620 - accuracy: 0.5724 - val_loss: 0.6970 - val_accuracy: 0.5286
Epoch 18/500
155/155 - 21s - loss: 0.7451 - accuracy: 0.5838 - val_loss: 0.6962 - val_accuracy: 0.5303
Epoch 19/500
155/155 - 21s - loss: 0.7482 - accuracy: 0.5757 - val_loss: 0.6957 - val_accuracy: 0.5221
Epoch 20/500
155/155 - 21s - loss: 0.7420 - accuracy: 0.5858 - val_loss: 0.6945 - val_accuracy: 0.5336
Epoch 21/500
155/155 - 21s - loss: 0.7383 - accuracy: 0.5795 - val_loss: 0.6939 - val_accuracy: 0.5401
Epoch 22/500
155/155 - 21s - loss: 0.7305 - accuracy: 0.5870 - val_loss: 0.6931 - val_accuracy: 0.5483
Epoch 23/500
155/155 - 21s - loss: 0.7344 - accuracy: 0.5860 - val_loss: 0.6928 - val_accuracy: 0.5516
Epoch 24/500
155/155 - 21s - loss: 0.7271 - accuracy: 0.5953 - val_loss: 0.6912 - val_accuracy: 0.5516
Epoch 25/500
155/155 - 21s - loss: 0.7116 - accuracy: 0.6121 - val_loss: 0.6906 - val_accuracy: 0.5581
Epoch 26/500
155/155 - 21s - loss: 0.7180 - accuracy: 0.5945 - val_loss: 0.6902 - val_accuracy: 0.5614
Epoch 27/500
155/155 - 21s - loss: 0.7079 - accuracy: 0.6103 - val_loss: 0.6898 - val_accuracy: 0.5565
Epoch 28/500
155/155 - 22s - loss: 0.6949 - accuracy: 0.6115 - val_loss: 0.6886 - val_accuracy: 0.5696
Epoch 29/500
155/155 - 21s - loss: 0.6839 - accuracy: 0.6293 - val_loss: 0.6875 - val_accuracy: 0.5843
Epoch 30/500
155/155 - 21s - loss: 0.6944 - accuracy: 0.6206 - val_loss: 0.6873 - val_accuracy: 0.5794
Epoch 31/500
155/155 - 21s - loss: 0.6875 - accuracy: 0.6168 - val_loss: 0.6860 - val_accuracy: 0.5859
Epoch 32/500
155/155 - 21s - loss: 0.6734 - accuracy: 0.6208 - val_loss: 0.6857 - val_accuracy: 0.5827
Epoch 33/500
155/155 - 21s - loss: 0.6772 - accuracy: 0.6307 - val_loss: 0.6846 - val_accuracy: 0.5876
Epoch 34/500
155/155 - 21s - loss: 0.6699 - accuracy: 0.6335 - val_loss: 0.6839 - val_accuracy: 0.5810
Epoch 35/500
155/155 - 21s - loss: 0.6565 - accuracy: 0.6418 - val_loss: 0.6834 - val_accuracy: 0.5794
Epoch 36/500
155/155 - 21s - loss: 0.6538 - accuracy: 0.6493 - val_loss: 0.6832 - val_accuracy: 0.5810
Epoch 37/500
155/155 - 21s - loss: 0.6510 - accuracy: 0.6538 - val_loss: 0.6829 - val_accuracy: 0.5777
Epoch 38/500
155/155 - 21s - loss: 0.6365 - accuracy: 0.6649 - val_loss: 0.6822 - val_accuracy: 0.5810
Epoch 39/500
155/155 - 21s - loss: 0.6120 - accuracy: 0.6746 - val_loss: 0.6816 - val_accuracy: 0.5777
Epoch 40/500
155/155 - 21s - loss: 0.6277 - accuracy: 0.6679 - val_loss: 0.6814 - val_accuracy: 0.5827
Epoch 41/500
155/155 - 21s - loss: 0.5960 - accuracy: 0.6815 - val_loss: 0.6810 - val_accuracy: 0.5908
Epoch 42/500
155/155 - 21s - loss: 0.5992 - accuracy: 0.6884 - val_loss: 0.6805 - val_accuracy: 0.5908
Epoch 43/500
155/155 - 21s - loss: 0.5956 - accuracy: 0.6947 - val_loss: 0.6803 - val_accuracy: 0.5827
Epoch 44/500
155/155 - 21s - loss: 0.5880 - accuracy: 0.6989 - val_loss: 0.6805 - val_accuracy: 0.5761
Epoch 45/500
155/155 - 21s - loss: 0.5826 - accuracy: 0.7038 - val_loss: 0.6805 - val_accuracy: 0.5777
Epoch 46/500
155/155 - 21s - loss: 0.5761 - accuracy: 0.6991 - val_loss: 0.6806 - val_accuracy: 0.5794
Epoch 47/500
155/155 - 21s - loss: 0.5761 - accuracy: 0.6975 - val_loss: 0.6806 - val_accuracy: 0.5810
Epoch 48/500
155/155 - 21s - loss: 0.5508 - accuracy: 0.7167 - val_loss: 0.6799 - val_accuracy: 0.5810
Epoch 49/500
155/155 - 21s - loss: 0.5621 - accuracy: 0.7157 - val_loss: 0.6796 - val_accuracy: 0.5859
Epoch 50/500
155/155 - 21s - loss: 0.5384 - accuracy: 0.7299 - val_loss: 0.6808 - val_accuracy: 0.5843
Epoch 51/500
155/155 - 21s - loss: 0.5476 - accuracy: 0.7307 - val_loss: 0.6808 - val_accuracy: 0.5827
Epoch 52/500
155/155 - 21s - loss: 0.5267 - accuracy: 0.7331 - val_loss: 0.6804 - val_accuracy: 0.5859
Epoch 53/500
155/155 - 21s - loss: 0.5169 - accuracy: 0.7392 - val_loss: 0.6806 - val_accuracy: 0.5892
Epoch 54/500
155/155 - 21s - loss: 0.5092 - accuracy: 0.7469 - val_loss: 0.6804 - val_accuracy: 0.5892
Epoch 55/500
155/155 - 21s - loss: 0.5163 - accuracy: 0.7424 - val_loss: 0.6809 - val_accuracy: 0.5941
Epoch 56/500
155/155 - 21s - loss: 0.4720 - accuracy: 0.7770 - val_loss: 0.6817 - val_accuracy: 0.5925
Epoch 57/500
155/155 - 21s - loss: 0.4973 - accuracy: 0.7582 - val_loss: 0.6824 - val_accuracy: 0.5941
Epoch 58/500
155/155 - 21s - loss: 0.4677 - accuracy: 0.7715 - val_loss: 0.6827 - val_accuracy: 0.5974
Epoch 59/500
155/155 - 21s - loss: 0.4606 - accuracy: 0.7805 - val_loss: 0.6838 - val_accuracy: 0.5908
Epoch 60/500
155/155 - 21s - loss: 0.4611 - accuracy: 0.7798 - val_loss: 0.6844 - val_accuracy: 0.5908
Epoch 61/500
155/155 - 21s - loss: 0.4486 - accuracy: 0.7904 - val_loss: 0.6850 - val_accuracy: 0.5908
Epoch 62/500
155/155 - 21s - loss: 0.4422 - accuracy: 0.7936 - val_loss: 0.6851 - val_accuracy: 0.5892
Epoch 63/500
155/155 - 21s - loss: 0.4339 - accuracy: 0.7926 - val_loss: 0.6858 - val_accuracy: 0.5941
Epoch 64/500
155/155 - 21s - loss: 0.4302 - accuracy: 0.8031 - val_loss: 0.6882 - val_accuracy: 0.5827
Epoch 65/500
155/155 - 21s - loss: 0.4058 - accuracy: 0.8146 - val_loss: 0.6891 - val_accuracy: 0.5957
Epoch 66/500
155/155 - 21s - loss: 0.4127 - accuracy: 0.8090 - val_loss: 0.6898 - val_accuracy: 0.6023
Epoch 67/500
155/155 - 21s - loss: 0.3957 - accuracy: 0.8157 - val_loss: 0.6896 - val_accuracy: 0.6023
Epoch 68/500
155/155 - 21s - loss: 0.3916 - accuracy: 0.8227 - val_loss: 0.6911 - val_accuracy: 0.6039
Epoch 69/500
155/155 - 21s - loss: 0.3883 - accuracy: 0.8229 - val_loss: 0.6930 - val_accuracy: 0.6007
Epoch 70/500
155/155 - 21s - loss: 0.3700 - accuracy: 0.8393 - val_loss: 0.6940 - val_accuracy: 0.6056
Epoch 71/500
155/155 - 21s - loss: 0.3572 - accuracy: 0.8389 - val_loss: 0.6958 - val_accuracy: 0.6007
Epoch 72/500
155/155 - 21s - loss: 0.3451 - accuracy: 0.8432 - val_loss: 0.6969 - val_accuracy: 0.6105
Epoch 73/500
155/155 - 21s - loss: 0.3561 - accuracy: 0.8470 - val_loss: 0.6990 - val_accuracy: 0.6088
Epoch 74/500
155/155 - 21s - loss: 0.3341 - accuracy: 0.8555 - val_loss: 0.7012 - val_accuracy: 0.6039
Epoch 75/500
155/155 - 21s - loss: 0.3224 - accuracy: 0.8630 - val_loss: 0.7026 - val_accuracy: 0.6039
Epoch 76/500
155/155 - 21s - loss: 0.3217 - accuracy: 0.8632 - val_loss: 0.7035 - val_accuracy: 0.6056
Epoch 77/500
155/155 - 21s - loss: 0.3113 - accuracy: 0.8669 - val_loss: 0.7064 - val_accuracy: 0.6039
Epoch 78/500
155/155 - 21s - loss: 0.3063 - accuracy: 0.8679 - val_loss: 0.7085 - val_accuracy: 0.6056
Epoch 79/500
155/155 - 21s - loss: 0.2944 - accuracy: 0.8778 - val_loss: 0.7109 - val_accuracy: 0.6072
Epoch 80/500
155/155 - 21s - loss: 0.2882 - accuracy: 0.8828 - val_loss: 0.7119 - val_accuracy: 0.6039
Epoch 81/500
155/155 - 21s - loss: 0.2887 - accuracy: 0.8834 - val_loss: 0.7143 - val_accuracy: 0.6039
Epoch 82/500
155/155 - 21s - loss: 0.2806 - accuracy: 0.8865 - val_loss: 0.7173 - val_accuracy: 0.6039
Epoch 83/500
155/155 - 21s - loss: 0.2719 - accuracy: 0.8857 - val_loss: 0.7192 - val_accuracy: 0.6039
Epoch 84/500
155/155 - 21s - loss: 0.2668 - accuracy: 0.8899 - val_loss: 0.7211 - val_accuracy: 0.6039
Epoch 85/500
155/155 - 21s - loss: 0.2647 - accuracy: 0.8905 - val_loss: 0.7252 - val_accuracy: 0.6039
Epoch 86/500
155/155 - 21s - loss: 0.2483 - accuracy: 0.8984 - val_loss: 0.7275 - val_accuracy: 0.6023
Epoch 87/500
155/155 - 21s - loss: 0.2414 - accuracy: 0.9023 - val_loss: 0.7322 - val_accuracy: 0.6023
Epoch 88/500
155/155 - 21s - loss: 0.2239 - accuracy: 0.9144 - val_loss: 0.7358 - val_accuracy: 0.6007
Epoch 89/500
155/155 - 21s - loss: 0.2358 - accuracy: 0.9059 - val_loss: 0.7397 - val_accuracy: 0.6023
Epoch 90/500
155/155 - 21s - loss: 0.2259 - accuracy: 0.9130 - val_loss: 0.7423 - val_accuracy: 0.6023
Epoch 91/500
155/155 - 21s - loss: 0.2170 - accuracy: 0.9174 - val_loss: 0.7445 - val_accuracy: 0.6023
Epoch 92/500
155/155 - 21s - loss: 0.2196 - accuracy: 0.9118 - val_loss: 0.7476 - val_accuracy: 0.6072
========================================
save_weights
h5_weights/BL1.po/onehot_embedding_cnn_two_branch.h5
========================================

end time >>> Sun Oct  3 00:54:51 2021

end time >>> Sun Oct  3 00:54:51 2021

end time >>> Sun Oct  3 00:54:51 2021

end time >>> Sun Oct  3 00:54:51 2021

end time >>> Sun Oct  3 00:54:51 2021












args.model = onehot_embedding_cnn_two_branch
time used = 1978.5163216590881


