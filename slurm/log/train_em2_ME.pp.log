************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 19:18:09 2021

begin time >>> Sun Oct  3 19:18:09 2021

begin time >>> Sun Oct  3 19:18:09 2021

begin time >>> Sun Oct  3 19:18:09 2021

begin time >>> Sun Oct  3 19:18:09 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_dense
args.type = train
args.name = ME.pp
args.length = 10001
===========================


-> h5_weights/ME.pp folder already exist. pass.
-> result/ME.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/ME.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/ME.pp/onehot_embedding_dense folder already exist. pass.
-> result/ME.pp/onehot_dense folder already exist. pass.
-> result/ME.pp/onehot_resnet18 folder already exist. pass.
-> result/ME.pp/onehot_resnet34 folder already exist. pass.
-> result/ME.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/ME.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/ME.pp/embedding_dense folder already exist. pass.
-> result/ME.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/ME.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
ME.pp
########################################

########################################
model_name
onehot_embedding_dense
########################################

Found 3436 images belonging to 2 classes.
Found 424 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 20002, 5, 1, 8)    32776     
_________________________________________________________________
flatten (Flatten)            (None, 800080)            0         
_________________________________________________________________
batch_normalization (BatchNo (None, 800080)            3200320   
_________________________________________________________________
dense (Dense)                (None, 512)               409641472 
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_3 (Batch (None, 512)               2048      
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_3 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 413,671,754
Trainable params: 412,067,498
Non-trainable params: 1,604,256
_________________________________________________________________
Epoch 1/500
107/107 - 87s - loss: 0.8097 - accuracy: 0.5182 - val_loss: 0.7031 - val_accuracy: 0.4976
Epoch 2/500
107/107 - 20s - loss: 0.6757 - accuracy: 0.6278 - val_loss: 0.7575 - val_accuracy: 0.4976
Epoch 3/500
107/107 - 20s - loss: 0.6010 - accuracy: 0.6830 - val_loss: 0.8729 - val_accuracy: 0.4976
Epoch 4/500
107/107 - 21s - loss: 0.5219 - accuracy: 0.7397 - val_loss: 1.0402 - val_accuracy: 0.5048
Epoch 5/500
107/107 - 20s - loss: 0.4364 - accuracy: 0.8032 - val_loss: 1.2042 - val_accuracy: 0.4976
Epoch 6/500
107/107 - 20s - loss: 0.3526 - accuracy: 0.8484 - val_loss: 1.3144 - val_accuracy: 0.5024
Epoch 7/500
107/107 - 21s - loss: 0.2849 - accuracy: 0.8881 - val_loss: 1.3999 - val_accuracy: 0.5288
Epoch 8/500
107/107 - 21s - loss: 0.2196 - accuracy: 0.9130 - val_loss: 1.5122 - val_accuracy: 0.5601
Epoch 9/500
107/107 - 21s - loss: 0.1810 - accuracy: 0.9330 - val_loss: 1.6380 - val_accuracy: 0.5625
Epoch 10/500
107/107 - 20s - loss: 0.1578 - accuracy: 0.9395 - val_loss: 1.7631 - val_accuracy: 0.5505
Epoch 11/500
107/107 - 21s - loss: 0.1371 - accuracy: 0.9509 - val_loss: 1.8303 - val_accuracy: 0.5721
Epoch 12/500
107/107 - 20s - loss: 0.1148 - accuracy: 0.9574 - val_loss: 1.9397 - val_accuracy: 0.5625
Epoch 13/500
107/107 - 20s - loss: 0.0906 - accuracy: 0.9668 - val_loss: 1.9677 - val_accuracy: 0.5697
Epoch 14/500
107/107 - 21s - loss: 0.0790 - accuracy: 0.9765 - val_loss: 2.0520 - val_accuracy: 0.5769
Epoch 15/500
107/107 - 20s - loss: 0.0810 - accuracy: 0.9689 - val_loss: 2.1177 - val_accuracy: 0.5721
Epoch 16/500
107/107 - 20s - loss: 0.0706 - accuracy: 0.9736 - val_loss: 2.1786 - val_accuracy: 0.5673
Epoch 17/500
107/107 - 21s - loss: 0.0619 - accuracy: 0.9783 - val_loss: 2.1847 - val_accuracy: 0.5865
Epoch 18/500
107/107 - 20s - loss: 0.0643 - accuracy: 0.9786 - val_loss: 2.2273 - val_accuracy: 0.5793
Epoch 19/500
107/107 - 21s - loss: 0.0634 - accuracy: 0.9765 - val_loss: 2.2444 - val_accuracy: 0.5938
Epoch 20/500
107/107 - 20s - loss: 0.0530 - accuracy: 0.9821 - val_loss: 2.2385 - val_accuracy: 0.5889
Epoch 21/500
107/107 - 20s - loss: 0.0523 - accuracy: 0.9844 - val_loss: 2.3596 - val_accuracy: 0.5793
Epoch 22/500
107/107 - 20s - loss: 0.0461 - accuracy: 0.9838 - val_loss: 2.3478 - val_accuracy: 0.5889
Epoch 23/500
107/107 - 20s - loss: 0.0373 - accuracy: 0.9871 - val_loss: 2.3809 - val_accuracy: 0.5841
Epoch 24/500
107/107 - 20s - loss: 0.0437 - accuracy: 0.9838 - val_loss: 2.4104 - val_accuracy: 0.5938
Epoch 25/500
107/107 - 20s - loss: 0.0376 - accuracy: 0.9874 - val_loss: 2.4923 - val_accuracy: 0.5817
Epoch 26/500
107/107 - 20s - loss: 0.0330 - accuracy: 0.9888 - val_loss: 2.5153 - val_accuracy: 0.5745
Epoch 27/500
107/107 - 20s - loss: 0.0414 - accuracy: 0.9856 - val_loss: 2.4614 - val_accuracy: 0.5841
Epoch 28/500
107/107 - 20s - loss: 0.0468 - accuracy: 0.9844 - val_loss: 2.4552 - val_accuracy: 0.5938
Epoch 29/500
107/107 - 21s - loss: 0.0382 - accuracy: 0.9877 - val_loss: 2.5289 - val_accuracy: 0.5769
========================================
save_weights
h5_weights/ME.pp/onehot_embedding_dense.h5
========================================

end time >>> Sun Oct  3 19:29:25 2021

end time >>> Sun Oct  3 19:29:25 2021

end time >>> Sun Oct  3 19:29:25 2021

end time >>> Sun Oct  3 19:29:25 2021

end time >>> Sun Oct  3 19:29:25 2021












args.model = onehot_embedding_dense
time used = 675.732691526413


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 19:29:26 2021

begin time >>> Sun Oct  3 19:29:26 2021

begin time >>> Sun Oct  3 19:29:26 2021

begin time >>> Sun Oct  3 19:29:26 2021

begin time >>> Sun Oct  3 19:29:26 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_one_branch
args.type = train
args.name = ME.pp
args.length = 10001
===========================


-> h5_weights/ME.pp folder already exist. pass.
-> result/ME.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/ME.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/ME.pp/onehot_embedding_dense folder already exist. pass.
-> result/ME.pp/onehot_dense folder already exist. pass.
-> result/ME.pp/onehot_resnet18 folder already exist. pass.
-> result/ME.pp/onehot_resnet34 folder already exist. pass.
-> result/ME.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/ME.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/ME.pp/embedding_dense folder already exist. pass.
-> result/ME.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/ME.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
ME.pp
########################################

########################################
model_name
onehot_embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
108/108 - 15s - loss: 0.8870 - accuracy: 0.5105 - val_loss: 0.6937 - val_accuracy: 0.4941
Epoch 2/500
108/108 - 15s - loss: 0.8642 - accuracy: 0.5215 - val_loss: 0.6937 - val_accuracy: 0.4941
Epoch 3/500
108/108 - 15s - loss: 0.8586 - accuracy: 0.5090 - val_loss: 0.6944 - val_accuracy: 0.4941
Epoch 4/500
108/108 - 15s - loss: 0.8465 - accuracy: 0.5102 - val_loss: 0.6942 - val_accuracy: 0.4847
Epoch 5/500
108/108 - 15s - loss: 0.8507 - accuracy: 0.5093 - val_loss: 0.6948 - val_accuracy: 0.5176
Epoch 6/500
108/108 - 15s - loss: 0.8373 - accuracy: 0.5169 - val_loss: 0.6942 - val_accuracy: 0.5059
Epoch 7/500
108/108 - 15s - loss: 0.8315 - accuracy: 0.5288 - val_loss: 0.6951 - val_accuracy: 0.5294
Epoch 8/500
108/108 - 15s - loss: 0.8126 - accuracy: 0.5335 - val_loss: 0.6957 - val_accuracy: 0.5365
Epoch 9/500
108/108 - 15s - loss: 0.7892 - accuracy: 0.5565 - val_loss: 0.6957 - val_accuracy: 0.5365
Epoch 10/500
108/108 - 15s - loss: 0.8138 - accuracy: 0.5332 - val_loss: 0.6952 - val_accuracy: 0.5271
Epoch 11/500
108/108 - 15s - loss: 0.7839 - accuracy: 0.5570 - val_loss: 0.6947 - val_accuracy: 0.5365
Epoch 12/500
108/108 - 15s - loss: 0.8190 - accuracy: 0.5311 - val_loss: 0.6932 - val_accuracy: 0.5412
Epoch 13/500
108/108 - 15s - loss: 0.8018 - accuracy: 0.5416 - val_loss: 0.6931 - val_accuracy: 0.5341
Epoch 14/500
108/108 - 15s - loss: 0.7778 - accuracy: 0.5559 - val_loss: 0.6927 - val_accuracy: 0.5459
Epoch 15/500
108/108 - 15s - loss: 0.7898 - accuracy: 0.5521 - val_loss: 0.6929 - val_accuracy: 0.5459
Epoch 16/500
108/108 - 15s - loss: 0.7731 - accuracy: 0.5690 - val_loss: 0.6922 - val_accuracy: 0.5435
Epoch 17/500
108/108 - 15s - loss: 0.7584 - accuracy: 0.5637 - val_loss: 0.6909 - val_accuracy: 0.5341
Epoch 18/500
108/108 - 15s - loss: 0.7555 - accuracy: 0.5739 - val_loss: 0.6898 - val_accuracy: 0.5412
Epoch 19/500
108/108 - 15s - loss: 0.7509 - accuracy: 0.5678 - val_loss: 0.6889 - val_accuracy: 0.5482
Epoch 20/500
108/108 - 15s - loss: 0.7545 - accuracy: 0.5681 - val_loss: 0.6881 - val_accuracy: 0.5576
Epoch 21/500
108/108 - 15s - loss: 0.7582 - accuracy: 0.5655 - val_loss: 0.6881 - val_accuracy: 0.5529
Epoch 22/500
108/108 - 15s - loss: 0.7452 - accuracy: 0.5748 - val_loss: 0.6875 - val_accuracy: 0.5600
Epoch 23/500
108/108 - 15s - loss: 0.7492 - accuracy: 0.5687 - val_loss: 0.6863 - val_accuracy: 0.5671
Epoch 24/500
108/108 - 15s - loss: 0.7506 - accuracy: 0.5669 - val_loss: 0.6862 - val_accuracy: 0.5600
Epoch 25/500
108/108 - 15s - loss: 0.7253 - accuracy: 0.5946 - val_loss: 0.6860 - val_accuracy: 0.5576
Epoch 26/500
108/108 - 15s - loss: 0.7314 - accuracy: 0.5844 - val_loss: 0.6851 - val_accuracy: 0.5600
Epoch 27/500
108/108 - 15s - loss: 0.7391 - accuracy: 0.5847 - val_loss: 0.6855 - val_accuracy: 0.5694
Epoch 28/500
108/108 - 15s - loss: 0.7110 - accuracy: 0.5995 - val_loss: 0.6845 - val_accuracy: 0.5624
Epoch 29/500
108/108 - 15s - loss: 0.7156 - accuracy: 0.6048 - val_loss: 0.6847 - val_accuracy: 0.5624
Epoch 30/500
108/108 - 15s - loss: 0.7102 - accuracy: 0.6071 - val_loss: 0.6843 - val_accuracy: 0.5624
Epoch 31/500
108/108 - 15s - loss: 0.7033 - accuracy: 0.6054 - val_loss: 0.6843 - val_accuracy: 0.5624
Epoch 32/500
108/108 - 15s - loss: 0.6914 - accuracy: 0.6222 - val_loss: 0.6831 - val_accuracy: 0.5600
Epoch 33/500
108/108 - 15s - loss: 0.6901 - accuracy: 0.6129 - val_loss: 0.6830 - val_accuracy: 0.5671
Epoch 34/500
108/108 - 15s - loss: 0.6822 - accuracy: 0.6310 - val_loss: 0.6826 - val_accuracy: 0.5718
Epoch 35/500
108/108 - 15s - loss: 0.6916 - accuracy: 0.6150 - val_loss: 0.6823 - val_accuracy: 0.5741
Epoch 36/500
108/108 - 15s - loss: 0.6628 - accuracy: 0.6403 - val_loss: 0.6825 - val_accuracy: 0.5765
Epoch 37/500
108/108 - 15s - loss: 0.6387 - accuracy: 0.6604 - val_loss: 0.6818 - val_accuracy: 0.5765
Epoch 38/500
108/108 - 15s - loss: 0.6555 - accuracy: 0.6347 - val_loss: 0.6814 - val_accuracy: 0.5671
Epoch 39/500
108/108 - 15s - loss: 0.6549 - accuracy: 0.6467 - val_loss: 0.6815 - val_accuracy: 0.5694
Epoch 40/500
108/108 - 15s - loss: 0.6404 - accuracy: 0.6441 - val_loss: 0.6811 - val_accuracy: 0.5718
Epoch 41/500
108/108 - 15s - loss: 0.6295 - accuracy: 0.6627 - val_loss: 0.6803 - val_accuracy: 0.5765
Epoch 42/500
108/108 - 15s - loss: 0.6409 - accuracy: 0.6534 - val_loss: 0.6806 - val_accuracy: 0.5835
Epoch 43/500
108/108 - 15s - loss: 0.6411 - accuracy: 0.6548 - val_loss: 0.6808 - val_accuracy: 0.5859
Epoch 44/500
108/108 - 15s - loss: 0.6289 - accuracy: 0.6630 - val_loss: 0.6801 - val_accuracy: 0.5765
Epoch 45/500
108/108 - 15s - loss: 0.6120 - accuracy: 0.6772 - val_loss: 0.6795 - val_accuracy: 0.5835
Epoch 46/500
108/108 - 15s - loss: 0.6196 - accuracy: 0.6740 - val_loss: 0.6795 - val_accuracy: 0.5788
Epoch 47/500
108/108 - 15s - loss: 0.6204 - accuracy: 0.6691 - val_loss: 0.6796 - val_accuracy: 0.5859
Epoch 48/500
108/108 - 15s - loss: 0.5905 - accuracy: 0.6778 - val_loss: 0.6795 - val_accuracy: 0.5882
Epoch 49/500
108/108 - 15s - loss: 0.5907 - accuracy: 0.6877 - val_loss: 0.6800 - val_accuracy: 0.5906
Epoch 50/500
108/108 - 15s - loss: 0.5853 - accuracy: 0.6909 - val_loss: 0.6801 - val_accuracy: 0.5882
Epoch 51/500
108/108 - 15s - loss: 0.5986 - accuracy: 0.6793 - val_loss: 0.6803 - val_accuracy: 0.5906
Epoch 52/500
108/108 - 15s - loss: 0.5790 - accuracy: 0.7072 - val_loss: 0.6808 - val_accuracy: 0.5882
Epoch 53/500
108/108 - 15s - loss: 0.5681 - accuracy: 0.7127 - val_loss: 0.6808 - val_accuracy: 0.5835
Epoch 54/500
108/108 - 15s - loss: 0.5720 - accuracy: 0.7046 - val_loss: 0.6808 - val_accuracy: 0.5835
Epoch 55/500
108/108 - 15s - loss: 0.5489 - accuracy: 0.7209 - val_loss: 0.6812 - val_accuracy: 0.5835
Epoch 56/500
108/108 - 15s - loss: 0.5479 - accuracy: 0.7311 - val_loss: 0.6814 - val_accuracy: 0.5859
Epoch 57/500
108/108 - 15s - loss: 0.5460 - accuracy: 0.7197 - val_loss: 0.6822 - val_accuracy: 0.5835
Epoch 58/500
108/108 - 15s - loss: 0.5336 - accuracy: 0.7267 - val_loss: 0.6826 - val_accuracy: 0.5835
Epoch 59/500
108/108 - 15s - loss: 0.5261 - accuracy: 0.7372 - val_loss: 0.6828 - val_accuracy: 0.5765
Epoch 60/500
108/108 - 15s - loss: 0.5155 - accuracy: 0.7395 - val_loss: 0.6826 - val_accuracy: 0.5859
Epoch 61/500
108/108 - 15s - loss: 0.5309 - accuracy: 0.7366 - val_loss: 0.6834 - val_accuracy: 0.5859
Epoch 62/500
108/108 - 15s - loss: 0.5224 - accuracy: 0.7395 - val_loss: 0.6844 - val_accuracy: 0.5788
Epoch 63/500
108/108 - 15s - loss: 0.5026 - accuracy: 0.7552 - val_loss: 0.6847 - val_accuracy: 0.5835
Epoch 64/500
108/108 - 15s - loss: 0.4944 - accuracy: 0.7614 - val_loss: 0.6850 - val_accuracy: 0.5835
Epoch 65/500
108/108 - 15s - loss: 0.5018 - accuracy: 0.7480 - val_loss: 0.6858 - val_accuracy: 0.5812
Epoch 66/500
108/108 - 15s - loss: 0.4808 - accuracy: 0.7692 - val_loss: 0.6872 - val_accuracy: 0.5765
Epoch 67/500
108/108 - 15s - loss: 0.4912 - accuracy: 0.7520 - val_loss: 0.6873 - val_accuracy: 0.5788
Epoch 68/500
108/108 - 15s - loss: 0.4614 - accuracy: 0.7820 - val_loss: 0.6873 - val_accuracy: 0.5812
Epoch 69/500
108/108 - 15s - loss: 0.4776 - accuracy: 0.7695 - val_loss: 0.6893 - val_accuracy: 0.5788
========================================
save_weights
h5_weights/ME.pp/onehot_embedding_cnn_one_branch.h5
========================================

end time >>> Sun Oct  3 19:46:57 2021

end time >>> Sun Oct  3 19:46:57 2021

end time >>> Sun Oct  3 19:46:57 2021

end time >>> Sun Oct  3 19:46:57 2021

end time >>> Sun Oct  3 19:46:57 2021












args.model = onehot_embedding_cnn_one_branch
time used = 1050.8939199447632


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 19:46:58 2021

begin time >>> Sun Oct  3 19:46:58 2021

begin time >>> Sun Oct  3 19:46:58 2021

begin time >>> Sun Oct  3 19:46:58 2021

begin time >>> Sun Oct  3 19:46:58 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_two_branch
args.type = train
args.name = ME.pp
args.length = 10001
===========================


-> h5_weights/ME.pp folder already exist. pass.
-> result/ME.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/ME.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/ME.pp/onehot_embedding_dense folder already exist. pass.
-> result/ME.pp/onehot_dense folder already exist. pass.
-> result/ME.pp/onehot_resnet18 folder already exist. pass.
-> result/ME.pp/onehot_resnet34 folder already exist. pass.
-> result/ME.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/ME.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/ME.pp/embedding_dense folder already exist. pass.
-> result/ME.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/ME.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
ME.pp
########################################

########################################
model_name
onehot_embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
108/108 - 15s - loss: 0.8754 - accuracy: 0.5006 - val_loss: 0.7027 - val_accuracy: 0.4941
Epoch 2/500
108/108 - 15s - loss: 0.8695 - accuracy: 0.5012 - val_loss: 0.7086 - val_accuracy: 0.4941
Epoch 3/500
108/108 - 15s - loss: 0.8594 - accuracy: 0.5055 - val_loss: 0.7094 - val_accuracy: 0.4941
Epoch 4/500
108/108 - 15s - loss: 0.8533 - accuracy: 0.5064 - val_loss: 0.7084 - val_accuracy: 0.4941
Epoch 5/500
108/108 - 15s - loss: 0.8553 - accuracy: 0.5143 - val_loss: 0.7108 - val_accuracy: 0.4894
Epoch 6/500
108/108 - 15s - loss: 0.8259 - accuracy: 0.5227 - val_loss: 0.7031 - val_accuracy: 0.5012
Epoch 7/500
108/108 - 15s - loss: 0.8291 - accuracy: 0.5268 - val_loss: 0.7029 - val_accuracy: 0.5224
Epoch 8/500
108/108 - 15s - loss: 0.8171 - accuracy: 0.5259 - val_loss: 0.7051 - val_accuracy: 0.5247
Epoch 9/500
108/108 - 15s - loss: 0.8515 - accuracy: 0.5140 - val_loss: 0.7043 - val_accuracy: 0.5412
Epoch 10/500
108/108 - 15s - loss: 0.7903 - accuracy: 0.5463 - val_loss: 0.7035 - val_accuracy: 0.5294
Epoch 11/500
108/108 - 15s - loss: 0.8036 - accuracy: 0.5428 - val_loss: 0.7028 - val_accuracy: 0.5318
Epoch 12/500
108/108 - 15s - loss: 0.7948 - accuracy: 0.5521 - val_loss: 0.7010 - val_accuracy: 0.5365
Epoch 13/500
108/108 - 15s - loss: 0.7924 - accuracy: 0.5538 - val_loss: 0.7006 - val_accuracy: 0.5459
Epoch 14/500
108/108 - 15s - loss: 0.7799 - accuracy: 0.5556 - val_loss: 0.6997 - val_accuracy: 0.5459
Epoch 15/500
108/108 - 15s - loss: 0.7787 - accuracy: 0.5643 - val_loss: 0.6983 - val_accuracy: 0.5506
Epoch 16/500
108/108 - 15s - loss: 0.7631 - accuracy: 0.5632 - val_loss: 0.6974 - val_accuracy: 0.5553
Epoch 17/500
108/108 - 15s - loss: 0.7683 - accuracy: 0.5611 - val_loss: 0.6962 - val_accuracy: 0.5576
Epoch 18/500
108/108 - 15s - loss: 0.7876 - accuracy: 0.5591 - val_loss: 0.6953 - val_accuracy: 0.5647
Epoch 19/500
108/108 - 14s - loss: 0.7500 - accuracy: 0.5795 - val_loss: 0.6946 - val_accuracy: 0.5624
Epoch 20/500
108/108 - 14s - loss: 0.7580 - accuracy: 0.5719 - val_loss: 0.6940 - val_accuracy: 0.5600
Epoch 21/500
108/108 - 14s - loss: 0.7398 - accuracy: 0.5780 - val_loss: 0.6922 - val_accuracy: 0.5576
Epoch 22/500
108/108 - 14s - loss: 0.7176 - accuracy: 0.5905 - val_loss: 0.6921 - val_accuracy: 0.5529
Epoch 23/500
108/108 - 14s - loss: 0.7311 - accuracy: 0.5885 - val_loss: 0.6914 - val_accuracy: 0.5553
Epoch 24/500
108/108 - 14s - loss: 0.7379 - accuracy: 0.5841 - val_loss: 0.6904 - val_accuracy: 0.5600
Epoch 25/500
108/108 - 14s - loss: 0.7088 - accuracy: 0.6120 - val_loss: 0.6895 - val_accuracy: 0.5600
Epoch 26/500
108/108 - 14s - loss: 0.6926 - accuracy: 0.6193 - val_loss: 0.6881 - val_accuracy: 0.5600
Epoch 27/500
108/108 - 14s - loss: 0.6842 - accuracy: 0.6190 - val_loss: 0.6866 - val_accuracy: 0.5647
Epoch 28/500
108/108 - 15s - loss: 0.7039 - accuracy: 0.6103 - val_loss: 0.6860 - val_accuracy: 0.5576
Epoch 29/500
108/108 - 14s - loss: 0.6958 - accuracy: 0.6251 - val_loss: 0.6853 - val_accuracy: 0.5576
Epoch 30/500
108/108 - 14s - loss: 0.6857 - accuracy: 0.6179 - val_loss: 0.6838 - val_accuracy: 0.5671
Epoch 31/500
108/108 - 14s - loss: 0.7039 - accuracy: 0.6051 - val_loss: 0.6826 - val_accuracy: 0.5671
Epoch 32/500
108/108 - 14s - loss: 0.6808 - accuracy: 0.6129 - val_loss: 0.6823 - val_accuracy: 0.5671
Epoch 33/500
108/108 - 14s - loss: 0.6855 - accuracy: 0.6199 - val_loss: 0.6813 - val_accuracy: 0.5694
Epoch 34/500
108/108 - 14s - loss: 0.6599 - accuracy: 0.6449 - val_loss: 0.6807 - val_accuracy: 0.5741
Epoch 35/500
108/108 - 14s - loss: 0.6504 - accuracy: 0.6347 - val_loss: 0.6791 - val_accuracy: 0.5718
Epoch 36/500
108/108 - 14s - loss: 0.6564 - accuracy: 0.6388 - val_loss: 0.6779 - val_accuracy: 0.5741
Epoch 37/500
108/108 - 14s - loss: 0.6557 - accuracy: 0.6382 - val_loss: 0.6779 - val_accuracy: 0.5718
Epoch 38/500
108/108 - 14s - loss: 0.6340 - accuracy: 0.6676 - val_loss: 0.6777 - val_accuracy: 0.5788
Epoch 39/500
108/108 - 14s - loss: 0.6580 - accuracy: 0.6432 - val_loss: 0.6773 - val_accuracy: 0.5812
Epoch 40/500
108/108 - 15s - loss: 0.6158 - accuracy: 0.6813 - val_loss: 0.6765 - val_accuracy: 0.5859
Epoch 41/500
108/108 - 14s - loss: 0.6088 - accuracy: 0.6720 - val_loss: 0.6749 - val_accuracy: 0.5882
Epoch 42/500
108/108 - 14s - loss: 0.5918 - accuracy: 0.6781 - val_loss: 0.6754 - val_accuracy: 0.5859
Epoch 43/500
108/108 - 14s - loss: 0.6182 - accuracy: 0.6787 - val_loss: 0.6753 - val_accuracy: 0.5906
Epoch 44/500
108/108 - 14s - loss: 0.6069 - accuracy: 0.6790 - val_loss: 0.6750 - val_accuracy: 0.5976
Epoch 45/500
108/108 - 14s - loss: 0.5913 - accuracy: 0.6918 - val_loss: 0.6738 - val_accuracy: 0.5953
Epoch 46/500
108/108 - 14s - loss: 0.5913 - accuracy: 0.6874 - val_loss: 0.6735 - val_accuracy: 0.5976
Epoch 47/500
108/108 - 14s - loss: 0.5852 - accuracy: 0.6947 - val_loss: 0.6725 - val_accuracy: 0.6024
Epoch 48/500
108/108 - 14s - loss: 0.5862 - accuracy: 0.6930 - val_loss: 0.6727 - val_accuracy: 0.6024
Epoch 49/500
108/108 - 14s - loss: 0.5756 - accuracy: 0.6997 - val_loss: 0.6733 - val_accuracy: 0.6047
Epoch 50/500
108/108 - 14s - loss: 0.5882 - accuracy: 0.6953 - val_loss: 0.6720 - val_accuracy: 0.6071
Epoch 51/500
108/108 - 14s - loss: 0.5723 - accuracy: 0.7122 - val_loss: 0.6713 - val_accuracy: 0.6118
Epoch 52/500
108/108 - 15s - loss: 0.5586 - accuracy: 0.7119 - val_loss: 0.6710 - val_accuracy: 0.6141
Epoch 53/500
108/108 - 14s - loss: 0.5658 - accuracy: 0.7200 - val_loss: 0.6709 - val_accuracy: 0.6165
Epoch 54/500
108/108 - 14s - loss: 0.5427 - accuracy: 0.7212 - val_loss: 0.6709 - val_accuracy: 0.6094
Epoch 55/500
108/108 - 14s - loss: 0.5292 - accuracy: 0.7354 - val_loss: 0.6709 - val_accuracy: 0.6165
Epoch 56/500
108/108 - 14s - loss: 0.5333 - accuracy: 0.7305 - val_loss: 0.6704 - val_accuracy: 0.6094
Epoch 57/500
108/108 - 14s - loss: 0.5070 - accuracy: 0.7465 - val_loss: 0.6700 - val_accuracy: 0.6141
Epoch 58/500
108/108 - 14s - loss: 0.5136 - accuracy: 0.7488 - val_loss: 0.6704 - val_accuracy: 0.6141
Epoch 59/500
108/108 - 14s - loss: 0.5016 - accuracy: 0.7544 - val_loss: 0.6702 - val_accuracy: 0.6212
Epoch 60/500
108/108 - 14s - loss: 0.4996 - accuracy: 0.7436 - val_loss: 0.6704 - val_accuracy: 0.6165
Epoch 61/500
108/108 - 14s - loss: 0.4827 - accuracy: 0.7666 - val_loss: 0.6706 - val_accuracy: 0.6188
Epoch 62/500
108/108 - 15s - loss: 0.4860 - accuracy: 0.7599 - val_loss: 0.6713 - val_accuracy: 0.6235
Epoch 63/500
108/108 - 14s - loss: 0.4777 - accuracy: 0.7829 - val_loss: 0.6708 - val_accuracy: 0.6259
Epoch 64/500
108/108 - 14s - loss: 0.4487 - accuracy: 0.7957 - val_loss: 0.6711 - val_accuracy: 0.6235
Epoch 65/500
108/108 - 14s - loss: 0.4592 - accuracy: 0.7841 - val_loss: 0.6708 - val_accuracy: 0.6306
Epoch 66/500
108/108 - 14s - loss: 0.4545 - accuracy: 0.7890 - val_loss: 0.6709 - val_accuracy: 0.6282
Epoch 67/500
108/108 - 14s - loss: 0.4411 - accuracy: 0.7966 - val_loss: 0.6723 - val_accuracy: 0.6306
Epoch 68/500
108/108 - 14s - loss: 0.4366 - accuracy: 0.7954 - val_loss: 0.6708 - val_accuracy: 0.6329
Epoch 69/500
108/108 - 14s - loss: 0.4387 - accuracy: 0.7890 - val_loss: 0.6719 - val_accuracy: 0.6329
Epoch 70/500
108/108 - 14s - loss: 0.4194 - accuracy: 0.8027 - val_loss: 0.6729 - val_accuracy: 0.6353
Epoch 71/500
108/108 - 14s - loss: 0.4191 - accuracy: 0.8047 - val_loss: 0.6734 - val_accuracy: 0.6376
Epoch 72/500
108/108 - 14s - loss: 0.4301 - accuracy: 0.7931 - val_loss: 0.6758 - val_accuracy: 0.6306
Epoch 73/500
108/108 - 14s - loss: 0.4145 - accuracy: 0.8129 - val_loss: 0.6756 - val_accuracy: 0.6400
Epoch 74/500
108/108 - 15s - loss: 0.3960 - accuracy: 0.8201 - val_loss: 0.6767 - val_accuracy: 0.6376
Epoch 75/500
108/108 - 14s - loss: 0.3902 - accuracy: 0.8306 - val_loss: 0.6765 - val_accuracy: 0.6400
Epoch 76/500
108/108 - 14s - loss: 0.3880 - accuracy: 0.8231 - val_loss: 0.6770 - val_accuracy: 0.6376
Epoch 77/500
108/108 - 14s - loss: 0.3664 - accuracy: 0.8393 - val_loss: 0.6783 - val_accuracy: 0.6376
Epoch 78/500
108/108 - 14s - loss: 0.3683 - accuracy: 0.8359 - val_loss: 0.6785 - val_accuracy: 0.6400
Epoch 79/500
108/108 - 14s - loss: 0.3619 - accuracy: 0.8399 - val_loss: 0.6809 - val_accuracy: 0.6424
Epoch 80/500
108/108 - 14s - loss: 0.3667 - accuracy: 0.8344 - val_loss: 0.6811 - val_accuracy: 0.6447
Epoch 81/500
108/108 - 14s - loss: 0.3614 - accuracy: 0.8463 - val_loss: 0.6825 - val_accuracy: 0.6447
Epoch 82/500
108/108 - 14s - loss: 0.3335 - accuracy: 0.8539 - val_loss: 0.6840 - val_accuracy: 0.6329
Epoch 83/500
108/108 - 14s - loss: 0.3310 - accuracy: 0.8685 - val_loss: 0.6858 - val_accuracy: 0.6400
Epoch 84/500
108/108 - 14s - loss: 0.3214 - accuracy: 0.8594 - val_loss: 0.6876 - val_accuracy: 0.6424
Epoch 85/500
108/108 - 15s - loss: 0.3174 - accuracy: 0.8667 - val_loss: 0.6891 - val_accuracy: 0.6400
Epoch 86/500
108/108 - 14s - loss: 0.3266 - accuracy: 0.8600 - val_loss: 0.6910 - val_accuracy: 0.6424
Epoch 87/500
108/108 - 14s - loss: 0.3090 - accuracy: 0.8740 - val_loss: 0.6931 - val_accuracy: 0.6424
Epoch 88/500
108/108 - 14s - loss: 0.2953 - accuracy: 0.8781 - val_loss: 0.6958 - val_accuracy: 0.6447
Epoch 89/500
108/108 - 14s - loss: 0.2997 - accuracy: 0.8685 - val_loss: 0.6965 - val_accuracy: 0.6376
Epoch 90/500
108/108 - 14s - loss: 0.2983 - accuracy: 0.8769 - val_loss: 0.6986 - val_accuracy: 0.6400
Epoch 91/500
108/108 - 14s - loss: 0.2911 - accuracy: 0.8798 - val_loss: 0.7003 - val_accuracy: 0.6400
Epoch 92/500
108/108 - 14s - loss: 0.2791 - accuracy: 0.8891 - val_loss: 0.7019 - val_accuracy: 0.6424
Epoch 93/500
108/108 - 14s - loss: 0.2673 - accuracy: 0.8944 - val_loss: 0.7037 - val_accuracy: 0.6471
Epoch 94/500
108/108 - 14s - loss: 0.2675 - accuracy: 0.8923 - val_loss: 0.7070 - val_accuracy: 0.6400
Epoch 95/500
108/108 - 14s - loss: 0.2600 - accuracy: 0.8949 - val_loss: 0.7082 - val_accuracy: 0.6400
Epoch 96/500
108/108 - 14s - loss: 0.2556 - accuracy: 0.8961 - val_loss: 0.7103 - val_accuracy: 0.6471
Epoch 97/500
108/108 - 15s - loss: 0.2613 - accuracy: 0.8912 - val_loss: 0.7117 - val_accuracy: 0.6424
Epoch 98/500
108/108 - 14s - loss: 0.2478 - accuracy: 0.9080 - val_loss: 0.7151 - val_accuracy: 0.6400
Epoch 99/500
108/108 - 14s - loss: 0.2351 - accuracy: 0.9025 - val_loss: 0.7187 - val_accuracy: 0.6400
Epoch 100/500
108/108 - 14s - loss: 0.2480 - accuracy: 0.8976 - val_loss: 0.7212 - val_accuracy: 0.6376
Epoch 101/500
108/108 - 14s - loss: 0.2399 - accuracy: 0.9037 - val_loss: 0.7238 - val_accuracy: 0.6306
Epoch 102/500
108/108 - 14s - loss: 0.2300 - accuracy: 0.9104 - val_loss: 0.7262 - val_accuracy: 0.6329
Epoch 103/500
108/108 - 14s - loss: 0.2419 - accuracy: 0.9072 - val_loss: 0.7277 - val_accuracy: 0.6353
Epoch 104/500
108/108 - 14s - loss: 0.2111 - accuracy: 0.9214 - val_loss: 0.7312 - val_accuracy: 0.6329
Epoch 105/500
108/108 - 14s - loss: 0.2035 - accuracy: 0.9267 - val_loss: 0.7342 - val_accuracy: 0.6376
Epoch 106/500
108/108 - 14s - loss: 0.2052 - accuracy: 0.9226 - val_loss: 0.7377 - val_accuracy: 0.6329
Epoch 107/500
108/108 - 14s - loss: 0.1994 - accuracy: 0.9264 - val_loss: 0.7399 - val_accuracy: 0.6329
Epoch 108/500
108/108 - 15s - loss: 0.1991 - accuracy: 0.9299 - val_loss: 0.7448 - val_accuracy: 0.6329
Epoch 109/500
108/108 - 14s - loss: 0.1912 - accuracy: 0.9261 - val_loss: 0.7462 - val_accuracy: 0.6282
Epoch 110/500
108/108 - 14s - loss: 0.1968 - accuracy: 0.9255 - val_loss: 0.7497 - val_accuracy: 0.6259
Epoch 111/500
108/108 - 14s - loss: 0.1907 - accuracy: 0.9243 - val_loss: 0.7516 - val_accuracy: 0.6329
Epoch 112/500
108/108 - 14s - loss: 0.1820 - accuracy: 0.9339 - val_loss: 0.7554 - val_accuracy: 0.6306
Epoch 113/500
108/108 - 14s - loss: 0.1768 - accuracy: 0.9363 - val_loss: 0.7581 - val_accuracy: 0.6306
========================================
save_weights
h5_weights/ME.pp/onehot_embedding_cnn_two_branch.h5
========================================

end time >>> Sun Oct  3 20:14:47 2021

end time >>> Sun Oct  3 20:14:47 2021

end time >>> Sun Oct  3 20:14:47 2021

end time >>> Sun Oct  3 20:14:47 2021

end time >>> Sun Oct  3 20:14:47 2021












args.model = onehot_embedding_cnn_two_branch
time used = 1668.573515176773


