************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 16:58:38 2021

begin time >>> Sun Oct  3 16:58:38 2021

begin time >>> Sun Oct  3 16:58:38 2021

begin time >>> Sun Oct  3 16:58:38 2021

begin time >>> Sun Oct  3 16:58:38 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_dense
args.type = train
args.name = IMR90.pp
args.length = 10001
===========================


-> h5_weights/IMR90.pp folder already exist. pass.
-> result/IMR90.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/IMR90.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/IMR90.pp/onehot_embedding_dense folder already exist. pass.
-> result/IMR90.pp/onehot_dense folder already exist. pass.
-> result/IMR90.pp/onehot_resnet18 folder already exist. pass.
-> result/IMR90.pp/onehot_resnet34 folder already exist. pass.
-> result/IMR90.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/IMR90.pp/embedding_dense folder already exist. pass.
-> result/IMR90.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
IMR90.pp
########################################

########################################
model_name
onehot_embedding_dense
########################################

Found 3890 images belonging to 2 classes.
Found 480 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 20002, 5, 1, 8)    32776     
_________________________________________________________________
flatten (Flatten)            (None, 800080)            0         
_________________________________________________________________
batch_normalization (BatchNo (None, 800080)            3200320   
_________________________________________________________________
dense (Dense)                (None, 512)               409641472 
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_3 (Batch (None, 512)               2048      
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_3 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 413,671,754
Trainable params: 412,067,498
Non-trainable params: 1,604,256
_________________________________________________________________
Epoch 1/500
121/121 - 29s - loss: 0.7970 - accuracy: 0.5404 - val_loss: 0.6889 - val_accuracy: 0.5250
Epoch 2/500
121/121 - 23s - loss: 0.6737 - accuracy: 0.6293 - val_loss: 0.7174 - val_accuracy: 0.5000
Epoch 3/500
121/121 - 22s - loss: 0.6140 - accuracy: 0.6778 - val_loss: 0.7578 - val_accuracy: 0.5000
Epoch 4/500
121/121 - 22s - loss: 0.5123 - accuracy: 0.7465 - val_loss: 0.8104 - val_accuracy: 0.5021
Epoch 5/500
121/121 - 24s - loss: 0.4278 - accuracy: 0.8059 - val_loss: 0.8717 - val_accuracy: 0.5354
Epoch 6/500
121/121 - 24s - loss: 0.3243 - accuracy: 0.8629 - val_loss: 0.9557 - val_accuracy: 0.5750
Epoch 7/500
121/121 - 24s - loss: 0.2625 - accuracy: 0.8945 - val_loss: 1.1020 - val_accuracy: 0.5854
Epoch 8/500
121/121 - 23s - loss: 0.2133 - accuracy: 0.9202 - val_loss: 1.1743 - val_accuracy: 0.6000
Epoch 9/500
121/121 - 22s - loss: 0.1722 - accuracy: 0.9365 - val_loss: 1.2658 - val_accuracy: 0.5938
Epoch 10/500
121/121 - 22s - loss: 0.1387 - accuracy: 0.9502 - val_loss: 1.3989 - val_accuracy: 0.5875
Epoch 11/500
121/121 - 22s - loss: 0.1097 - accuracy: 0.9624 - val_loss: 1.4635 - val_accuracy: 0.5958
Epoch 12/500
121/121 - 22s - loss: 0.0966 - accuracy: 0.9635 - val_loss: 1.5716 - val_accuracy: 0.5875
Epoch 13/500
121/121 - 23s - loss: 0.0840 - accuracy: 0.9707 - val_loss: 1.6629 - val_accuracy: 0.5854
Epoch 14/500
121/121 - 23s - loss: 0.0778 - accuracy: 0.9730 - val_loss: 1.7033 - val_accuracy: 0.5958
Epoch 15/500
121/121 - 23s - loss: 0.0730 - accuracy: 0.9723 - val_loss: 1.7207 - val_accuracy: 0.5938
Epoch 16/500
121/121 - 24s - loss: 0.0599 - accuracy: 0.9798 - val_loss: 1.7912 - val_accuracy: 0.5938
Epoch 17/500
121/121 - 24s - loss: 0.0513 - accuracy: 0.9852 - val_loss: 1.8456 - val_accuracy: 0.6021
Epoch 18/500
121/121 - 25s - loss: 0.0480 - accuracy: 0.9844 - val_loss: 1.9116 - val_accuracy: 0.6062
Epoch 19/500
121/121 - 24s - loss: 0.0467 - accuracy: 0.9850 - val_loss: 1.9344 - val_accuracy: 0.6104
Epoch 20/500
121/121 - 24s - loss: 0.0481 - accuracy: 0.9842 - val_loss: 1.9113 - val_accuracy: 0.6104
Epoch 21/500
121/121 - 23s - loss: 0.0515 - accuracy: 0.9808 - val_loss: 1.9530 - val_accuracy: 0.6062
Epoch 22/500
121/121 - 23s - loss: 0.0406 - accuracy: 0.9870 - val_loss: 1.9620 - val_accuracy: 0.6042
Epoch 23/500
121/121 - 22s - loss: 0.0351 - accuracy: 0.9881 - val_loss: 1.9971 - val_accuracy: 0.6083
Epoch 24/500
121/121 - 23s - loss: 0.0393 - accuracy: 0.9865 - val_loss: 2.0114 - val_accuracy: 0.6229
Epoch 25/500
121/121 - 22s - loss: 0.0330 - accuracy: 0.9904 - val_loss: 1.9861 - val_accuracy: 0.6187
Epoch 26/500
121/121 - 23s - loss: 0.0299 - accuracy: 0.9894 - val_loss: 2.0095 - val_accuracy: 0.6187
Epoch 27/500
121/121 - 22s - loss: 0.0333 - accuracy: 0.9870 - val_loss: 2.0346 - val_accuracy: 0.6104
Epoch 28/500
121/121 - 22s - loss: 0.0408 - accuracy: 0.9860 - val_loss: 2.1018 - val_accuracy: 0.6000
Epoch 29/500
121/121 - 23s - loss: 0.0298 - accuracy: 0.9894 - val_loss: 2.1659 - val_accuracy: 0.6187
Epoch 30/500
121/121 - 22s - loss: 0.0305 - accuracy: 0.9907 - val_loss: 2.1710 - val_accuracy: 0.6208
Epoch 31/500
121/121 - 22s - loss: 0.0383 - accuracy: 0.9878 - val_loss: 2.1737 - val_accuracy: 0.6125
Epoch 32/500
121/121 - 22s - loss: 0.0290 - accuracy: 0.9907 - val_loss: 2.1797 - val_accuracy: 0.6187
Epoch 33/500
121/121 - 23s - loss: 0.0238 - accuracy: 0.9917 - val_loss: 2.1802 - val_accuracy: 0.6000
Epoch 34/500
121/121 - 23s - loss: 0.0262 - accuracy: 0.9917 - val_loss: 2.2079 - val_accuracy: 0.5979
========================================
save_weights
h5_weights/IMR90.pp/onehot_embedding_dense.h5
========================================

end time >>> Sun Oct  3 17:12:05 2021

end time >>> Sun Oct  3 17:12:05 2021

end time >>> Sun Oct  3 17:12:05 2021

end time >>> Sun Oct  3 17:12:05 2021

end time >>> Sun Oct  3 17:12:05 2021












args.model = onehot_embedding_dense
time used = 806.4799129962921


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 17:12:05 2021

begin time >>> Sun Oct  3 17:12:05 2021

begin time >>> Sun Oct  3 17:12:05 2021

begin time >>> Sun Oct  3 17:12:05 2021

begin time >>> Sun Oct  3 17:12:05 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_one_branch
args.type = train
args.name = IMR90.pp
args.length = 10001
===========================


-> h5_weights/IMR90.pp folder already exist. pass.
-> result/IMR90.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/IMR90.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/IMR90.pp/onehot_embedding_dense folder already exist. pass.
-> result/IMR90.pp/onehot_dense folder already exist. pass.
-> result/IMR90.pp/onehot_resnet18 folder already exist. pass.
-> result/IMR90.pp/onehot_resnet34 folder already exist. pass.
-> result/IMR90.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/IMR90.pp/embedding_dense folder already exist. pass.
-> result/IMR90.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
IMR90.pp
########################################

########################################
model_name
onehot_embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
122/122 - 17s - loss: 0.9127 - accuracy: 0.4968 - val_loss: 0.7110 - val_accuracy: 0.4511
Epoch 2/500
122/122 - 17s - loss: 0.8968 - accuracy: 0.5001 - val_loss: 0.7157 - val_accuracy: 0.4511
Epoch 3/500
122/122 - 17s - loss: 0.8908 - accuracy: 0.4988 - val_loss: 0.7124 - val_accuracy: 0.4511
Epoch 4/500
122/122 - 17s - loss: 0.8578 - accuracy: 0.5174 - val_loss: 0.7089 - val_accuracy: 0.4428
Epoch 5/500
122/122 - 17s - loss: 0.8661 - accuracy: 0.5120 - val_loss: 0.7103 - val_accuracy: 0.4782
Epoch 6/500
122/122 - 17s - loss: 0.8561 - accuracy: 0.5099 - val_loss: 0.7086 - val_accuracy: 0.4948
Epoch 7/500
122/122 - 17s - loss: 0.8461 - accuracy: 0.5168 - val_loss: 0.7112 - val_accuracy: 0.4886
Epoch 8/500
122/122 - 17s - loss: 0.8245 - accuracy: 0.5328 - val_loss: 0.7114 - val_accuracy: 0.4886
Epoch 9/500
122/122 - 17s - loss: 0.8164 - accuracy: 0.5343 - val_loss: 0.7103 - val_accuracy: 0.4906
Epoch 10/500
122/122 - 17s - loss: 0.8318 - accuracy: 0.5300 - val_loss: 0.7087 - val_accuracy: 0.4969
Epoch 11/500
122/122 - 17s - loss: 0.7947 - accuracy: 0.5585 - val_loss: 0.7076 - val_accuracy: 0.5031
Epoch 12/500
122/122 - 17s - loss: 0.7803 - accuracy: 0.5564 - val_loss: 0.7066 - val_accuracy: 0.5031
Epoch 13/500
122/122 - 17s - loss: 0.7940 - accuracy: 0.5492 - val_loss: 0.7042 - val_accuracy: 0.5031
Epoch 14/500
122/122 - 17s - loss: 0.7983 - accuracy: 0.5459 - val_loss: 0.7024 - val_accuracy: 0.5135
Epoch 15/500
122/122 - 17s - loss: 0.7870 - accuracy: 0.5474 - val_loss: 0.7013 - val_accuracy: 0.5114
Epoch 16/500
122/122 - 17s - loss: 0.7769 - accuracy: 0.5595 - val_loss: 0.7010 - val_accuracy: 0.5135
Epoch 17/500
122/122 - 17s - loss: 0.7895 - accuracy: 0.5528 - val_loss: 0.7004 - val_accuracy: 0.5218
Epoch 18/500
122/122 - 17s - loss: 0.7865 - accuracy: 0.5487 - val_loss: 0.6988 - val_accuracy: 0.5239
Epoch 19/500
122/122 - 17s - loss: 0.7755 - accuracy: 0.5588 - val_loss: 0.6978 - val_accuracy: 0.5343
Epoch 20/500
122/122 - 17s - loss: 0.7571 - accuracy: 0.5621 - val_loss: 0.6969 - val_accuracy: 0.5301
Epoch 21/500
122/122 - 17s - loss: 0.7634 - accuracy: 0.5649 - val_loss: 0.6955 - val_accuracy: 0.5364
Epoch 22/500
122/122 - 17s - loss: 0.7567 - accuracy: 0.5706 - val_loss: 0.6952 - val_accuracy: 0.5281
Epoch 23/500
122/122 - 17s - loss: 0.7352 - accuracy: 0.5894 - val_loss: 0.6937 - val_accuracy: 0.5385
Epoch 24/500
122/122 - 17s - loss: 0.7321 - accuracy: 0.5806 - val_loss: 0.6934 - val_accuracy: 0.5426
Epoch 25/500
122/122 - 17s - loss: 0.7231 - accuracy: 0.5919 - val_loss: 0.6926 - val_accuracy: 0.5405
Epoch 26/500
122/122 - 17s - loss: 0.7203 - accuracy: 0.6007 - val_loss: 0.6923 - val_accuracy: 0.5447
Epoch 27/500
122/122 - 17s - loss: 0.7256 - accuracy: 0.5966 - val_loss: 0.6905 - val_accuracy: 0.5489
Epoch 28/500
122/122 - 17s - loss: 0.7071 - accuracy: 0.6053 - val_loss: 0.6903 - val_accuracy: 0.5593
Epoch 29/500
122/122 - 17s - loss: 0.7063 - accuracy: 0.6063 - val_loss: 0.6893 - val_accuracy: 0.5489
Epoch 30/500
122/122 - 17s - loss: 0.6931 - accuracy: 0.6074 - val_loss: 0.6887 - val_accuracy: 0.5509
Epoch 31/500
122/122 - 17s - loss: 0.6915 - accuracy: 0.6202 - val_loss: 0.6882 - val_accuracy: 0.5530
Epoch 32/500
122/122 - 17s - loss: 0.6979 - accuracy: 0.6117 - val_loss: 0.6879 - val_accuracy: 0.5551
Epoch 33/500
122/122 - 17s - loss: 0.6919 - accuracy: 0.6081 - val_loss: 0.6869 - val_accuracy: 0.5530
Epoch 34/500
122/122 - 17s - loss: 0.6698 - accuracy: 0.6336 - val_loss: 0.6868 - val_accuracy: 0.5572
Epoch 35/500
122/122 - 17s - loss: 0.6838 - accuracy: 0.6223 - val_loss: 0.6864 - val_accuracy: 0.5655
Epoch 36/500
122/122 - 17s - loss: 0.6844 - accuracy: 0.6246 - val_loss: 0.6853 - val_accuracy: 0.5696
Epoch 37/500
122/122 - 17s - loss: 0.6818 - accuracy: 0.6248 - val_loss: 0.6851 - val_accuracy: 0.5696
Epoch 38/500
122/122 - 17s - loss: 0.6693 - accuracy: 0.6272 - val_loss: 0.6854 - val_accuracy: 0.5676
Epoch 39/500
122/122 - 17s - loss: 0.6582 - accuracy: 0.6444 - val_loss: 0.6833 - val_accuracy: 0.5780
Epoch 40/500
122/122 - 17s - loss: 0.6536 - accuracy: 0.6462 - val_loss: 0.6839 - val_accuracy: 0.5800
Epoch 41/500
122/122 - 17s - loss: 0.6445 - accuracy: 0.6616 - val_loss: 0.6832 - val_accuracy: 0.5821
Epoch 42/500
122/122 - 17s - loss: 0.6419 - accuracy: 0.6490 - val_loss: 0.6833 - val_accuracy: 0.5759
Epoch 43/500
122/122 - 17s - loss: 0.6308 - accuracy: 0.6624 - val_loss: 0.6820 - val_accuracy: 0.5821
Epoch 44/500
122/122 - 17s - loss: 0.6281 - accuracy: 0.6704 - val_loss: 0.6813 - val_accuracy: 0.5821
Epoch 45/500
122/122 - 17s - loss: 0.6127 - accuracy: 0.6781 - val_loss: 0.6822 - val_accuracy: 0.5821
Epoch 46/500
122/122 - 17s - loss: 0.6138 - accuracy: 0.6734 - val_loss: 0.6825 - val_accuracy: 0.5800
Epoch 47/500
122/122 - 17s - loss: 0.6224 - accuracy: 0.6724 - val_loss: 0.6822 - val_accuracy: 0.5925
Epoch 48/500
122/122 - 17s - loss: 0.5965 - accuracy: 0.6855 - val_loss: 0.6812 - val_accuracy: 0.5863
Epoch 49/500
122/122 - 17s - loss: 0.5934 - accuracy: 0.6935 - val_loss: 0.6806 - val_accuracy: 0.5884
Epoch 50/500
122/122 - 17s - loss: 0.5947 - accuracy: 0.6925 - val_loss: 0.6812 - val_accuracy: 0.5842
Epoch 51/500
122/122 - 17s - loss: 0.5948 - accuracy: 0.6886 - val_loss: 0.6809 - val_accuracy: 0.5904
Epoch 52/500
122/122 - 17s - loss: 0.5686 - accuracy: 0.7043 - val_loss: 0.6823 - val_accuracy: 0.5884
Epoch 53/500
122/122 - 17s - loss: 0.5768 - accuracy: 0.6994 - val_loss: 0.6814 - val_accuracy: 0.5946
Epoch 54/500
122/122 - 17s - loss: 0.5531 - accuracy: 0.7262 - val_loss: 0.6820 - val_accuracy: 0.5925
Epoch 55/500
122/122 - 17s - loss: 0.5474 - accuracy: 0.7205 - val_loss: 0.6822 - val_accuracy: 0.6008
Epoch 56/500
122/122 - 17s - loss: 0.5500 - accuracy: 0.7197 - val_loss: 0.6830 - val_accuracy: 0.6029
Epoch 57/500
122/122 - 17s - loss: 0.5330 - accuracy: 0.7303 - val_loss: 0.6823 - val_accuracy: 0.6050
Epoch 58/500
122/122 - 17s - loss: 0.5305 - accuracy: 0.7346 - val_loss: 0.6822 - val_accuracy: 0.6133
Epoch 59/500
122/122 - 17s - loss: 0.5375 - accuracy: 0.7251 - val_loss: 0.6830 - val_accuracy: 0.6154
Epoch 60/500
122/122 - 17s - loss: 0.5233 - accuracy: 0.7421 - val_loss: 0.6830 - val_accuracy: 0.6133
Epoch 61/500
122/122 - 17s - loss: 0.5161 - accuracy: 0.7424 - val_loss: 0.6834 - val_accuracy: 0.6133
Epoch 62/500
122/122 - 17s - loss: 0.5249 - accuracy: 0.7385 - val_loss: 0.6854 - val_accuracy: 0.6071
Epoch 63/500
122/122 - 17s - loss: 0.4703 - accuracy: 0.7745 - val_loss: 0.6852 - val_accuracy: 0.6154
Epoch 64/500
122/122 - 17s - loss: 0.5071 - accuracy: 0.7442 - val_loss: 0.6855 - val_accuracy: 0.6154
Epoch 65/500
122/122 - 17s - loss: 0.4809 - accuracy: 0.7657 - val_loss: 0.6871 - val_accuracy: 0.6091
Epoch 66/500
122/122 - 17s - loss: 0.4753 - accuracy: 0.7706 - val_loss: 0.6880 - val_accuracy: 0.6154
Epoch 67/500
122/122 - 17s - loss: 0.4653 - accuracy: 0.7809 - val_loss: 0.6889 - val_accuracy: 0.6112
Epoch 68/500
122/122 - 17s - loss: 0.4560 - accuracy: 0.7786 - val_loss: 0.6889 - val_accuracy: 0.6112
Epoch 69/500
122/122 - 17s - loss: 0.4483 - accuracy: 0.7850 - val_loss: 0.6900 - val_accuracy: 0.6195
Epoch 70/500
122/122 - 17s - loss: 0.4398 - accuracy: 0.7907 - val_loss: 0.6917 - val_accuracy: 0.6195
Epoch 71/500
122/122 - 17s - loss: 0.4448 - accuracy: 0.7940 - val_loss: 0.6922 - val_accuracy: 0.6133
Epoch 72/500
122/122 - 17s - loss: 0.4322 - accuracy: 0.7958 - val_loss: 0.6933 - val_accuracy: 0.6133
Epoch 73/500
122/122 - 17s - loss: 0.4278 - accuracy: 0.8059 - val_loss: 0.6937 - val_accuracy: 0.6050
Epoch 74/500
122/122 - 17s - loss: 0.4213 - accuracy: 0.8056 - val_loss: 0.6952 - val_accuracy: 0.6071
Epoch 75/500
122/122 - 17s - loss: 0.4093 - accuracy: 0.8051 - val_loss: 0.6975 - val_accuracy: 0.6071
Epoch 76/500
122/122 - 17s - loss: 0.4092 - accuracy: 0.8120 - val_loss: 0.7001 - val_accuracy: 0.6091
Epoch 77/500
122/122 - 17s - loss: 0.3842 - accuracy: 0.8272 - val_loss: 0.7015 - val_accuracy: 0.6071
Epoch 78/500
122/122 - 17s - loss: 0.3838 - accuracy: 0.8262 - val_loss: 0.7017 - val_accuracy: 0.6154
Epoch 79/500
122/122 - 17s - loss: 0.3826 - accuracy: 0.8254 - val_loss: 0.7044 - val_accuracy: 0.6091
Epoch 80/500
122/122 - 17s - loss: 0.3665 - accuracy: 0.8362 - val_loss: 0.7067 - val_accuracy: 0.6154
Epoch 81/500
122/122 - 17s - loss: 0.3632 - accuracy: 0.8336 - val_loss: 0.7075 - val_accuracy: 0.6216
Epoch 82/500
122/122 - 17s - loss: 0.3669 - accuracy: 0.8421 - val_loss: 0.7109 - val_accuracy: 0.6154
Epoch 83/500
122/122 - 17s - loss: 0.3669 - accuracy: 0.8411 - val_loss: 0.7130 - val_accuracy: 0.6154
Epoch 84/500
122/122 - 17s - loss: 0.3433 - accuracy: 0.8447 - val_loss: 0.7148 - val_accuracy: 0.6154
Epoch 85/500
122/122 - 17s - loss: 0.3414 - accuracy: 0.8480 - val_loss: 0.7163 - val_accuracy: 0.6154
Epoch 86/500
122/122 - 17s - loss: 0.3217 - accuracy: 0.8683 - val_loss: 0.7174 - val_accuracy: 0.6175
Epoch 87/500
122/122 - 17s - loss: 0.3417 - accuracy: 0.8537 - val_loss: 0.7208 - val_accuracy: 0.6175
Epoch 88/500
122/122 - 17s - loss: 0.3266 - accuracy: 0.8622 - val_loss: 0.7240 - val_accuracy: 0.6154
Epoch 89/500
122/122 - 17s - loss: 0.3104 - accuracy: 0.8617 - val_loss: 0.7253 - val_accuracy: 0.6175
Epoch 90/500
122/122 - 17s - loss: 0.3195 - accuracy: 0.8578 - val_loss: 0.7277 - val_accuracy: 0.6154
Epoch 91/500
122/122 - 17s - loss: 0.3143 - accuracy: 0.8671 - val_loss: 0.7326 - val_accuracy: 0.6112
Epoch 92/500
122/122 - 17s - loss: 0.3047 - accuracy: 0.8737 - val_loss: 0.7358 - val_accuracy: 0.6112
Epoch 93/500
122/122 - 17s - loss: 0.2958 - accuracy: 0.8766 - val_loss: 0.7364 - val_accuracy: 0.6112
Epoch 94/500
122/122 - 17s - loss: 0.2918 - accuracy: 0.8763 - val_loss: 0.7416 - val_accuracy: 0.6112
Epoch 95/500
122/122 - 17s - loss: 0.2829 - accuracy: 0.8812 - val_loss: 0.7448 - val_accuracy: 0.6071
Epoch 96/500
122/122 - 17s - loss: 0.2871 - accuracy: 0.8825 - val_loss: 0.7486 - val_accuracy: 0.6029
Epoch 97/500
122/122 - 17s - loss: 0.2765 - accuracy: 0.8840 - val_loss: 0.7486 - val_accuracy: 0.6091
Epoch 98/500
122/122 - 17s - loss: 0.2644 - accuracy: 0.8881 - val_loss: 0.7549 - val_accuracy: 0.6112
Epoch 99/500
122/122 - 17s - loss: 0.2616 - accuracy: 0.8928 - val_loss: 0.7570 - val_accuracy: 0.6091
Epoch 100/500
122/122 - 17s - loss: 0.2670 - accuracy: 0.8869 - val_loss: 0.7627 - val_accuracy: 0.6050
Epoch 101/500
122/122 - 17s - loss: 0.2602 - accuracy: 0.8930 - val_loss: 0.7660 - val_accuracy: 0.6071
========================================
save_weights
h5_weights/IMR90.pp/onehot_embedding_cnn_one_branch.h5
========================================

end time >>> Sun Oct  3 17:41:03 2021

end time >>> Sun Oct  3 17:41:03 2021

end time >>> Sun Oct  3 17:41:03 2021

end time >>> Sun Oct  3 17:41:03 2021

end time >>> Sun Oct  3 17:41:03 2021












args.model = onehot_embedding_cnn_one_branch
time used = 1737.5803830623627


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 17:41:04 2021

begin time >>> Sun Oct  3 17:41:04 2021

begin time >>> Sun Oct  3 17:41:04 2021

begin time >>> Sun Oct  3 17:41:04 2021

begin time >>> Sun Oct  3 17:41:04 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_two_branch
args.type = train
args.name = IMR90.pp
args.length = 10001
===========================


-> h5_weights/IMR90.pp folder already exist. pass.
-> result/IMR90.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/IMR90.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/IMR90.pp/onehot_embedding_dense folder already exist. pass.
-> result/IMR90.pp/onehot_dense folder already exist. pass.
-> result/IMR90.pp/onehot_resnet18 folder already exist. pass.
-> result/IMR90.pp/onehot_resnet34 folder already exist. pass.
-> result/IMR90.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/IMR90.pp/embedding_dense folder already exist. pass.
-> result/IMR90.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/IMR90.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
IMR90.pp
########################################

########################################
model_name
onehot_embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
122/122 - 17s - loss: 0.8856 - accuracy: 0.5050 - val_loss: 0.6923 - val_accuracy: 0.5489
Epoch 2/500
122/122 - 17s - loss: 0.8850 - accuracy: 0.4952 - val_loss: 0.6910 - val_accuracy: 0.5489
Epoch 3/500
122/122 - 17s - loss: 0.8584 - accuracy: 0.5125 - val_loss: 0.6907 - val_accuracy: 0.5489
Epoch 4/500
122/122 - 17s - loss: 0.8458 - accuracy: 0.5066 - val_loss: 0.6868 - val_accuracy: 0.5738
Epoch 5/500
122/122 - 17s - loss: 0.8500 - accuracy: 0.5153 - val_loss: 0.6881 - val_accuracy: 0.5593
Epoch 6/500
122/122 - 17s - loss: 0.8513 - accuracy: 0.5202 - val_loss: 0.6971 - val_accuracy: 0.5364
Epoch 7/500
122/122 - 17s - loss: 0.8495 - accuracy: 0.5066 - val_loss: 0.7020 - val_accuracy: 0.5260
Epoch 8/500
122/122 - 17s - loss: 0.8334 - accuracy: 0.5292 - val_loss: 0.7022 - val_accuracy: 0.5260
Epoch 9/500
122/122 - 17s - loss: 0.8142 - accuracy: 0.5315 - val_loss: 0.7003 - val_accuracy: 0.5281
Epoch 10/500
122/122 - 17s - loss: 0.8156 - accuracy: 0.5235 - val_loss: 0.6980 - val_accuracy: 0.5405
Epoch 11/500
122/122 - 17s - loss: 0.8052 - accuracy: 0.5433 - val_loss: 0.6963 - val_accuracy: 0.5468
Epoch 12/500
122/122 - 17s - loss: 0.7960 - accuracy: 0.5477 - val_loss: 0.6942 - val_accuracy: 0.5572
Epoch 13/500
122/122 - 17s - loss: 0.7802 - accuracy: 0.5570 - val_loss: 0.6930 - val_accuracy: 0.5489
Epoch 14/500
122/122 - 17s - loss: 0.7761 - accuracy: 0.5554 - val_loss: 0.6919 - val_accuracy: 0.5489
Epoch 15/500
122/122 - 17s - loss: 0.8049 - accuracy: 0.5441 - val_loss: 0.6890 - val_accuracy: 0.5572
Epoch 16/500
122/122 - 17s - loss: 0.7812 - accuracy: 0.5516 - val_loss: 0.6879 - val_accuracy: 0.5509
Epoch 17/500
122/122 - 17s - loss: 0.7625 - accuracy: 0.5644 - val_loss: 0.6877 - val_accuracy: 0.5489
Epoch 18/500
122/122 - 17s - loss: 0.7621 - accuracy: 0.5639 - val_loss: 0.6866 - val_accuracy: 0.5468
Epoch 19/500
122/122 - 17s - loss: 0.7759 - accuracy: 0.5600 - val_loss: 0.6868 - val_accuracy: 0.5405
Epoch 20/500
122/122 - 17s - loss: 0.7553 - accuracy: 0.5701 - val_loss: 0.6858 - val_accuracy: 0.5405
Epoch 21/500
122/122 - 17s - loss: 0.7447 - accuracy: 0.5814 - val_loss: 0.6847 - val_accuracy: 0.5447
Epoch 22/500
122/122 - 17s - loss: 0.7526 - accuracy: 0.5750 - val_loss: 0.6828 - val_accuracy: 0.5509
Epoch 23/500
122/122 - 17s - loss: 0.7378 - accuracy: 0.5858 - val_loss: 0.6821 - val_accuracy: 0.5613
Epoch 24/500
122/122 - 17s - loss: 0.7381 - accuracy: 0.5855 - val_loss: 0.6817 - val_accuracy: 0.5593
========================================
save_weights
h5_weights/IMR90.pp/onehot_embedding_cnn_two_branch.h5
========================================

end time >>> Sun Oct  3 17:48:05 2021

end time >>> Sun Oct  3 17:48:05 2021

end time >>> Sun Oct  3 17:48:05 2021

end time >>> Sun Oct  3 17:48:05 2021

end time >>> Sun Oct  3 17:48:05 2021












args.model = onehot_embedding_cnn_two_branch
time used = 420.99210381507874


