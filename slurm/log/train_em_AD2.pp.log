************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sat Oct  2 21:20:10 2021

begin time >>> Sat Oct  2 21:20:10 2021

begin time >>> Sat Oct  2 21:20:10 2021

begin time >>> Sat Oct  2 21:20:10 2021

begin time >>> Sat Oct  2 21:20:10 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_dense
args.type = train
args.name = AD2.pp
args.length = 10001
===========================


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sat Oct  2 21:20:50 2021

begin time >>> Sat Oct  2 21:20:50 2021

begin time >>> Sat Oct  2 21:20:50 2021

begin time >>> Sat Oct  2 21:20:50 2021

begin time >>> Sat Oct  2 21:20:50 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_cnn_one_branch
args.type = train
args.name = AD2.pp
args.length = 10001
===========================


-> h5_weights/AD2.pp folder already exist. pass.
-> result/AD2.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/AD2.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/AD2.pp/onehot_embedding_dense folder already exist. pass.
-> result/AD2.pp/onehot_dense folder already exist. pass.
-> result/AD2.pp/onehot_resnet18 folder already exist. pass.
-> result/AD2.pp/onehot_resnet34 folder already exist. pass.
-> result/AD2.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/AD2.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/AD2.pp/embedding_dense folder already exist. pass.
-> result/AD2.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/AD2.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
AD2.pp
########################################

########################################
model_name
embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
136/136 - 18s - loss: 0.8715 - accuracy: 0.5059 - val_loss: 0.6992 - val_accuracy: 0.4916
Epoch 2/500
136/136 - 18s - loss: 0.8760 - accuracy: 0.5027 - val_loss: 0.6985 - val_accuracy: 0.5308
Epoch 3/500
136/136 - 18s - loss: 0.8650 - accuracy: 0.5024 - val_loss: 0.6988 - val_accuracy: 0.5308
Epoch 4/500
136/136 - 18s - loss: 0.8592 - accuracy: 0.5064 - val_loss: 0.6980 - val_accuracy: 0.5383
Epoch 5/500
136/136 - 18s - loss: 0.8602 - accuracy: 0.5068 - val_loss: 0.6972 - val_accuracy: 0.5364
Epoch 6/500
136/136 - 18s - loss: 0.8423 - accuracy: 0.5202 - val_loss: 0.6965 - val_accuracy: 0.5308
Epoch 7/500
136/136 - 18s - loss: 0.8410 - accuracy: 0.5186 - val_loss: 0.6949 - val_accuracy: 0.5327
Epoch 8/500
136/136 - 18s - loss: 0.8453 - accuracy: 0.5186 - val_loss: 0.6946 - val_accuracy: 0.5327
Epoch 9/500
136/136 - 18s - loss: 0.8335 - accuracy: 0.5175 - val_loss: 0.6946 - val_accuracy: 0.5346
Epoch 10/500
136/136 - 18s - loss: 0.8330 - accuracy: 0.5209 - val_loss: 0.6945 - val_accuracy: 0.5402
Epoch 11/500
136/136 - 18s - loss: 0.8244 - accuracy: 0.5401 - val_loss: 0.6945 - val_accuracy: 0.5421
Epoch 12/500
136/136 - 18s - loss: 0.8265 - accuracy: 0.5251 - val_loss: 0.6932 - val_accuracy: 0.5402
Epoch 13/500
136/136 - 18s - loss: 0.8322 - accuracy: 0.5253 - val_loss: 0.6930 - val_accuracy: 0.5421
Epoch 14/500
136/136 - 18s - loss: 0.8150 - accuracy: 0.5288 - val_loss: 0.6924 - val_accuracy: 0.5383
Epoch 15/500
136/136 - 18s - loss: 0.8372 - accuracy: 0.5189 - val_loss: 0.6914 - val_accuracy: 0.5495
Epoch 16/500
136/136 - 18s - loss: 0.8181 - accuracy: 0.5341 - val_loss: 0.6913 - val_accuracy: 0.5495
Epoch 17/500
136/136 - 18s - loss: 0.8052 - accuracy: 0.5434 - val_loss: 0.6916 - val_accuracy: 0.5421
Epoch 18/500
136/136 - 18s - loss: 0.8030 - accuracy: 0.5482 - val_loss: 0.6913 - val_accuracy: 0.5421
Epoch 19/500
136/136 - 18s - loss: 0.7989 - accuracy: 0.5436 - val_loss: 0.6909 - val_accuracy: 0.5458
Epoch 20/500
136/136 - 18s - loss: 0.7891 - accuracy: 0.5431 - val_loss: 0.6908 - val_accuracy: 0.5458
Epoch 21/500
136/136 - 18s - loss: 0.7917 - accuracy: 0.5443 - val_loss: 0.6895 - val_accuracy: 0.5477
Epoch 22/500
136/136 - 18s - loss: 0.8053 - accuracy: 0.5300 - val_loss: 0.6894 - val_accuracy: 0.5495
Epoch 23/500
136/136 - 18s - loss: 0.7885 - accuracy: 0.5461 - val_loss: 0.6892 - val_accuracy: 0.5439
Epoch 24/500
136/136 - 18s - loss: 0.7848 - accuracy: 0.5519 - val_loss: 0.6892 - val_accuracy: 0.5495
Epoch 25/500
136/136 - 18s - loss: 0.7788 - accuracy: 0.5559 - val_loss: 0.6891 - val_accuracy: 0.5514
Epoch 26/500
136/136 - 18s - loss: 0.7728 - accuracy: 0.5559 - val_loss: 0.6895 - val_accuracy: 0.5533
Epoch 27/500
136/136 - 18s - loss: 0.7933 - accuracy: 0.5498 - val_loss: 0.6902 - val_accuracy: 0.5439
Epoch 28/500
136/136 - 18s - loss: 0.7630 - accuracy: 0.5549 - val_loss: 0.6892 - val_accuracy: 0.5551
Epoch 29/500
136/136 - 18s - loss: 0.7750 - accuracy: 0.5542 - val_loss: 0.6881 - val_accuracy: 0.5533
Epoch 30/500
136/136 - 18s - loss: 0.7627 - accuracy: 0.5700 - val_loss: 0.6883 - val_accuracy: 0.5551
Epoch 31/500
136/136 - 18s - loss: 0.7657 - accuracy: 0.5695 - val_loss: 0.6876 - val_accuracy: 0.5533
Epoch 32/500
136/136 - 18s - loss: 0.7795 - accuracy: 0.5589 - val_loss: 0.6872 - val_accuracy: 0.5514
Epoch 33/500
136/136 - 18s - loss: 0.7477 - accuracy: 0.5767 - val_loss: 0.6872 - val_accuracy: 0.5551
Epoch 34/500
136/136 - 18s - loss: 0.7707 - accuracy: 0.5568 - val_loss: 0.6861 - val_accuracy: 0.5589
Epoch 35/500
136/136 - 18s - loss: 0.7419 - accuracy: 0.5767 - val_loss: 0.6876 - val_accuracy: 0.5514
Epoch 36/500
136/136 - 18s - loss: 0.7512 - accuracy: 0.5721 - val_loss: 0.6876 - val_accuracy: 0.5664
Epoch 37/500
136/136 - 18s - loss: 0.7446 - accuracy: 0.5836 - val_loss: 0.6873 - val_accuracy: 0.5589
Epoch 38/500
136/136 - 18s - loss: 0.7422 - accuracy: 0.5785 - val_loss: 0.6868 - val_accuracy: 0.5570
Epoch 39/500
136/136 - 18s - loss: 0.7438 - accuracy: 0.5808 - val_loss: 0.6873 - val_accuracy: 0.5664
Epoch 40/500
136/136 - 18s - loss: 0.7377 - accuracy: 0.5785 - val_loss: 0.6872 - val_accuracy: 0.5664
Epoch 41/500
136/136 - 18s - loss: 0.7148 - accuracy: 0.5843 - val_loss: 0.6865 - val_accuracy: 0.5682
Epoch 42/500
136/136 - 18s - loss: 0.7510 - accuracy: 0.5714 - val_loss: 0.6869 - val_accuracy: 0.5645
Epoch 43/500
136/136 - 18s - loss: 0.7338 - accuracy: 0.5841 - val_loss: 0.6868 - val_accuracy: 0.5701
Epoch 44/500
136/136 - 18s - loss: 0.7254 - accuracy: 0.5857 - val_loss: 0.6861 - val_accuracy: 0.5738
Epoch 45/500
136/136 - 18s - loss: 0.7139 - accuracy: 0.5959 - val_loss: 0.6849 - val_accuracy: 0.5701
Epoch 46/500
136/136 - 18s - loss: 0.7157 - accuracy: 0.5922 - val_loss: 0.6861 - val_accuracy: 0.5720
Epoch 47/500
136/136 - 18s - loss: 0.7174 - accuracy: 0.5959 - val_loss: 0.6866 - val_accuracy: 0.5664
Epoch 48/500
136/136 - 18s - loss: 0.7022 - accuracy: 0.6107 - val_loss: 0.6861 - val_accuracy: 0.5626
Epoch 49/500
136/136 - 18s - loss: 0.7169 - accuracy: 0.6042 - val_loss: 0.6865 - val_accuracy: 0.5645
Epoch 50/500
136/136 - 18s - loss: 0.7046 - accuracy: 0.6056 - val_loss: 0.6862 - val_accuracy: 0.5607
Epoch 51/500
136/136 - 18s - loss: 0.7097 - accuracy: 0.6058 - val_loss: 0.6865 - val_accuracy: 0.5626
Epoch 52/500
136/136 - 18s - loss: 0.7100 - accuracy: 0.6014 - val_loss: 0.6865 - val_accuracy: 0.5607
Epoch 53/500
136/136 - 18s - loss: 0.7127 - accuracy: 0.6035 - val_loss: 0.6861 - val_accuracy: 0.5570
Epoch 54/500
136/136 - 18s - loss: 0.6999 - accuracy: 0.6179 - val_loss: 0.6870 - val_accuracy: 0.5533
Epoch 55/500
136/136 - 18s - loss: 0.7043 - accuracy: 0.6000 - val_loss: 0.6869 - val_accuracy: 0.5607
Epoch 56/500
136/136 - 18s - loss: 0.6922 - accuracy: 0.6199 - val_loss: 0.6862 - val_accuracy: 0.5682
Epoch 57/500
136/136 - 18s - loss: 0.6828 - accuracy: 0.6218 - val_loss: 0.6853 - val_accuracy: 0.5664
Epoch 58/500
136/136 - 18s - loss: 0.6995 - accuracy: 0.6195 - val_loss: 0.6860 - val_accuracy: 0.5701
Epoch 59/500
136/136 - 18s - loss: 0.6893 - accuracy: 0.6151 - val_loss: 0.6864 - val_accuracy: 0.5664
Epoch 60/500
136/136 - 18s - loss: 0.6727 - accuracy: 0.6273 - val_loss: 0.6851 - val_accuracy: 0.5664
Epoch 61/500
136/136 - 18s - loss: 0.6625 - accuracy: 0.6378 - val_loss: 0.6858 - val_accuracy: 0.5664
Epoch 62/500
136/136 - 18s - loss: 0.6623 - accuracy: 0.6278 - val_loss: 0.6854 - val_accuracy: 0.5701
Epoch 63/500
136/136 - 18s - loss: 0.6755 - accuracy: 0.6239 - val_loss: 0.6861 - val_accuracy: 0.5664
Epoch 64/500
136/136 - 18s - loss: 0.6702 - accuracy: 0.6243 - val_loss: 0.6864 - val_accuracy: 0.5645
========================================
save_weights
h5_weights/AD2.pp/embedding_cnn_one_branch.h5
========================================

end time >>> Sat Oct  2 21:40:30 2021

end time >>> Sat Oct  2 21:40:30 2021

end time >>> Sat Oct  2 21:40:30 2021

end time >>> Sat Oct  2 21:40:30 2021

end time >>> Sat Oct  2 21:40:30 2021












args.model = embedding_cnn_one_branch
time used = 1179.667545080185


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sat Oct  2 21:40:31 2021

begin time >>> Sat Oct  2 21:40:31 2021

begin time >>> Sat Oct  2 21:40:31 2021

begin time >>> Sat Oct  2 21:40:31 2021

begin time >>> Sat Oct  2 21:40:31 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_cnn_two_branch
args.type = train
args.name = AD2.pp
args.length = 10001
===========================


-> h5_weights/AD2.pp folder already exist. pass.
-> result/AD2.pp/onehot_cnn_one_branch folder already exist. pass.
-> result/AD2.pp/onehot_cnn_two_branch folder already exist. pass.
-> result/AD2.pp/onehot_embedding_dense folder already exist. pass.
-> result/AD2.pp/onehot_dense folder already exist. pass.
-> result/AD2.pp/onehot_resnet18 folder already exist. pass.
-> result/AD2.pp/onehot_resnet34 folder already exist. pass.
-> result/AD2.pp/embedding_cnn_one_branch folder already exist. pass.
-> result/AD2.pp/embedding_cnn_two_branch folder already exist. pass.
-> result/AD2.pp/embedding_dense folder already exist. pass.
-> result/AD2.pp/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/AD2.pp/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
AD2.pp
########################################

########################################
model_name
embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
136/136 - 18s - loss: 0.8791 - accuracy: 0.4980 - val_loss: 0.6954 - val_accuracy: 0.5121
Epoch 2/500
136/136 - 18s - loss: 0.8659 - accuracy: 0.5135 - val_loss: 0.6989 - val_accuracy: 0.5103
Epoch 3/500
136/136 - 18s - loss: 0.8365 - accuracy: 0.5295 - val_loss: 0.7050 - val_accuracy: 0.5103
Epoch 4/500
136/136 - 18s - loss: 0.8599 - accuracy: 0.5154 - val_loss: 0.7073 - val_accuracy: 0.5159
Epoch 5/500
136/136 - 18s - loss: 0.8624 - accuracy: 0.5047 - val_loss: 0.7077 - val_accuracy: 0.5084
Epoch 6/500
136/136 - 18s - loss: 0.8438 - accuracy: 0.5195 - val_loss: 0.7081 - val_accuracy: 0.5215
Epoch 7/500
136/136 - 18s - loss: 0.8214 - accuracy: 0.5274 - val_loss: 0.7083 - val_accuracy: 0.5290
Epoch 8/500
136/136 - 18s - loss: 0.8382 - accuracy: 0.5212 - val_loss: 0.7073 - val_accuracy: 0.5196
Epoch 9/500
136/136 - 18s - loss: 0.8247 - accuracy: 0.5260 - val_loss: 0.7074 - val_accuracy: 0.5159
Epoch 10/500
136/136 - 18s - loss: 0.8378 - accuracy: 0.5214 - val_loss: 0.7068 - val_accuracy: 0.5159
Epoch 11/500
136/136 - 18s - loss: 0.8261 - accuracy: 0.5283 - val_loss: 0.7068 - val_accuracy: 0.5252
Epoch 12/500
136/136 - 18s - loss: 0.8163 - accuracy: 0.5311 - val_loss: 0.7082 - val_accuracy: 0.5178
Epoch 13/500
136/136 - 18s - loss: 0.8295 - accuracy: 0.5244 - val_loss: 0.7077 - val_accuracy: 0.5215
Epoch 14/500
136/136 - 18s - loss: 0.8165 - accuracy: 0.5235 - val_loss: 0.7076 - val_accuracy: 0.5196
Epoch 15/500
136/136 - 18s - loss: 0.8093 - accuracy: 0.5334 - val_loss: 0.7077 - val_accuracy: 0.5196
Epoch 16/500
136/136 - 18s - loss: 0.7998 - accuracy: 0.5360 - val_loss: 0.7065 - val_accuracy: 0.5121
Epoch 17/500
136/136 - 18s - loss: 0.7978 - accuracy: 0.5427 - val_loss: 0.7066 - val_accuracy: 0.5121
Epoch 18/500
136/136 - 18s - loss: 0.8049 - accuracy: 0.5332 - val_loss: 0.7061 - val_accuracy: 0.5140
Epoch 19/500
136/136 - 18s - loss: 0.7890 - accuracy: 0.5515 - val_loss: 0.7071 - val_accuracy: 0.5196
Epoch 20/500
136/136 - 18s - loss: 0.7803 - accuracy: 0.5529 - val_loss: 0.7061 - val_accuracy: 0.5121
Epoch 21/500
136/136 - 18s - loss: 0.7809 - accuracy: 0.5503 - val_loss: 0.7057 - val_accuracy: 0.5234
Epoch 22/500
136/136 - 18s - loss: 0.7874 - accuracy: 0.5457 - val_loss: 0.7053 - val_accuracy: 0.5234
Epoch 23/500
136/136 - 18s - loss: 0.7845 - accuracy: 0.5526 - val_loss: 0.7056 - val_accuracy: 0.5215
Epoch 24/500
136/136 - 18s - loss: 0.7831 - accuracy: 0.5508 - val_loss: 0.7044 - val_accuracy: 0.5178
Epoch 25/500
136/136 - 18s - loss: 0.7734 - accuracy: 0.5672 - val_loss: 0.7044 - val_accuracy: 0.5121
Epoch 26/500
136/136 - 18s - loss: 0.7756 - accuracy: 0.5610 - val_loss: 0.7038 - val_accuracy: 0.5252
Epoch 27/500
136/136 - 18s - loss: 0.7749 - accuracy: 0.5522 - val_loss: 0.7043 - val_accuracy: 0.5290
========================================
save_weights
h5_weights/AD2.pp/embedding_cnn_two_branch.h5
========================================

end time >>> Sat Oct  2 21:48:54 2021

end time >>> Sat Oct  2 21:48:54 2021

end time >>> Sat Oct  2 21:48:54 2021

end time >>> Sat Oct  2 21:48:54 2021

end time >>> Sat Oct  2 21:48:54 2021












args.model = embedding_cnn_two_branch
time used = 503.4208302497864


