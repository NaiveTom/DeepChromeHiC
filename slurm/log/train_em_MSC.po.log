************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 19:56:14 2021

begin time >>> Sun Oct  3 19:56:14 2021

begin time >>> Sun Oct  3 19:56:14 2021

begin time >>> Sun Oct  3 19:56:14 2021

begin time >>> Sun Oct  3 19:56:14 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_dense
args.type = train
args.name = MSC.po
args.length = 10001
===========================


-> h5_weights/MSC.po folder already exist. pass.
-> result/MSC.po/onehot_cnn_one_branch folder already exist. pass.
-> result/MSC.po/onehot_cnn_two_branch folder already exist. pass.
-> result/MSC.po/onehot_embedding_dense folder already exist. pass.
-> result/MSC.po/onehot_dense folder already exist. pass.
-> result/MSC.po/onehot_resnet18 folder already exist. pass.
-> result/MSC.po/onehot_resnet34 folder already exist. pass.
-> result/MSC.po/embedding_cnn_one_branch folder already exist. pass.
-> result/MSC.po/embedding_cnn_two_branch folder already exist. pass.
-> result/MSC.po/embedding_dense folder already exist. pass.
-> result/MSC.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/MSC.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
MSC.po
########################################

########################################
model_name
embedding_dense
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 626, 64)      409664      concatenate[0][0]                
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, 38, 64)       0           conv1d[0][0]                     
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 38, 64)       256         max_pooling1d[0][0]              
__________________________________________________________________________________________________
flatten (Flatten)               (None, 2432)         0           batch_normalization[0][0]        
__________________________________________________________________________________________________
dropout (Dropout)               (None, 2432)         0           flatten[0][0]                    
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          1245696     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation (Activation)         (None, 512)          0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation[0][0]                 
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 512)          0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_1[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 512)          2048        dense_2[0][0]                    
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 512)          0           activation_2[0][0]               
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            513         dropout_3[0][0]                  
==================================================================================================
Total params: 3,006,985
Trainable params: 3,003,785
Non-trainable params: 3,200
__________________________________________________________________________________________________
Epoch 1/500
553/553 - 72s - loss: 0.9197 - accuracy: 0.4990 - val_loss: 0.6999 - val_accuracy: 0.5073
Epoch 2/500
553/553 - 72s - loss: 0.8804 - accuracy: 0.5000 - val_loss: 0.6993 - val_accuracy: 0.5114
Epoch 3/500
553/553 - 72s - loss: 0.8655 - accuracy: 0.5074 - val_loss: 0.6985 - val_accuracy: 0.5064
Epoch 4/500
553/553 - 72s - loss: 0.8629 - accuracy: 0.5008 - val_loss: 0.6983 - val_accuracy: 0.5078
Epoch 5/500
553/553 - 72s - loss: 0.8507 - accuracy: 0.5022 - val_loss: 0.6974 - val_accuracy: 0.5060
Epoch 6/500
553/553 - 72s - loss: 0.8560 - accuracy: 0.4963 - val_loss: 0.6975 - val_accuracy: 0.5073
Epoch 7/500
553/553 - 72s - loss: 0.8470 - accuracy: 0.5045 - val_loss: 0.6970 - val_accuracy: 0.5110
Epoch 8/500
553/553 - 72s - loss: 0.8358 - accuracy: 0.5067 - val_loss: 0.6962 - val_accuracy: 0.5087
Epoch 9/500
553/553 - 72s - loss: 0.8374 - accuracy: 0.5059 - val_loss: 0.6956 - val_accuracy: 0.5142
Epoch 10/500
553/553 - 72s - loss: 0.8323 - accuracy: 0.5043 - val_loss: 0.6943 - val_accuracy: 0.5192
Epoch 11/500
553/553 - 72s - loss: 0.8260 - accuracy: 0.5127 - val_loss: 0.6949 - val_accuracy: 0.5133
Epoch 12/500
553/553 - 72s - loss: 0.8315 - accuracy: 0.5079 - val_loss: 0.6940 - val_accuracy: 0.5160
Epoch 13/500
553/553 - 72s - loss: 0.8212 - accuracy: 0.5080 - val_loss: 0.6936 - val_accuracy: 0.5165
Epoch 14/500
553/553 - 72s - loss: 0.8240 - accuracy: 0.5063 - val_loss: 0.6928 - val_accuracy: 0.5220
Epoch 15/500
553/553 - 72s - loss: 0.8242 - accuracy: 0.5067 - val_loss: 0.6923 - val_accuracy: 0.5224
Epoch 16/500
553/553 - 72s - loss: 0.8191 - accuracy: 0.5067 - val_loss: 0.6921 - val_accuracy: 0.5238
Epoch 17/500
553/553 - 72s - loss: 0.8128 - accuracy: 0.5090 - val_loss: 0.6917 - val_accuracy: 0.5220
Epoch 18/500
553/553 - 72s - loss: 0.8146 - accuracy: 0.5104 - val_loss: 0.6918 - val_accuracy: 0.5238
Epoch 19/500
553/553 - 72s - loss: 0.8132 - accuracy: 0.5106 - val_loss: 0.6920 - val_accuracy: 0.5279
Epoch 20/500
553/553 - 72s - loss: 0.8189 - accuracy: 0.5010 - val_loss: 0.6907 - val_accuracy: 0.5229
Epoch 21/500
553/553 - 72s - loss: 0.8036 - accuracy: 0.5188 - val_loss: 0.6903 - val_accuracy: 0.5270
Epoch 22/500
553/553 - 72s - loss: 0.8161 - accuracy: 0.5038 - val_loss: 0.6899 - val_accuracy: 0.5288
Epoch 23/500
553/553 - 72s - loss: 0.8095 - accuracy: 0.5101 - val_loss: 0.6899 - val_accuracy: 0.5279
Epoch 24/500
553/553 - 72s - loss: 0.7986 - accuracy: 0.5125 - val_loss: 0.6898 - val_accuracy: 0.5330
Epoch 25/500
553/553 - 72s - loss: 0.8074 - accuracy: 0.5054 - val_loss: 0.6891 - val_accuracy: 0.5353
Epoch 26/500
553/553 - 72s - loss: 0.8005 - accuracy: 0.5123 - val_loss: 0.6889 - val_accuracy: 0.5334
Epoch 27/500
553/553 - 72s - loss: 0.7996 - accuracy: 0.5141 - val_loss: 0.6886 - val_accuracy: 0.5293
Epoch 28/500
553/553 - 72s - loss: 0.7920 - accuracy: 0.5191 - val_loss: 0.6882 - val_accuracy: 0.5325
Epoch 29/500
553/553 - 72s - loss: 0.8083 - accuracy: 0.5023 - val_loss: 0.6883 - val_accuracy: 0.5284
Epoch 30/500
553/553 - 72s - loss: 0.7926 - accuracy: 0.5151 - val_loss: 0.6877 - val_accuracy: 0.5371
Epoch 31/500
553/553 - 72s - loss: 0.7917 - accuracy: 0.5164 - val_loss: 0.6880 - val_accuracy: 0.5362
Epoch 32/500
553/553 - 72s - loss: 0.8010 - accuracy: 0.5107 - val_loss: 0.6873 - val_accuracy: 0.5385
Epoch 33/500
553/553 - 72s - loss: 0.7901 - accuracy: 0.5149 - val_loss: 0.6870 - val_accuracy: 0.5389
Epoch 34/500
553/553 - 72s - loss: 0.7901 - accuracy: 0.5189 - val_loss: 0.6866 - val_accuracy: 0.5389
Epoch 35/500
553/553 - 72s - loss: 0.7842 - accuracy: 0.5180 - val_loss: 0.6866 - val_accuracy: 0.5398
Epoch 36/500
553/553 - 72s - loss: 0.7923 - accuracy: 0.5145 - val_loss: 0.6860 - val_accuracy: 0.5430
Epoch 37/500
553/553 - 72s - loss: 0.7880 - accuracy: 0.5161 - val_loss: 0.6853 - val_accuracy: 0.5412
Epoch 38/500
553/553 - 72s - loss: 0.7965 - accuracy: 0.5098 - val_loss: 0.6855 - val_accuracy: 0.5435
Epoch 39/500
553/553 - 72s - loss: 0.7862 - accuracy: 0.5192 - val_loss: 0.6848 - val_accuracy: 0.5412
Epoch 40/500
553/553 - 72s - loss: 0.7780 - accuracy: 0.5198 - val_loss: 0.6843 - val_accuracy: 0.5467
Epoch 41/500
553/553 - 72s - loss: 0.7840 - accuracy: 0.5145 - val_loss: 0.6842 - val_accuracy: 0.5485
Epoch 42/500
553/553 - 72s - loss: 0.7872 - accuracy: 0.5144 - val_loss: 0.6836 - val_accuracy: 0.5499
Epoch 43/500
553/553 - 72s - loss: 0.7771 - accuracy: 0.5202 - val_loss: 0.6834 - val_accuracy: 0.5476
Epoch 44/500
553/553 - 72s - loss: 0.7863 - accuracy: 0.5141 - val_loss: 0.6829 - val_accuracy: 0.5467
Epoch 45/500
553/553 - 72s - loss: 0.7774 - accuracy: 0.5256 - val_loss: 0.6829 - val_accuracy: 0.5495
Epoch 46/500
553/553 - 72s - loss: 0.7806 - accuracy: 0.5226 - val_loss: 0.6818 - val_accuracy: 0.5490
Epoch 47/500
553/553 - 72s - loss: 0.7755 - accuracy: 0.5224 - val_loss: 0.6818 - val_accuracy: 0.5499
Epoch 48/500
553/553 - 72s - loss: 0.7738 - accuracy: 0.5296 - val_loss: 0.6812 - val_accuracy: 0.5495
Epoch 49/500
553/553 - 72s - loss: 0.7755 - accuracy: 0.5252 - val_loss: 0.6803 - val_accuracy: 0.5536
Epoch 50/500
553/553 - 72s - loss: 0.7711 - accuracy: 0.5265 - val_loss: 0.6801 - val_accuracy: 0.5517
Epoch 51/500
553/553 - 72s - loss: 0.7739 - accuracy: 0.5239 - val_loss: 0.6798 - val_accuracy: 0.5545
Epoch 52/500
553/553 - 72s - loss: 0.7643 - accuracy: 0.5328 - val_loss: 0.6790 - val_accuracy: 0.5536
Epoch 53/500
553/553 - 72s - loss: 0.7706 - accuracy: 0.5308 - val_loss: 0.6788 - val_accuracy: 0.5595
Epoch 54/500
553/553 - 72s - loss: 0.7674 - accuracy: 0.5269 - val_loss: 0.6782 - val_accuracy: 0.5609
Epoch 55/500
553/553 - 72s - loss: 0.7672 - accuracy: 0.5228 - val_loss: 0.6777 - val_accuracy: 0.5655
Epoch 56/500
553/553 - 72s - loss: 0.7639 - accuracy: 0.5303 - val_loss: 0.6769 - val_accuracy: 0.5678
Epoch 57/500
553/553 - 72s - loss: 0.7611 - accuracy: 0.5316 - val_loss: 0.6760 - val_accuracy: 0.5687
Epoch 58/500
553/553 - 72s - loss: 0.7590 - accuracy: 0.5380 - val_loss: 0.6755 - val_accuracy: 0.5682
Epoch 59/500
553/553 - 72s - loss: 0.7564 - accuracy: 0.5389 - val_loss: 0.6753 - val_accuracy: 0.5710
Epoch 60/500
553/553 - 72s - loss: 0.7518 - accuracy: 0.5410 - val_loss: 0.6745 - val_accuracy: 0.5737
Epoch 61/500
553/553 - 72s - loss: 0.7599 - accuracy: 0.5374 - val_loss: 0.6733 - val_accuracy: 0.5751
Epoch 62/500
553/553 - 72s - loss: 0.7507 - accuracy: 0.5401 - val_loss: 0.6720 - val_accuracy: 0.5810
Epoch 63/500
553/553 - 72s - loss: 0.7571 - accuracy: 0.5381 - val_loss: 0.6719 - val_accuracy: 0.5792
Epoch 64/500
553/553 - 72s - loss: 0.7471 - accuracy: 0.5439 - val_loss: 0.6707 - val_accuracy: 0.5824
Epoch 65/500
553/553 - 72s - loss: 0.7461 - accuracy: 0.5473 - val_loss: 0.6703 - val_accuracy: 0.5838
Epoch 66/500
553/553 - 72s - loss: 0.7507 - accuracy: 0.5480 - val_loss: 0.6692 - val_accuracy: 0.5865
Epoch 67/500
553/553 - 72s - loss: 0.7447 - accuracy: 0.5449 - val_loss: 0.6682 - val_accuracy: 0.5861
Epoch 68/500
553/553 - 72s - loss: 0.7388 - accuracy: 0.5530 - val_loss: 0.6669 - val_accuracy: 0.5852
Epoch 69/500
553/553 - 72s - loss: 0.7399 - accuracy: 0.5548 - val_loss: 0.6663 - val_accuracy: 0.5884
Epoch 70/500
553/553 - 72s - loss: 0.7314 - accuracy: 0.5608 - val_loss: 0.6648 - val_accuracy: 0.5911
Epoch 71/500
553/553 - 72s - loss: 0.7294 - accuracy: 0.5662 - val_loss: 0.6648 - val_accuracy: 0.5897
Epoch 72/500
553/553 - 72s - loss: 0.7384 - accuracy: 0.5551 - val_loss: 0.6633 - val_accuracy: 0.5907
Epoch 73/500
553/553 - 72s - loss: 0.7267 - accuracy: 0.5630 - val_loss: 0.6628 - val_accuracy: 0.5929
Epoch 74/500
553/553 - 72s - loss: 0.7233 - accuracy: 0.5708 - val_loss: 0.6613 - val_accuracy: 0.5994
Epoch 75/500
553/553 - 72s - loss: 0.7307 - accuracy: 0.5617 - val_loss: 0.6597 - val_accuracy: 0.5994
Epoch 76/500
553/553 - 72s - loss: 0.7234 - accuracy: 0.5676 - val_loss: 0.6579 - val_accuracy: 0.6021
Epoch 77/500
553/553 - 72s - loss: 0.7220 - accuracy: 0.5705 - val_loss: 0.6570 - val_accuracy: 0.6049
Epoch 78/500
553/553 - 72s - loss: 0.7165 - accuracy: 0.5759 - val_loss: 0.6564 - val_accuracy: 0.6076
Epoch 79/500
553/553 - 72s - loss: 0.7120 - accuracy: 0.5791 - val_loss: 0.6550 - val_accuracy: 0.6076
Epoch 80/500
553/553 - 72s - loss: 0.7098 - accuracy: 0.5844 - val_loss: 0.6533 - val_accuracy: 0.6108
Epoch 81/500
553/553 - 72s - loss: 0.7041 - accuracy: 0.5925 - val_loss: 0.6522 - val_accuracy: 0.6108
Epoch 82/500
553/553 - 72s - loss: 0.7087 - accuracy: 0.5855 - val_loss: 0.6505 - val_accuracy: 0.6122
Epoch 83/500
553/553 - 72s - loss: 0.7034 - accuracy: 0.5912 - val_loss: 0.6491 - val_accuracy: 0.6149
Epoch 84/500
553/553 - 72s - loss: 0.7038 - accuracy: 0.5882 - val_loss: 0.6481 - val_accuracy: 0.6108
Epoch 85/500
553/553 - 72s - loss: 0.6989 - accuracy: 0.5960 - val_loss: 0.6466 - val_accuracy: 0.6177
Epoch 86/500
553/553 - 72s - loss: 0.6928 - accuracy: 0.6014 - val_loss: 0.6453 - val_accuracy: 0.6140
Epoch 87/500
553/553 - 72s - loss: 0.6873 - accuracy: 0.6065 - val_loss: 0.6435 - val_accuracy: 0.6232
Epoch 88/500
553/553 - 72s - loss: 0.6874 - accuracy: 0.6095 - val_loss: 0.6418 - val_accuracy: 0.6268
Epoch 89/500
553/553 - 72s - loss: 0.6854 - accuracy: 0.6069 - val_loss: 0.6403 - val_accuracy: 0.6273
Epoch 90/500
553/553 - 72s - loss: 0.6766 - accuracy: 0.6160 - val_loss: 0.6392 - val_accuracy: 0.6287
Epoch 91/500
553/553 - 72s - loss: 0.6718 - accuracy: 0.6192 - val_loss: 0.6379 - val_accuracy: 0.6328
Epoch 92/500
553/553 - 72s - loss: 0.6702 - accuracy: 0.6228 - val_loss: 0.6373 - val_accuracy: 0.6291
Epoch 93/500
553/553 - 72s - loss: 0.6646 - accuracy: 0.6291 - val_loss: 0.6353 - val_accuracy: 0.6314
Epoch 94/500
553/553 - 72s - loss: 0.6591 - accuracy: 0.6308 - val_loss: 0.6340 - val_accuracy: 0.6305
Epoch 95/500
553/553 - 72s - loss: 0.6586 - accuracy: 0.6325 - val_loss: 0.6332 - val_accuracy: 0.6323
Epoch 96/500
553/553 - 72s - loss: 0.6575 - accuracy: 0.6347 - val_loss: 0.6324 - val_accuracy: 0.6319
Epoch 97/500
553/553 - 72s - loss: 0.6558 - accuracy: 0.6427 - val_loss: 0.6305 - val_accuracy: 0.6351
Epoch 98/500
553/553 - 72s - loss: 0.6482 - accuracy: 0.6402 - val_loss: 0.6307 - val_accuracy: 0.6346
Epoch 99/500
553/553 - 72s - loss: 0.6406 - accuracy: 0.6503 - val_loss: 0.6291 - val_accuracy: 0.6351
Epoch 100/500
553/553 - 72s - loss: 0.6394 - accuracy: 0.6490 - val_loss: 0.6268 - val_accuracy: 0.6360
Epoch 101/500
553/553 - 72s - loss: 0.6348 - accuracy: 0.6583 - val_loss: 0.6260 - val_accuracy: 0.6378
Epoch 102/500
553/553 - 72s - loss: 0.6305 - accuracy: 0.6554 - val_loss: 0.6248 - val_accuracy: 0.6401
Epoch 103/500
553/553 - 72s - loss: 0.6233 - accuracy: 0.6659 - val_loss: 0.6240 - val_accuracy: 0.6374
Epoch 104/500
553/553 - 72s - loss: 0.6216 - accuracy: 0.6711 - val_loss: 0.6227 - val_accuracy: 0.6419
Epoch 105/500
553/553 - 72s - loss: 0.6217 - accuracy: 0.6672 - val_loss: 0.6225 - val_accuracy: 0.6383
Epoch 106/500
553/553 - 72s - loss: 0.6142 - accuracy: 0.6741 - val_loss: 0.6215 - val_accuracy: 0.6415
Epoch 107/500
553/553 - 72s - loss: 0.6149 - accuracy: 0.6740 - val_loss: 0.6209 - val_accuracy: 0.6392
Epoch 108/500
553/553 - 72s - loss: 0.6091 - accuracy: 0.6817 - val_loss: 0.6215 - val_accuracy: 0.6410
Epoch 109/500
553/553 - 72s - loss: 0.5993 - accuracy: 0.6839 - val_loss: 0.6207 - val_accuracy: 0.6442
Epoch 110/500
553/553 - 72s - loss: 0.5932 - accuracy: 0.6909 - val_loss: 0.6200 - val_accuracy: 0.6442
Epoch 111/500
553/553 - 72s - loss: 0.5926 - accuracy: 0.6912 - val_loss: 0.6193 - val_accuracy: 0.6488
Epoch 112/500
553/553 - 72s - loss: 0.5947 - accuracy: 0.6920 - val_loss: 0.6191 - val_accuracy: 0.6447
Epoch 113/500
553/553 - 72s - loss: 0.5785 - accuracy: 0.7005 - val_loss: 0.6186 - val_accuracy: 0.6479
Epoch 114/500
553/553 - 72s - loss: 0.5799 - accuracy: 0.7032 - val_loss: 0.6188 - val_accuracy: 0.6461
Epoch 115/500
553/553 - 72s - loss: 0.5743 - accuracy: 0.7096 - val_loss: 0.6180 - val_accuracy: 0.6493
Epoch 116/500
553/553 - 72s - loss: 0.5730 - accuracy: 0.7089 - val_loss: 0.6162 - val_accuracy: 0.6493
Epoch 117/500
553/553 - 72s - loss: 0.5631 - accuracy: 0.7102 - val_loss: 0.6163 - val_accuracy: 0.6506
Epoch 118/500
553/553 - 72s - loss: 0.5579 - accuracy: 0.7169 - val_loss: 0.6166 - val_accuracy: 0.6516
Epoch 119/500
553/553 - 72s - loss: 0.5621 - accuracy: 0.7160 - val_loss: 0.6175 - val_accuracy: 0.6511
Epoch 120/500
553/553 - 72s - loss: 0.5609 - accuracy: 0.7207 - val_loss: 0.6183 - val_accuracy: 0.6511
Epoch 121/500
553/553 - 72s - loss: 0.5573 - accuracy: 0.7213 - val_loss: 0.6162 - val_accuracy: 0.6529
Epoch 122/500
553/553 - 72s - loss: 0.5420 - accuracy: 0.7320 - val_loss: 0.6175 - val_accuracy: 0.6538
Epoch 123/500
553/553 - 72s - loss: 0.5439 - accuracy: 0.7289 - val_loss: 0.6173 - val_accuracy: 0.6561
Epoch 124/500
553/553 - 72s - loss: 0.5410 - accuracy: 0.7334 - val_loss: 0.6156 - val_accuracy: 0.6571
Epoch 125/500
553/553 - 72s - loss: 0.5377 - accuracy: 0.7345 - val_loss: 0.6155 - val_accuracy: 0.6593
Epoch 126/500
553/553 - 72s - loss: 0.5377 - accuracy: 0.7380 - val_loss: 0.6170 - val_accuracy: 0.6589
Epoch 127/500
553/553 - 73s - loss: 0.5242 - accuracy: 0.7413 - val_loss: 0.6194 - val_accuracy: 0.6571
Epoch 128/500
553/553 - 72s - loss: 0.5267 - accuracy: 0.7456 - val_loss: 0.6167 - val_accuracy: 0.6612
Epoch 129/500
553/553 - 72s - loss: 0.5105 - accuracy: 0.7546 - val_loss: 0.6201 - val_accuracy: 0.6575
Epoch 130/500
553/553 - 72s - loss: 0.5168 - accuracy: 0.7494 - val_loss: 0.6202 - val_accuracy: 0.6603
Epoch 131/500
553/553 - 72s - loss: 0.5029 - accuracy: 0.7556 - val_loss: 0.6198 - val_accuracy: 0.6612
Epoch 132/500
553/553 - 72s - loss: 0.5047 - accuracy: 0.7555 - val_loss: 0.6195 - val_accuracy: 0.6607
Epoch 133/500
553/553 - 72s - loss: 0.5001 - accuracy: 0.7619 - val_loss: 0.6191 - val_accuracy: 0.6607
Epoch 134/500
553/553 - 72s - loss: 0.5001 - accuracy: 0.7618 - val_loss: 0.6212 - val_accuracy: 0.6625
Epoch 135/500
553/553 - 72s - loss: 0.4893 - accuracy: 0.7692 - val_loss: 0.6212 - val_accuracy: 0.6639
Epoch 136/500
553/553 - 72s - loss: 0.4899 - accuracy: 0.7684 - val_loss: 0.6249 - val_accuracy: 0.6575
Epoch 137/500
553/553 - 72s - loss: 0.4921 - accuracy: 0.7647 - val_loss: 0.6230 - val_accuracy: 0.6639
Epoch 138/500
553/553 - 72s - loss: 0.4823 - accuracy: 0.7749 - val_loss: 0.6250 - val_accuracy: 0.6616
Epoch 139/500
553/553 - 72s - loss: 0.4901 - accuracy: 0.7721 - val_loss: 0.6280 - val_accuracy: 0.6575
Epoch 140/500
553/553 - 72s - loss: 0.4805 - accuracy: 0.7748 - val_loss: 0.6282 - val_accuracy: 0.6593
Epoch 141/500
553/553 - 72s - loss: 0.4781 - accuracy: 0.7745 - val_loss: 0.6265 - val_accuracy: 0.6603
Epoch 142/500
553/553 - 72s - loss: 0.4718 - accuracy: 0.7824 - val_loss: 0.6261 - val_accuracy: 0.6639
Epoch 143/500
553/553 - 72s - loss: 0.4724 - accuracy: 0.7818 - val_loss: 0.6287 - val_accuracy: 0.6593
Epoch 144/500
553/553 - 72s - loss: 0.4583 - accuracy: 0.7871 - val_loss: 0.6317 - val_accuracy: 0.6607
Epoch 145/500
553/553 - 72s - loss: 0.4632 - accuracy: 0.7860 - val_loss: 0.6318 - val_accuracy: 0.6612
Epoch 146/500
553/553 - 72s - loss: 0.4490 - accuracy: 0.7927 - val_loss: 0.6347 - val_accuracy: 0.6621
Epoch 147/500
553/553 - 72s - loss: 0.4478 - accuracy: 0.7941 - val_loss: 0.6338 - val_accuracy: 0.6625
Epoch 148/500
553/553 - 72s - loss: 0.4505 - accuracy: 0.7909 - val_loss: 0.6333 - val_accuracy: 0.6630
Epoch 149/500
553/553 - 72s - loss: 0.4427 - accuracy: 0.7986 - val_loss: 0.6337 - val_accuracy: 0.6625
Epoch 150/500
553/553 - 72s - loss: 0.4396 - accuracy: 0.7941 - val_loss: 0.6380 - val_accuracy: 0.6644
Epoch 151/500
553/553 - 72s - loss: 0.4363 - accuracy: 0.7985 - val_loss: 0.6421 - val_accuracy: 0.6653
Epoch 152/500
553/553 - 72s - loss: 0.4340 - accuracy: 0.8026 - val_loss: 0.6404 - val_accuracy: 0.6685
Epoch 153/500
553/553 - 72s - loss: 0.4224 - accuracy: 0.8075 - val_loss: 0.6422 - val_accuracy: 0.6667
Epoch 154/500
553/553 - 72s - loss: 0.4283 - accuracy: 0.8053 - val_loss: 0.6454 - val_accuracy: 0.6676
Epoch 155/500
553/553 - 72s - loss: 0.4255 - accuracy: 0.8083 - val_loss: 0.6502 - val_accuracy: 0.6648
Epoch 156/500
553/553 - 72s - loss: 0.4149 - accuracy: 0.8095 - val_loss: 0.6466 - val_accuracy: 0.6708
Epoch 157/500
553/553 - 72s - loss: 0.4187 - accuracy: 0.8092 - val_loss: 0.6481 - val_accuracy: 0.6694
Epoch 158/500
553/553 - 72s - loss: 0.4144 - accuracy: 0.8131 - val_loss: 0.6505 - val_accuracy: 0.6708
Epoch 159/500
553/553 - 72s - loss: 0.4157 - accuracy: 0.8134 - val_loss: 0.6496 - val_accuracy: 0.6722
Epoch 160/500
553/553 - 72s - loss: 0.4102 - accuracy: 0.8143 - val_loss: 0.6535 - val_accuracy: 0.6694
Epoch 161/500
553/553 - 72s - loss: 0.4038 - accuracy: 0.8226 - val_loss: 0.6506 - val_accuracy: 0.6694
Epoch 162/500
553/553 - 72s - loss: 0.3989 - accuracy: 0.8196 - val_loss: 0.6589 - val_accuracy: 0.6699
Epoch 163/500
553/553 - 72s - loss: 0.3955 - accuracy: 0.8240 - val_loss: 0.6540 - val_accuracy: 0.6731
Epoch 164/500
553/553 - 72s - loss: 0.3867 - accuracy: 0.8282 - val_loss: 0.6549 - val_accuracy: 0.6717
Epoch 165/500
553/553 - 73s - loss: 0.3909 - accuracy: 0.8250 - val_loss: 0.6608 - val_accuracy: 0.6703
Epoch 166/500
553/553 - 73s - loss: 0.3849 - accuracy: 0.8280 - val_loss: 0.6648 - val_accuracy: 0.6690
Epoch 167/500
553/553 - 73s - loss: 0.3842 - accuracy: 0.8326 - val_loss: 0.6656 - val_accuracy: 0.6717
Epoch 168/500
553/553 - 73s - loss: 0.3760 - accuracy: 0.8351 - val_loss: 0.6637 - val_accuracy: 0.6717
Epoch 169/500
553/553 - 73s - loss: 0.3764 - accuracy: 0.8317 - val_loss: 0.6652 - val_accuracy: 0.6703
Epoch 170/500
553/553 - 73s - loss: 0.3680 - accuracy: 0.8396 - val_loss: 0.6688 - val_accuracy: 0.6703
Epoch 171/500
553/553 - 73s - loss: 0.3668 - accuracy: 0.8354 - val_loss: 0.6725 - val_accuracy: 0.6690
Epoch 172/500
553/553 - 73s - loss: 0.3683 - accuracy: 0.8365 - val_loss: 0.6702 - val_accuracy: 0.6735
Epoch 173/500
553/553 - 73s - loss: 0.3684 - accuracy: 0.8388 - val_loss: 0.6769 - val_accuracy: 0.6708
Epoch 174/500
553/553 - 73s - loss: 0.3586 - accuracy: 0.8418 - val_loss: 0.6773 - val_accuracy: 0.6712
Epoch 175/500
553/553 - 73s - loss: 0.3534 - accuracy: 0.8457 - val_loss: 0.6802 - val_accuracy: 0.6703
Epoch 176/500
553/553 - 73s - loss: 0.3557 - accuracy: 0.8460 - val_loss: 0.6834 - val_accuracy: 0.6712
Epoch 177/500
553/553 - 73s - loss: 0.3534 - accuracy: 0.8449 - val_loss: 0.6841 - val_accuracy: 0.6708
Epoch 178/500
553/553 - 73s - loss: 0.3484 - accuracy: 0.8484 - val_loss: 0.6846 - val_accuracy: 0.6717
Epoch 179/500
553/553 - 73s - loss: 0.3493 - accuracy: 0.8477 - val_loss: 0.6917 - val_accuracy: 0.6703
Epoch 180/500
553/553 - 73s - loss: 0.3327 - accuracy: 0.8565 - val_loss: 0.6909 - val_accuracy: 0.6717
Epoch 181/500
553/553 - 73s - loss: 0.3409 - accuracy: 0.8535 - val_loss: 0.6941 - val_accuracy: 0.6703
Epoch 182/500
553/553 - 73s - loss: 0.3391 - accuracy: 0.8547 - val_loss: 0.6929 - val_accuracy: 0.6708
Epoch 183/500
553/553 - 73s - loss: 0.3379 - accuracy: 0.8541 - val_loss: 0.6952 - val_accuracy: 0.6699
Epoch 184/500
553/553 - 73s - loss: 0.3344 - accuracy: 0.8550 - val_loss: 0.6987 - val_accuracy: 0.6735
Epoch 185/500
553/553 - 73s - loss: 0.3278 - accuracy: 0.8595 - val_loss: 0.6971 - val_accuracy: 0.6726
Epoch 186/500
553/553 - 73s - loss: 0.3254 - accuracy: 0.8594 - val_loss: 0.6977 - val_accuracy: 0.6717
Epoch 187/500
553/553 - 73s - loss: 0.3258 - accuracy: 0.8589 - val_loss: 0.6982 - val_accuracy: 0.6717
Epoch 188/500
553/553 - 73s - loss: 0.3240 - accuracy: 0.8582 - val_loss: 0.7063 - val_accuracy: 0.6722
Epoch 189/500
553/553 - 73s - loss: 0.3214 - accuracy: 0.8616 - val_loss: 0.7075 - val_accuracy: 0.6735
Epoch 190/500
553/553 - 73s - loss: 0.3133 - accuracy: 0.8647 - val_loss: 0.7119 - val_accuracy: 0.6712
Epoch 191/500
553/553 - 73s - loss: 0.3149 - accuracy: 0.8647 - val_loss: 0.7105 - val_accuracy: 0.6722
Epoch 192/500
553/553 - 73s - loss: 0.3106 - accuracy: 0.8679 - val_loss: 0.7129 - val_accuracy: 0.6726
========================================
save_weights
h5_weights/MSC.po/embedding_dense.h5
========================================

end time >>> Sun Oct  3 23:48:13 2021

end time >>> Sun Oct  3 23:48:13 2021

end time >>> Sun Oct  3 23:48:13 2021

end time >>> Sun Oct  3 23:48:13 2021

end time >>> Sun Oct  3 23:48:13 2021












args.model = embedding_dense
time used = 13919.277100801468


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 23:48:15 2021

begin time >>> Sun Oct  3 23:48:15 2021

begin time >>> Sun Oct  3 23:48:15 2021

begin time >>> Sun Oct  3 23:48:15 2021

begin time >>> Sun Oct  3 23:48:15 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_cnn_one_branch
args.type = train
args.name = MSC.po
args.length = 10001
===========================


-> h5_weights/MSC.po folder already exist. pass.
-> result/MSC.po/onehot_cnn_one_branch folder already exist. pass.
-> result/MSC.po/onehot_cnn_two_branch folder already exist. pass.
-> result/MSC.po/onehot_embedding_dense folder already exist. pass.
-> result/MSC.po/onehot_dense folder already exist. pass.
-> result/MSC.po/onehot_resnet18 folder already exist. pass.
-> result/MSC.po/onehot_resnet34 folder already exist. pass.
-> result/MSC.po/embedding_cnn_one_branch folder already exist. pass.
-> result/MSC.po/embedding_cnn_two_branch folder already exist. pass.
-> result/MSC.po/embedding_dense folder already exist. pass.
-> result/MSC.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/MSC.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
MSC.po
########################################

########################################
model_name
embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
553/553 - 77s - loss: 0.8972 - accuracy: 0.4994 - val_loss: 0.7051 - val_accuracy: 0.5119
Epoch 2/500
553/553 - 77s - loss: 0.8794 - accuracy: 0.5053 - val_loss: 0.7016 - val_accuracy: 0.5238
Epoch 3/500
553/553 - 77s - loss: 0.8745 - accuracy: 0.5027 - val_loss: 0.6965 - val_accuracy: 0.5307
Epoch 4/500
553/553 - 77s - loss: 0.8554 - accuracy: 0.5108 - val_loss: 0.6930 - val_accuracy: 0.5334
Epoch 5/500
553/553 - 77s - loss: 0.8377 - accuracy: 0.5209 - val_loss: 0.6895 - val_accuracy: 0.5426
Epoch 6/500
553/553 - 77s - loss: 0.8278 - accuracy: 0.5243 - val_loss: 0.6867 - val_accuracy: 0.5435
Epoch 7/500
553/553 - 77s - loss: 0.8280 - accuracy: 0.5248 - val_loss: 0.6849 - val_accuracy: 0.5476
Epoch 8/500
553/553 - 77s - loss: 0.8147 - accuracy: 0.5292 - val_loss: 0.6819 - val_accuracy: 0.5559
Epoch 9/500
553/553 - 77s - loss: 0.8051 - accuracy: 0.5371 - val_loss: 0.6803 - val_accuracy: 0.5572
Epoch 10/500
553/553 - 77s - loss: 0.8041 - accuracy: 0.5357 - val_loss: 0.6783 - val_accuracy: 0.5641
Epoch 11/500
553/553 - 77s - loss: 0.8038 - accuracy: 0.5365 - val_loss: 0.6763 - val_accuracy: 0.5705
Epoch 12/500
553/553 - 77s - loss: 0.7963 - accuracy: 0.5359 - val_loss: 0.6747 - val_accuracy: 0.5710
Epoch 13/500
553/553 - 77s - loss: 0.7884 - accuracy: 0.5439 - val_loss: 0.6730 - val_accuracy: 0.5751
Epoch 14/500
553/553 - 77s - loss: 0.7851 - accuracy: 0.5493 - val_loss: 0.6708 - val_accuracy: 0.5765
Epoch 15/500
553/553 - 77s - loss: 0.7866 - accuracy: 0.5425 - val_loss: 0.6695 - val_accuracy: 0.5815
Epoch 16/500
553/553 - 77s - loss: 0.7677 - accuracy: 0.5568 - val_loss: 0.6679 - val_accuracy: 0.5815
Epoch 17/500
553/553 - 77s - loss: 0.7710 - accuracy: 0.5564 - val_loss: 0.6659 - val_accuracy: 0.5897
Epoch 18/500
553/553 - 77s - loss: 0.7713 - accuracy: 0.5539 - val_loss: 0.6640 - val_accuracy: 0.5934
Epoch 19/500
553/553 - 77s - loss: 0.7579 - accuracy: 0.5663 - val_loss: 0.6622 - val_accuracy: 0.5975
Epoch 20/500
553/553 - 77s - loss: 0.7643 - accuracy: 0.5617 - val_loss: 0.6609 - val_accuracy: 0.5934
Epoch 21/500
553/553 - 77s - loss: 0.7494 - accuracy: 0.5694 - val_loss: 0.6592 - val_accuracy: 0.5984
Epoch 22/500
553/553 - 77s - loss: 0.7494 - accuracy: 0.5675 - val_loss: 0.6570 - val_accuracy: 0.6035
Epoch 23/500
553/553 - 77s - loss: 0.7359 - accuracy: 0.5816 - val_loss: 0.6560 - val_accuracy: 0.6044
Epoch 24/500
553/553 - 77s - loss: 0.7391 - accuracy: 0.5770 - val_loss: 0.6545 - val_accuracy: 0.6071
Epoch 25/500
553/553 - 77s - loss: 0.7322 - accuracy: 0.5783 - val_loss: 0.6529 - val_accuracy: 0.6049
Epoch 26/500
553/553 - 77s - loss: 0.7181 - accuracy: 0.5952 - val_loss: 0.6509 - val_accuracy: 0.6071
Epoch 27/500
553/553 - 77s - loss: 0.7265 - accuracy: 0.5902 - val_loss: 0.6496 - val_accuracy: 0.6117
Epoch 28/500
553/553 - 77s - loss: 0.7248 - accuracy: 0.5880 - val_loss: 0.6479 - val_accuracy: 0.6177
Epoch 29/500
553/553 - 77s - loss: 0.7185 - accuracy: 0.5938 - val_loss: 0.6458 - val_accuracy: 0.6154
Epoch 30/500
553/553 - 77s - loss: 0.7114 - accuracy: 0.6005 - val_loss: 0.6445 - val_accuracy: 0.6227
Epoch 31/500
553/553 - 77s - loss: 0.7027 - accuracy: 0.6061 - val_loss: 0.6431 - val_accuracy: 0.6213
Epoch 32/500
553/553 - 77s - loss: 0.6996 - accuracy: 0.6086 - val_loss: 0.6414 - val_accuracy: 0.6227
Epoch 33/500
553/553 - 77s - loss: 0.6953 - accuracy: 0.6157 - val_loss: 0.6399 - val_accuracy: 0.6227
Epoch 34/500
553/553 - 76s - loss: 0.6836 - accuracy: 0.6216 - val_loss: 0.6383 - val_accuracy: 0.6287
Epoch 35/500
553/553 - 76s - loss: 0.6794 - accuracy: 0.6235 - val_loss: 0.6367 - val_accuracy: 0.6264
Epoch 36/500
553/553 - 76s - loss: 0.6749 - accuracy: 0.6340 - val_loss: 0.6353 - val_accuracy: 0.6319
Epoch 37/500
553/553 - 76s - loss: 0.6599 - accuracy: 0.6376 - val_loss: 0.6341 - val_accuracy: 0.6319
Epoch 38/500
553/553 - 76s - loss: 0.6672 - accuracy: 0.6390 - val_loss: 0.6337 - val_accuracy: 0.6319
Epoch 39/500
553/553 - 76s - loss: 0.6604 - accuracy: 0.6419 - val_loss: 0.6317 - val_accuracy: 0.6351
Epoch 40/500
553/553 - 76s - loss: 0.6501 - accuracy: 0.6492 - val_loss: 0.6309 - val_accuracy: 0.6323
Epoch 41/500
553/553 - 76s - loss: 0.6496 - accuracy: 0.6533 - val_loss: 0.6292 - val_accuracy: 0.6378
Epoch 42/500
553/553 - 76s - loss: 0.6433 - accuracy: 0.6550 - val_loss: 0.6284 - val_accuracy: 0.6364
Epoch 43/500
553/553 - 76s - loss: 0.6375 - accuracy: 0.6630 - val_loss: 0.6280 - val_accuracy: 0.6374
Epoch 44/500
553/553 - 76s - loss: 0.6345 - accuracy: 0.6576 - val_loss: 0.6275 - val_accuracy: 0.6397
Epoch 45/500
553/553 - 76s - loss: 0.6297 - accuracy: 0.6680 - val_loss: 0.6267 - val_accuracy: 0.6406
Epoch 46/500
553/553 - 76s - loss: 0.6280 - accuracy: 0.6735 - val_loss: 0.6260 - val_accuracy: 0.6484
Epoch 47/500
553/553 - 76s - loss: 0.6224 - accuracy: 0.6735 - val_loss: 0.6248 - val_accuracy: 0.6429
Epoch 48/500
553/553 - 76s - loss: 0.6142 - accuracy: 0.6795 - val_loss: 0.6249 - val_accuracy: 0.6451
Epoch 49/500
553/553 - 76s - loss: 0.6042 - accuracy: 0.6856 - val_loss: 0.6241 - val_accuracy: 0.6484
Epoch 50/500
553/553 - 76s - loss: 0.5976 - accuracy: 0.6921 - val_loss: 0.6237 - val_accuracy: 0.6497
Epoch 51/500
553/553 - 76s - loss: 0.5940 - accuracy: 0.6895 - val_loss: 0.6230 - val_accuracy: 0.6493
Epoch 52/500
553/553 - 76s - loss: 0.5905 - accuracy: 0.6921 - val_loss: 0.6220 - val_accuracy: 0.6543
Epoch 53/500
553/553 - 76s - loss: 0.5752 - accuracy: 0.7070 - val_loss: 0.6220 - val_accuracy: 0.6497
Epoch 54/500
553/553 - 77s - loss: 0.5846 - accuracy: 0.7018 - val_loss: 0.6215 - val_accuracy: 0.6511
Epoch 55/500
553/553 - 76s - loss: 0.5703 - accuracy: 0.7135 - val_loss: 0.6223 - val_accuracy: 0.6511
Epoch 56/500
553/553 - 76s - loss: 0.5740 - accuracy: 0.7088 - val_loss: 0.6214 - val_accuracy: 0.6538
Epoch 57/500
553/553 - 76s - loss: 0.5687 - accuracy: 0.7128 - val_loss: 0.6215 - val_accuracy: 0.6538
Epoch 58/500
553/553 - 76s - loss: 0.5614 - accuracy: 0.7176 - val_loss: 0.6216 - val_accuracy: 0.6548
Epoch 59/500
553/553 - 76s - loss: 0.5513 - accuracy: 0.7250 - val_loss: 0.6221 - val_accuracy: 0.6529
Epoch 60/500
553/553 - 76s - loss: 0.5407 - accuracy: 0.7320 - val_loss: 0.6214 - val_accuracy: 0.6589
Epoch 61/500
553/553 - 76s - loss: 0.5488 - accuracy: 0.7273 - val_loss: 0.6221 - val_accuracy: 0.6548
Epoch 62/500
553/553 - 76s - loss: 0.5404 - accuracy: 0.7311 - val_loss: 0.6231 - val_accuracy: 0.6538
Epoch 63/500
553/553 - 76s - loss: 0.5378 - accuracy: 0.7347 - val_loss: 0.6231 - val_accuracy: 0.6575
Epoch 64/500
553/553 - 76s - loss: 0.5368 - accuracy: 0.7359 - val_loss: 0.6229 - val_accuracy: 0.6593
Epoch 65/500
553/553 - 76s - loss: 0.5228 - accuracy: 0.7440 - val_loss: 0.6229 - val_accuracy: 0.6607
Epoch 66/500
553/553 - 76s - loss: 0.5185 - accuracy: 0.7484 - val_loss: 0.6248 - val_accuracy: 0.6589
Epoch 67/500
553/553 - 76s - loss: 0.5095 - accuracy: 0.7521 - val_loss: 0.6244 - val_accuracy: 0.6598
Epoch 68/500
553/553 - 77s - loss: 0.5119 - accuracy: 0.7537 - val_loss: 0.6241 - val_accuracy: 0.6644
Epoch 69/500
553/553 - 76s - loss: 0.5044 - accuracy: 0.7550 - val_loss: 0.6251 - val_accuracy: 0.6648
Epoch 70/500
553/553 - 76s - loss: 0.4921 - accuracy: 0.7650 - val_loss: 0.6264 - val_accuracy: 0.6662
Epoch 71/500
553/553 - 76s - loss: 0.5013 - accuracy: 0.7623 - val_loss: 0.6264 - val_accuracy: 0.6667
Epoch 72/500
553/553 - 76s - loss: 0.4912 - accuracy: 0.7628 - val_loss: 0.6289 - val_accuracy: 0.6644
Epoch 73/500
553/553 - 76s - loss: 0.4837 - accuracy: 0.7666 - val_loss: 0.6290 - val_accuracy: 0.6639
Epoch 74/500
553/553 - 76s - loss: 0.4858 - accuracy: 0.7689 - val_loss: 0.6295 - val_accuracy: 0.6653
Epoch 75/500
553/553 - 76s - loss: 0.4815 - accuracy: 0.7725 - val_loss: 0.6303 - val_accuracy: 0.6658
Epoch 76/500
553/553 - 76s - loss: 0.4716 - accuracy: 0.7801 - val_loss: 0.6314 - val_accuracy: 0.6662
Epoch 77/500
553/553 - 77s - loss: 0.4698 - accuracy: 0.7767 - val_loss: 0.6334 - val_accuracy: 0.6671
Epoch 78/500
553/553 - 77s - loss: 0.4589 - accuracy: 0.7867 - val_loss: 0.6330 - val_accuracy: 0.6671
Epoch 79/500
553/553 - 77s - loss: 0.4610 - accuracy: 0.7842 - val_loss: 0.6339 - val_accuracy: 0.6667
Epoch 80/500
553/553 - 77s - loss: 0.4576 - accuracy: 0.7874 - val_loss: 0.6348 - val_accuracy: 0.6662
Epoch 81/500
553/553 - 76s - loss: 0.4566 - accuracy: 0.7863 - val_loss: 0.6367 - val_accuracy: 0.6653
Epoch 82/500
553/553 - 76s - loss: 0.4477 - accuracy: 0.7946 - val_loss: 0.6382 - val_accuracy: 0.6653
Epoch 83/500
553/553 - 77s - loss: 0.4474 - accuracy: 0.7943 - val_loss: 0.6385 - val_accuracy: 0.6676
Epoch 84/500
553/553 - 76s - loss: 0.4339 - accuracy: 0.7955 - val_loss: 0.6405 - val_accuracy: 0.6680
Epoch 85/500
553/553 - 76s - loss: 0.4319 - accuracy: 0.7996 - val_loss: 0.6416 - val_accuracy: 0.6717
Epoch 86/500
553/553 - 76s - loss: 0.4296 - accuracy: 0.8018 - val_loss: 0.6428 - val_accuracy: 0.6699
Epoch 87/500
553/553 - 76s - loss: 0.4362 - accuracy: 0.7993 - val_loss: 0.6451 - val_accuracy: 0.6708
Epoch 88/500
553/553 - 76s - loss: 0.4273 - accuracy: 0.8021 - val_loss: 0.6457 - val_accuracy: 0.6708
Epoch 89/500
553/553 - 76s - loss: 0.4176 - accuracy: 0.8095 - val_loss: 0.6473 - val_accuracy: 0.6722
Epoch 90/500
553/553 - 76s - loss: 0.4235 - accuracy: 0.8068 - val_loss: 0.6490 - val_accuracy: 0.6699
Epoch 91/500
553/553 - 76s - loss: 0.4203 - accuracy: 0.8083 - val_loss: 0.6515 - val_accuracy: 0.6731
Epoch 92/500
553/553 - 76s - loss: 0.4130 - accuracy: 0.8135 - val_loss: 0.6512 - val_accuracy: 0.6717
Epoch 93/500
553/553 - 76s - loss: 0.4097 - accuracy: 0.8160 - val_loss: 0.6522 - val_accuracy: 0.6722
Epoch 94/500
553/553 - 76s - loss: 0.4077 - accuracy: 0.8176 - val_loss: 0.6551 - val_accuracy: 0.6712
Epoch 95/500
553/553 - 76s - loss: 0.4032 - accuracy: 0.8188 - val_loss: 0.6551 - val_accuracy: 0.6712
Epoch 96/500
553/553 - 76s - loss: 0.3903 - accuracy: 0.8243 - val_loss: 0.6570 - val_accuracy: 0.6745
Epoch 97/500
553/553 - 76s - loss: 0.3934 - accuracy: 0.8233 - val_loss: 0.6589 - val_accuracy: 0.6722
Epoch 98/500
553/553 - 76s - loss: 0.3951 - accuracy: 0.8243 - val_loss: 0.6607 - val_accuracy: 0.6717
Epoch 99/500
553/553 - 76s - loss: 0.3898 - accuracy: 0.8259 - val_loss: 0.6623 - val_accuracy: 0.6726
Epoch 100/500
553/553 - 77s - loss: 0.3872 - accuracy: 0.8275 - val_loss: 0.6643 - val_accuracy: 0.6726
Epoch 101/500
553/553 - 76s - loss: 0.3746 - accuracy: 0.8343 - val_loss: 0.6661 - val_accuracy: 0.6735
Epoch 102/500
553/553 - 76s - loss: 0.3812 - accuracy: 0.8295 - val_loss: 0.6655 - val_accuracy: 0.6772
Epoch 103/500
553/553 - 76s - loss: 0.3852 - accuracy: 0.8290 - val_loss: 0.6675 - val_accuracy: 0.6740
Epoch 104/500
553/553 - 76s - loss: 0.3700 - accuracy: 0.8345 - val_loss: 0.6692 - val_accuracy: 0.6754
Epoch 105/500
553/553 - 76s - loss: 0.3663 - accuracy: 0.8346 - val_loss: 0.6708 - val_accuracy: 0.6758
Epoch 106/500
553/553 - 76s - loss: 0.3680 - accuracy: 0.8414 - val_loss: 0.6711 - val_accuracy: 0.6740
Epoch 107/500
553/553 - 76s - loss: 0.3585 - accuracy: 0.8397 - val_loss: 0.6733 - val_accuracy: 0.6763
Epoch 108/500
553/553 - 76s - loss: 0.3602 - accuracy: 0.8422 - val_loss: 0.6752 - val_accuracy: 0.6754
Epoch 109/500
553/553 - 76s - loss: 0.3572 - accuracy: 0.8439 - val_loss: 0.6770 - val_accuracy: 0.6722
Epoch 110/500
553/553 - 76s - loss: 0.3576 - accuracy: 0.8426 - val_loss: 0.6770 - val_accuracy: 0.6781
Epoch 111/500
553/553 - 76s - loss: 0.3502 - accuracy: 0.8460 - val_loss: 0.6811 - val_accuracy: 0.6745
Epoch 112/500
553/553 - 76s - loss: 0.3473 - accuracy: 0.8494 - val_loss: 0.6821 - val_accuracy: 0.6772
Epoch 113/500
553/553 - 76s - loss: 0.3487 - accuracy: 0.8507 - val_loss: 0.6817 - val_accuracy: 0.6763
Epoch 114/500
553/553 - 76s - loss: 0.3472 - accuracy: 0.8502 - val_loss: 0.6849 - val_accuracy: 0.6781
Epoch 115/500
553/553 - 76s - loss: 0.3427 - accuracy: 0.8474 - val_loss: 0.6874 - val_accuracy: 0.6749
Epoch 116/500
553/553 - 76s - loss: 0.3425 - accuracy: 0.8512 - val_loss: 0.6879 - val_accuracy: 0.6758
Epoch 117/500
553/553 - 76s - loss: 0.3380 - accuracy: 0.8522 - val_loss: 0.6909 - val_accuracy: 0.6754
Epoch 118/500
553/553 - 75s - loss: 0.3313 - accuracy: 0.8570 - val_loss: 0.6957 - val_accuracy: 0.6726
Epoch 119/500
553/553 - 75s - loss: 0.3275 - accuracy: 0.8560 - val_loss: 0.6961 - val_accuracy: 0.6745
Epoch 120/500
553/553 - 75s - loss: 0.3260 - accuracy: 0.8594 - val_loss: 0.6943 - val_accuracy: 0.6767
Epoch 121/500
553/553 - 75s - loss: 0.3298 - accuracy: 0.8574 - val_loss: 0.6980 - val_accuracy: 0.6767
Epoch 122/500
553/553 - 75s - loss: 0.3165 - accuracy: 0.8620 - val_loss: 0.7006 - val_accuracy: 0.6735
Epoch 123/500
553/553 - 76s - loss: 0.3161 - accuracy: 0.8629 - val_loss: 0.7013 - val_accuracy: 0.6749
Epoch 124/500
553/553 - 76s - loss: 0.3119 - accuracy: 0.8657 - val_loss: 0.7039 - val_accuracy: 0.6758
Epoch 125/500
553/553 - 76s - loss: 0.3163 - accuracy: 0.8642 - val_loss: 0.7058 - val_accuracy: 0.6745
Epoch 126/500
553/553 - 76s - loss: 0.3167 - accuracy: 0.8626 - val_loss: 0.7087 - val_accuracy: 0.6754
Epoch 127/500
553/553 - 76s - loss: 0.3092 - accuracy: 0.8668 - val_loss: 0.7095 - val_accuracy: 0.6772
Epoch 128/500
553/553 - 76s - loss: 0.3094 - accuracy: 0.8650 - val_loss: 0.7110 - val_accuracy: 0.6758
Epoch 129/500
553/553 - 76s - loss: 0.3068 - accuracy: 0.8691 - val_loss: 0.7140 - val_accuracy: 0.6772
Epoch 130/500
553/553 - 76s - loss: 0.3003 - accuracy: 0.8714 - val_loss: 0.7170 - val_accuracy: 0.6786
Epoch 131/500
553/553 - 76s - loss: 0.3013 - accuracy: 0.8710 - val_loss: 0.7176 - val_accuracy: 0.6777
Epoch 132/500
553/553 - 76s - loss: 0.2948 - accuracy: 0.8735 - val_loss: 0.7209 - val_accuracy: 0.6786
Epoch 133/500
553/553 - 76s - loss: 0.2899 - accuracy: 0.8756 - val_loss: 0.7233 - val_accuracy: 0.6758
Epoch 134/500
553/553 - 76s - loss: 0.2928 - accuracy: 0.8765 - val_loss: 0.7238 - val_accuracy: 0.6767
Epoch 135/500
553/553 - 76s - loss: 0.2987 - accuracy: 0.8728 - val_loss: 0.7256 - val_accuracy: 0.6786
Epoch 136/500
553/553 - 76s - loss: 0.2913 - accuracy: 0.8725 - val_loss: 0.7272 - val_accuracy: 0.6795
Epoch 137/500
553/553 - 76s - loss: 0.2876 - accuracy: 0.8765 - val_loss: 0.7299 - val_accuracy: 0.6767
Epoch 138/500
553/553 - 76s - loss: 0.2908 - accuracy: 0.8744 - val_loss: 0.7308 - val_accuracy: 0.6781
Epoch 139/500
553/553 - 76s - loss: 0.2824 - accuracy: 0.8780 - val_loss: 0.7323 - val_accuracy: 0.6790
Epoch 140/500
553/553 - 76s - loss: 0.2751 - accuracy: 0.8834 - val_loss: 0.7344 - val_accuracy: 0.6786
Epoch 141/500
553/553 - 76s - loss: 0.2749 - accuracy: 0.8825 - val_loss: 0.7367 - val_accuracy: 0.6777
Epoch 142/500
553/553 - 76s - loss: 0.2786 - accuracy: 0.8839 - val_loss: 0.7386 - val_accuracy: 0.6758
Epoch 143/500
553/553 - 76s - loss: 0.2727 - accuracy: 0.8852 - val_loss: 0.7411 - val_accuracy: 0.6777
Epoch 144/500
553/553 - 76s - loss: 0.2754 - accuracy: 0.8838 - val_loss: 0.7436 - val_accuracy: 0.6790
Epoch 145/500
553/553 - 76s - loss: 0.2645 - accuracy: 0.8908 - val_loss: 0.7444 - val_accuracy: 0.6772
Epoch 146/500
553/553 - 76s - loss: 0.2671 - accuracy: 0.8884 - val_loss: 0.7466 - val_accuracy: 0.6763
Epoch 147/500
553/553 - 76s - loss: 0.2617 - accuracy: 0.8907 - val_loss: 0.7484 - val_accuracy: 0.6772
Epoch 148/500
553/553 - 76s - loss: 0.2640 - accuracy: 0.8891 - val_loss: 0.7516 - val_accuracy: 0.6777
Epoch 149/500
553/553 - 76s - loss: 0.2571 - accuracy: 0.8933 - val_loss: 0.7496 - val_accuracy: 0.6786
Epoch 150/500
553/553 - 76s - loss: 0.2637 - accuracy: 0.8907 - val_loss: 0.7551 - val_accuracy: 0.6754
Epoch 151/500
553/553 - 76s - loss: 0.2566 - accuracy: 0.8899 - val_loss: 0.7569 - val_accuracy: 0.6772
Epoch 152/500
553/553 - 76s - loss: 0.2557 - accuracy: 0.8933 - val_loss: 0.7571 - val_accuracy: 0.6786
Epoch 153/500
553/553 - 76s - loss: 0.2618 - accuracy: 0.8904 - val_loss: 0.7598 - val_accuracy: 0.6763
Epoch 154/500
553/553 - 76s - loss: 0.2540 - accuracy: 0.8937 - val_loss: 0.7619 - val_accuracy: 0.6767
Epoch 155/500
553/553 - 76s - loss: 0.2478 - accuracy: 0.8955 - val_loss: 0.7624 - val_accuracy: 0.6763
Epoch 156/500
553/553 - 76s - loss: 0.2497 - accuracy: 0.8949 - val_loss: 0.7649 - val_accuracy: 0.6786
========================================
save_weights
h5_weights/MSC.po/embedding_cnn_one_branch.h5
========================================

end time >>> Mon Oct  4 03:07:30 2021

end time >>> Mon Oct  4 03:07:30 2021

end time >>> Mon Oct  4 03:07:30 2021

end time >>> Mon Oct  4 03:07:30 2021

end time >>> Mon Oct  4 03:07:30 2021












args.model = embedding_cnn_one_branch
time used = 11955.166808605194


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 03:07:31 2021

begin time >>> Mon Oct  4 03:07:31 2021

begin time >>> Mon Oct  4 03:07:31 2021

begin time >>> Mon Oct  4 03:07:31 2021

begin time >>> Mon Oct  4 03:07:31 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = embedding_cnn_two_branch
args.type = train
args.name = MSC.po
args.length = 10001
===========================


-> h5_weights/MSC.po folder already exist. pass.
-> result/MSC.po/onehot_cnn_one_branch folder already exist. pass.
-> result/MSC.po/onehot_cnn_two_branch folder already exist. pass.
-> result/MSC.po/onehot_embedding_dense folder already exist. pass.
-> result/MSC.po/onehot_dense folder already exist. pass.
-> result/MSC.po/onehot_resnet18 folder already exist. pass.
-> result/MSC.po/onehot_resnet34 folder already exist. pass.
-> result/MSC.po/embedding_cnn_one_branch folder already exist. pass.
-> result/MSC.po/embedding_cnn_two_branch folder already exist. pass.
-> result/MSC.po/embedding_dense folder already exist. pass.
-> result/MSC.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/MSC.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
MSC.po
########################################

########################################
model_name
embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
553/553 - 77s - loss: 0.8807 - accuracy: 0.5005 - val_loss: 0.7041 - val_accuracy: 0.5137
Epoch 2/500
553/553 - 77s - loss: 0.8621 - accuracy: 0.5037 - val_loss: 0.7012 - val_accuracy: 0.5201
Epoch 3/500
553/553 - 77s - loss: 0.8575 - accuracy: 0.5037 - val_loss: 0.6969 - val_accuracy: 0.5316
Epoch 4/500
553/553 - 77s - loss: 0.8350 - accuracy: 0.5166 - val_loss: 0.6927 - val_accuracy: 0.5403
Epoch 5/500
553/553 - 77s - loss: 0.8261 - accuracy: 0.5242 - val_loss: 0.6901 - val_accuracy: 0.5485
Epoch 6/500
553/553 - 77s - loss: 0.8208 - accuracy: 0.5257 - val_loss: 0.6868 - val_accuracy: 0.5499
Epoch 7/500
553/553 - 77s - loss: 0.8006 - accuracy: 0.5366 - val_loss: 0.6837 - val_accuracy: 0.5577
Epoch 8/500
553/553 - 76s - loss: 0.8012 - accuracy: 0.5299 - val_loss: 0.6812 - val_accuracy: 0.5559
Epoch 9/500
553/553 - 76s - loss: 0.7900 - accuracy: 0.5383 - val_loss: 0.6781 - val_accuracy: 0.5618
Epoch 10/500
553/553 - 76s - loss: 0.7943 - accuracy: 0.5411 - val_loss: 0.6755 - val_accuracy: 0.5673
Epoch 11/500
553/553 - 76s - loss: 0.7809 - accuracy: 0.5460 - val_loss: 0.6732 - val_accuracy: 0.5705
Epoch 12/500
553/553 - 76s - loss: 0.7680 - accuracy: 0.5591 - val_loss: 0.6707 - val_accuracy: 0.5728
Epoch 13/500
553/553 - 76s - loss: 0.7759 - accuracy: 0.5485 - val_loss: 0.6688 - val_accuracy: 0.5760
Epoch 14/500
553/553 - 76s - loss: 0.7696 - accuracy: 0.5530 - val_loss: 0.6663 - val_accuracy: 0.5852
Epoch 15/500
553/553 - 76s - loss: 0.7598 - accuracy: 0.5634 - val_loss: 0.6643 - val_accuracy: 0.5943
Epoch 16/500
553/553 - 76s - loss: 0.7548 - accuracy: 0.5634 - val_loss: 0.6621 - val_accuracy: 0.6007
Epoch 17/500
553/553 - 76s - loss: 0.7533 - accuracy: 0.5669 - val_loss: 0.6600 - val_accuracy: 0.6058
Epoch 18/500
553/553 - 77s - loss: 0.7453 - accuracy: 0.5733 - val_loss: 0.6575 - val_accuracy: 0.6117
Epoch 19/500
553/553 - 76s - loss: 0.7296 - accuracy: 0.5834 - val_loss: 0.6554 - val_accuracy: 0.6126
Epoch 20/500
553/553 - 76s - loss: 0.7352 - accuracy: 0.5808 - val_loss: 0.6531 - val_accuracy: 0.6177
Epoch 21/500
553/553 - 76s - loss: 0.7211 - accuracy: 0.5887 - val_loss: 0.6510 - val_accuracy: 0.6241
Epoch 22/500
553/553 - 76s - loss: 0.7241 - accuracy: 0.5872 - val_loss: 0.6489 - val_accuracy: 0.6241
Epoch 23/500
553/553 - 76s - loss: 0.7145 - accuracy: 0.5986 - val_loss: 0.6471 - val_accuracy: 0.6268
Epoch 24/500
553/553 - 76s - loss: 0.7067 - accuracy: 0.6045 - val_loss: 0.6449 - val_accuracy: 0.6300
Epoch 25/500
553/553 - 76s - loss: 0.7055 - accuracy: 0.5988 - val_loss: 0.6426 - val_accuracy: 0.6364
Epoch 26/500
553/553 - 76s - loss: 0.7017 - accuracy: 0.6048 - val_loss: 0.6410 - val_accuracy: 0.6342
Epoch 27/500
553/553 - 76s - loss: 0.6911 - accuracy: 0.6131 - val_loss: 0.6388 - val_accuracy: 0.6410
Epoch 28/500
553/553 - 76s - loss: 0.6935 - accuracy: 0.6147 - val_loss: 0.6363 - val_accuracy: 0.6461
Epoch 29/500
553/553 - 76s - loss: 0.6821 - accuracy: 0.6237 - val_loss: 0.6348 - val_accuracy: 0.6484
Epoch 30/500
553/553 - 76s - loss: 0.6751 - accuracy: 0.6290 - val_loss: 0.6332 - val_accuracy: 0.6451
Epoch 31/500
553/553 - 76s - loss: 0.6621 - accuracy: 0.6379 - val_loss: 0.6308 - val_accuracy: 0.6474
Epoch 32/500
553/553 - 76s - loss: 0.6568 - accuracy: 0.6409 - val_loss: 0.6287 - val_accuracy: 0.6520
Epoch 33/500
553/553 - 76s - loss: 0.6514 - accuracy: 0.6479 - val_loss: 0.6272 - val_accuracy: 0.6566
Epoch 34/500
553/553 - 76s - loss: 0.6515 - accuracy: 0.6530 - val_loss: 0.6248 - val_accuracy: 0.6571
Epoch 35/500
553/553 - 76s - loss: 0.6342 - accuracy: 0.6608 - val_loss: 0.6232 - val_accuracy: 0.6557
Epoch 36/500
553/553 - 76s - loss: 0.6295 - accuracy: 0.6623 - val_loss: 0.6216 - val_accuracy: 0.6607
Epoch 37/500
553/553 - 76s - loss: 0.6253 - accuracy: 0.6681 - val_loss: 0.6195 - val_accuracy: 0.6625
Epoch 38/500
553/553 - 76s - loss: 0.6149 - accuracy: 0.6763 - val_loss: 0.6187 - val_accuracy: 0.6607
Epoch 39/500
553/553 - 76s - loss: 0.6168 - accuracy: 0.6756 - val_loss: 0.6171 - val_accuracy: 0.6639
Epoch 40/500
553/553 - 76s - loss: 0.6066 - accuracy: 0.6862 - val_loss: 0.6160 - val_accuracy: 0.6685
Epoch 41/500
553/553 - 76s - loss: 0.6033 - accuracy: 0.6875 - val_loss: 0.6153 - val_accuracy: 0.6694
Epoch 42/500
553/553 - 76s - loss: 0.5977 - accuracy: 0.6942 - val_loss: 0.6140 - val_accuracy: 0.6708
Epoch 43/500
553/553 - 76s - loss: 0.5940 - accuracy: 0.6991 - val_loss: 0.6122 - val_accuracy: 0.6722
Epoch 44/500
553/553 - 76s - loss: 0.5819 - accuracy: 0.7063 - val_loss: 0.6119 - val_accuracy: 0.6749
Epoch 45/500
553/553 - 76s - loss: 0.5772 - accuracy: 0.7034 - val_loss: 0.6117 - val_accuracy: 0.6735
Epoch 46/500
553/553 - 76s - loss: 0.5707 - accuracy: 0.7115 - val_loss: 0.6103 - val_accuracy: 0.6749
Epoch 47/500
553/553 - 76s - loss: 0.5597 - accuracy: 0.7181 - val_loss: 0.6098 - val_accuracy: 0.6763
Epoch 48/500
553/553 - 76s - loss: 0.5531 - accuracy: 0.7256 - val_loss: 0.6102 - val_accuracy: 0.6740
Epoch 49/500
553/553 - 76s - loss: 0.5577 - accuracy: 0.7171 - val_loss: 0.6099 - val_accuracy: 0.6745
Epoch 50/500
553/553 - 76s - loss: 0.5442 - accuracy: 0.7287 - val_loss: 0.6095 - val_accuracy: 0.6754
Epoch 51/500
553/553 - 76s - loss: 0.5335 - accuracy: 0.7375 - val_loss: 0.6096 - val_accuracy: 0.6777
Epoch 52/500
553/553 - 76s - loss: 0.5336 - accuracy: 0.7367 - val_loss: 0.6092 - val_accuracy: 0.6795
Epoch 53/500
553/553 - 76s - loss: 0.5221 - accuracy: 0.7448 - val_loss: 0.6092 - val_accuracy: 0.6777
Epoch 54/500
553/553 - 76s - loss: 0.5146 - accuracy: 0.7492 - val_loss: 0.6091 - val_accuracy: 0.6795
Epoch 55/500
553/553 - 76s - loss: 0.5086 - accuracy: 0.7525 - val_loss: 0.6090 - val_accuracy: 0.6781
Epoch 56/500
553/553 - 76s - loss: 0.5090 - accuracy: 0.7508 - val_loss: 0.6089 - val_accuracy: 0.6809
Epoch 57/500
553/553 - 76s - loss: 0.5017 - accuracy: 0.7596 - val_loss: 0.6097 - val_accuracy: 0.6790
Epoch 58/500
553/553 - 76s - loss: 0.4921 - accuracy: 0.7640 - val_loss: 0.6103 - val_accuracy: 0.6850
Epoch 59/500
553/553 - 76s - loss: 0.4877 - accuracy: 0.7667 - val_loss: 0.6107 - val_accuracy: 0.6877
Epoch 60/500
553/553 - 76s - loss: 0.4792 - accuracy: 0.7713 - val_loss: 0.6119 - val_accuracy: 0.6882
Epoch 61/500
553/553 - 76s - loss: 0.4784 - accuracy: 0.7719 - val_loss: 0.6122 - val_accuracy: 0.6918
Epoch 62/500
553/553 - 75s - loss: 0.4773 - accuracy: 0.7784 - val_loss: 0.6122 - val_accuracy: 0.6891
Epoch 63/500
553/553 - 76s - loss: 0.4643 - accuracy: 0.7818 - val_loss: 0.6139 - val_accuracy: 0.6928
Epoch 64/500
553/553 - 76s - loss: 0.4629 - accuracy: 0.7854 - val_loss: 0.6153 - val_accuracy: 0.6900
Epoch 65/500
553/553 - 76s - loss: 0.4606 - accuracy: 0.7826 - val_loss: 0.6152 - val_accuracy: 0.6923
Epoch 66/500
553/553 - 76s - loss: 0.4504 - accuracy: 0.7908 - val_loss: 0.6165 - val_accuracy: 0.6900
Epoch 67/500
553/553 - 76s - loss: 0.4444 - accuracy: 0.7934 - val_loss: 0.6172 - val_accuracy: 0.6900
Epoch 68/500
553/553 - 76s - loss: 0.4405 - accuracy: 0.7939 - val_loss: 0.6186 - val_accuracy: 0.6905
Epoch 69/500
553/553 - 76s - loss: 0.4341 - accuracy: 0.7952 - val_loss: 0.6199 - val_accuracy: 0.6918
Epoch 70/500
553/553 - 76s - loss: 0.4345 - accuracy: 0.7994 - val_loss: 0.6213 - val_accuracy: 0.6923
Epoch 71/500
553/553 - 75s - loss: 0.4237 - accuracy: 0.8048 - val_loss: 0.6221 - val_accuracy: 0.6914
Epoch 72/500
553/553 - 76s - loss: 0.4178 - accuracy: 0.8102 - val_loss: 0.6241 - val_accuracy: 0.6909
Epoch 73/500
553/553 - 76s - loss: 0.4151 - accuracy: 0.8109 - val_loss: 0.6265 - val_accuracy: 0.6900
Epoch 74/500
553/553 - 76s - loss: 0.4136 - accuracy: 0.8093 - val_loss: 0.6279 - val_accuracy: 0.6914
Epoch 75/500
553/553 - 76s - loss: 0.4066 - accuracy: 0.8138 - val_loss: 0.6288 - val_accuracy: 0.6896
Epoch 76/500
553/553 - 76s - loss: 0.4045 - accuracy: 0.8171 - val_loss: 0.6293 - val_accuracy: 0.6928
Epoch 77/500
553/553 - 76s - loss: 0.3996 - accuracy: 0.8218 - val_loss: 0.6316 - val_accuracy: 0.6896
Epoch 78/500
553/553 - 76s - loss: 0.3806 - accuracy: 0.8321 - val_loss: 0.6335 - val_accuracy: 0.6928
Epoch 79/500
553/553 - 76s - loss: 0.3906 - accuracy: 0.8244 - val_loss: 0.6364 - val_accuracy: 0.6877
Epoch 80/500
553/553 - 76s - loss: 0.3870 - accuracy: 0.8269 - val_loss: 0.6361 - val_accuracy: 0.6905
Epoch 81/500
553/553 - 76s - loss: 0.3764 - accuracy: 0.8340 - val_loss: 0.6376 - val_accuracy: 0.6928
Epoch 82/500
553/553 - 76s - loss: 0.3730 - accuracy: 0.8330 - val_loss: 0.6401 - val_accuracy: 0.6928
Epoch 83/500
553/553 - 76s - loss: 0.3685 - accuracy: 0.8316 - val_loss: 0.6436 - val_accuracy: 0.6896
========================================
save_weights
h5_weights/MSC.po/embedding_cnn_two_branch.h5
========================================

end time >>> Mon Oct  4 04:53:22 2021

end time >>> Mon Oct  4 04:53:22 2021

end time >>> Mon Oct  4 04:53:22 2021

end time >>> Mon Oct  4 04:53:22 2021

end time >>> Mon Oct  4 04:53:22 2021












args.model = embedding_cnn_two_branch
time used = 6350.205717563629


