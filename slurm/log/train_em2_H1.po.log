************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 05:53:57 2021

begin time >>> Sun Oct  3 05:53:57 2021

begin time >>> Sun Oct  3 05:53:57 2021

begin time >>> Sun Oct  3 05:53:57 2021

begin time >>> Sun Oct  3 05:53:57 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_dense
args.type = train
args.name = H1.po
args.length = 10001
===========================


-> h5_weights/H1.po folder already exist. pass.
-> result/H1.po/onehot_cnn_one_branch folder already exist. pass.
-> result/H1.po/onehot_cnn_two_branch folder already exist. pass.
-> result/H1.po/onehot_embedding_dense folder already exist. pass.
-> result/H1.po/onehot_dense folder already exist. pass.
-> result/H1.po/onehot_resnet18 folder already exist. pass.
-> result/H1.po/onehot_resnet34 folder already exist. pass.
-> result/H1.po/embedding_cnn_one_branch folder already exist. pass.
-> result/H1.po/embedding_cnn_two_branch folder already exist. pass.
-> result/H1.po/embedding_dense folder already exist. pass.
-> result/H1.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/H1.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
H1.po
########################################

########################################
model_name
onehot_embedding_dense
########################################

Found 8172 images belonging to 2 classes.
Found 1008 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 20002, 5, 1, 8)    32776     
_________________________________________________________________
flatten (Flatten)            (None, 800080)            0         
_________________________________________________________________
batch_normalization (BatchNo (None, 800080)            3200320   
_________________________________________________________________
dense (Dense)                (None, 512)               409641472 
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_3 (Batch (None, 512)               2048      
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_3 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 413,671,754
Trainable params: 412,067,498
Non-trainable params: 1,604,256
_________________________________________________________________
Epoch 1/500
255/255 - 136s - loss: 0.7588 - accuracy: 0.5514 - val_loss: 0.7553 - val_accuracy: 0.4950
Epoch 2/500
255/255 - 54s - loss: 0.6576 - accuracy: 0.6348 - val_loss: 0.9081 - val_accuracy: 0.5050
Epoch 3/500
255/255 - 49s - loss: 0.5656 - accuracy: 0.7152 - val_loss: 1.0820 - val_accuracy: 0.5363
Epoch 4/500
255/255 - 50s - loss: 0.4374 - accuracy: 0.8080 - val_loss: 1.3852 - val_accuracy: 0.5504
Epoch 5/500
255/255 - 51s - loss: 0.3242 - accuracy: 0.8639 - val_loss: 1.6416 - val_accuracy: 0.5625
Epoch 6/500
255/255 - 51s - loss: 0.2480 - accuracy: 0.9012 - val_loss: 1.8659 - val_accuracy: 0.5776
Epoch 7/500
255/255 - 48s - loss: 0.1828 - accuracy: 0.9278 - val_loss: 2.0799 - val_accuracy: 0.5756
Epoch 8/500
255/255 - 50s - loss: 0.1527 - accuracy: 0.9424 - val_loss: 2.1342 - val_accuracy: 0.5716
Epoch 9/500
255/255 - 50s - loss: 0.1268 - accuracy: 0.9516 - val_loss: 2.1746 - val_accuracy: 0.5877
Epoch 10/500
255/255 - 48s - loss: 0.1039 - accuracy: 0.9609 - val_loss: 2.2966 - val_accuracy: 0.5827
Epoch 11/500
255/255 - 49s - loss: 0.0904 - accuracy: 0.9655 - val_loss: 2.2981 - val_accuracy: 0.5948
Epoch 12/500
255/255 - 49s - loss: 0.0794 - accuracy: 0.9695 - val_loss: 2.3189 - val_accuracy: 0.5978
Epoch 13/500
255/255 - 50s - loss: 0.0712 - accuracy: 0.9741 - val_loss: 2.3337 - val_accuracy: 0.5988
Epoch 14/500
255/255 - 49s - loss: 0.0672 - accuracy: 0.9770 - val_loss: 2.4227 - val_accuracy: 0.5988
Epoch 15/500
255/255 - 48s - loss: 0.0645 - accuracy: 0.9769 - val_loss: 2.4759 - val_accuracy: 0.5978
Epoch 16/500
255/255 - 49s - loss: 0.0579 - accuracy: 0.9801 - val_loss: 2.5236 - val_accuracy: 0.5938
Epoch 17/500
255/255 - 48s - loss: 0.0550 - accuracy: 0.9801 - val_loss: 2.5789 - val_accuracy: 0.5958
Epoch 18/500
255/255 - 47s - loss: 0.0466 - accuracy: 0.9844 - val_loss: 2.5940 - val_accuracy: 0.5958
Epoch 19/500
255/255 - 49s - loss: 0.0568 - accuracy: 0.9816 - val_loss: 2.6124 - val_accuracy: 0.6038
Epoch 20/500
255/255 - 49s - loss: 0.0613 - accuracy: 0.9787 - val_loss: 2.5864 - val_accuracy: 0.6119
Epoch 21/500
255/255 - 47s - loss: 0.0600 - accuracy: 0.9774 - val_loss: 2.5261 - val_accuracy: 0.6038
Epoch 22/500
255/255 - 48s - loss: 0.0524 - accuracy: 0.9839 - val_loss: 2.4605 - val_accuracy: 0.6109
Epoch 23/500
255/255 - 48s - loss: 0.0494 - accuracy: 0.9814 - val_loss: 2.3955 - val_accuracy: 0.6099
Epoch 24/500
255/255 - 49s - loss: 0.0541 - accuracy: 0.9816 - val_loss: 2.4585 - val_accuracy: 0.6058
Epoch 25/500
255/255 - 47s - loss: 0.0464 - accuracy: 0.9848 - val_loss: 2.4817 - val_accuracy: 0.6038
Epoch 26/500
255/255 - 47s - loss: 0.0407 - accuracy: 0.9869 - val_loss: 2.4738 - val_accuracy: 0.6018
Epoch 27/500
255/255 - 47s - loss: 0.0381 - accuracy: 0.9866 - val_loss: 2.5617 - val_accuracy: 0.6089
Epoch 28/500
255/255 - 47s - loss: 0.0412 - accuracy: 0.9843 - val_loss: 2.4827 - val_accuracy: 0.6099
Epoch 29/500
255/255 - 47s - loss: 0.0381 - accuracy: 0.9871 - val_loss: 2.6035 - val_accuracy: 0.6079
Epoch 30/500
255/255 - 48s - loss: 0.0363 - accuracy: 0.9877 - val_loss: 2.5475 - val_accuracy: 0.6069
========================================
save_weights
h5_weights/H1.po/onehot_embedding_dense.h5
========================================

end time >>> Sun Oct  3 06:20:06 2021

end time >>> Sun Oct  3 06:20:06 2021

end time >>> Sun Oct  3 06:20:06 2021

end time >>> Sun Oct  3 06:20:06 2021

end time >>> Sun Oct  3 06:20:06 2021












args.model = onehot_embedding_dense
time used = 1569.099047422409


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 06:20:07 2021

begin time >>> Sun Oct  3 06:20:07 2021

begin time >>> Sun Oct  3 06:20:07 2021

begin time >>> Sun Oct  3 06:20:07 2021

begin time >>> Sun Oct  3 06:20:07 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_one_branch
args.type = train
args.name = H1.po
args.length = 10001
===========================


-> h5_weights/H1.po folder already exist. pass.
-> result/H1.po/onehot_cnn_one_branch folder already exist. pass.
-> result/H1.po/onehot_cnn_two_branch folder already exist. pass.
-> result/H1.po/onehot_embedding_dense folder already exist. pass.
-> result/H1.po/onehot_dense folder already exist. pass.
-> result/H1.po/onehot_resnet18 folder already exist. pass.
-> result/H1.po/onehot_resnet34 folder already exist. pass.
-> result/H1.po/embedding_cnn_one_branch folder already exist. pass.
-> result/H1.po/embedding_cnn_two_branch folder already exist. pass.
-> result/H1.po/embedding_dense folder already exist. pass.
-> result/H1.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/H1.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
H1.po
########################################

########################################
model_name
onehot_embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
256/256 - 36s - loss: 0.8588 - accuracy: 0.5097 - val_loss: 0.6929 - val_accuracy: 0.5178
Epoch 2/500
256/256 - 36s - loss: 0.8728 - accuracy: 0.5014 - val_loss: 0.6956 - val_accuracy: 0.5050
Epoch 3/500
256/256 - 36s - loss: 0.8561 - accuracy: 0.5095 - val_loss: 0.7012 - val_accuracy: 0.5059
Epoch 4/500
256/256 - 35s - loss: 0.8466 - accuracy: 0.5073 - val_loss: 0.7022 - val_accuracy: 0.5178
Epoch 5/500
256/256 - 35s - loss: 0.8396 - accuracy: 0.5150 - val_loss: 0.7012 - val_accuracy: 0.5119
Epoch 6/500
256/256 - 35s - loss: 0.8279 - accuracy: 0.5215 - val_loss: 0.6996 - val_accuracy: 0.5158
Epoch 7/500
256/256 - 35s - loss: 0.8187 - accuracy: 0.5256 - val_loss: 0.6982 - val_accuracy: 0.5208
Epoch 8/500
256/256 - 35s - loss: 0.8093 - accuracy: 0.5294 - val_loss: 0.6972 - val_accuracy: 0.5297
Epoch 9/500
256/256 - 35s - loss: 0.8059 - accuracy: 0.5332 - val_loss: 0.6963 - val_accuracy: 0.5287
Epoch 10/500
256/256 - 35s - loss: 0.8242 - accuracy: 0.5184 - val_loss: 0.6950 - val_accuracy: 0.5257
Epoch 11/500
256/256 - 35s - loss: 0.7916 - accuracy: 0.5424 - val_loss: 0.6943 - val_accuracy: 0.5287
Epoch 12/500
256/256 - 35s - loss: 0.7821 - accuracy: 0.5438 - val_loss: 0.6927 - val_accuracy: 0.5366
Epoch 13/500
256/256 - 35s - loss: 0.7879 - accuracy: 0.5413 - val_loss: 0.6918 - val_accuracy: 0.5366
Epoch 14/500
256/256 - 35s - loss: 0.7783 - accuracy: 0.5554 - val_loss: 0.6909 - val_accuracy: 0.5366
Epoch 15/500
256/256 - 35s - loss: 0.7655 - accuracy: 0.5544 - val_loss: 0.6900 - val_accuracy: 0.5426
Epoch 16/500
256/256 - 35s - loss: 0.7764 - accuracy: 0.5554 - val_loss: 0.6892 - val_accuracy: 0.5436
Epoch 17/500
256/256 - 35s - loss: 0.7704 - accuracy: 0.5509 - val_loss: 0.6882 - val_accuracy: 0.5455
Epoch 18/500
256/256 - 35s - loss: 0.7654 - accuracy: 0.5603 - val_loss: 0.6869 - val_accuracy: 0.5475
Epoch 19/500
256/256 - 35s - loss: 0.7546 - accuracy: 0.5603 - val_loss: 0.6859 - val_accuracy: 0.5505
Epoch 20/500
256/256 - 35s - loss: 0.7518 - accuracy: 0.5729 - val_loss: 0.6853 - val_accuracy: 0.5505
Epoch 21/500
256/256 - 35s - loss: 0.7393 - accuracy: 0.5814 - val_loss: 0.6842 - val_accuracy: 0.5545
Epoch 22/500
256/256 - 35s - loss: 0.7383 - accuracy: 0.5708 - val_loss: 0.6838 - val_accuracy: 0.5614
Epoch 23/500
256/256 - 35s - loss: 0.7400 - accuracy: 0.5803 - val_loss: 0.6828 - val_accuracy: 0.5554
Epoch 24/500
256/256 - 35s - loss: 0.7300 - accuracy: 0.5859 - val_loss: 0.6822 - val_accuracy: 0.5584
Epoch 25/500
256/256 - 35s - loss: 0.7214 - accuracy: 0.5888 - val_loss: 0.6818 - val_accuracy: 0.5584
Epoch 26/500
256/256 - 35s - loss: 0.7168 - accuracy: 0.5932 - val_loss: 0.6810 - val_accuracy: 0.5554
Epoch 27/500
256/256 - 35s - loss: 0.7146 - accuracy: 0.5950 - val_loss: 0.6797 - val_accuracy: 0.5634
Epoch 28/500
256/256 - 35s - loss: 0.7121 - accuracy: 0.5947 - val_loss: 0.6783 - val_accuracy: 0.5653
Epoch 29/500
256/256 - 35s - loss: 0.7053 - accuracy: 0.5982 - val_loss: 0.6773 - val_accuracy: 0.5644
Epoch 30/500
256/256 - 35s - loss: 0.7103 - accuracy: 0.5921 - val_loss: 0.6769 - val_accuracy: 0.5604
Epoch 31/500
256/256 - 36s - loss: 0.6882 - accuracy: 0.6161 - val_loss: 0.6761 - val_accuracy: 0.5683
Epoch 32/500
256/256 - 35s - loss: 0.6926 - accuracy: 0.6122 - val_loss: 0.6754 - val_accuracy: 0.5644
Epoch 33/500
256/256 - 36s - loss: 0.6858 - accuracy: 0.6245 - val_loss: 0.6741 - val_accuracy: 0.5693
Epoch 34/500
256/256 - 35s - loss: 0.6706 - accuracy: 0.6320 - val_loss: 0.6738 - val_accuracy: 0.5693
Epoch 35/500
256/256 - 35s - loss: 0.6576 - accuracy: 0.6387 - val_loss: 0.6728 - val_accuracy: 0.5673
Epoch 36/500
256/256 - 35s - loss: 0.6610 - accuracy: 0.6335 - val_loss: 0.6720 - val_accuracy: 0.5644
Epoch 37/500
256/256 - 35s - loss: 0.6484 - accuracy: 0.6457 - val_loss: 0.6714 - val_accuracy: 0.5703
Epoch 38/500
256/256 - 35s - loss: 0.6479 - accuracy: 0.6463 - val_loss: 0.6711 - val_accuracy: 0.5703
Epoch 39/500
256/256 - 35s - loss: 0.6470 - accuracy: 0.6420 - val_loss: 0.6706 - val_accuracy: 0.5673
Epoch 40/500
256/256 - 35s - loss: 0.6333 - accuracy: 0.6595 - val_loss: 0.6694 - val_accuracy: 0.5703
Epoch 41/500
256/256 - 35s - loss: 0.6322 - accuracy: 0.6616 - val_loss: 0.6693 - val_accuracy: 0.5713
Epoch 42/500
256/256 - 35s - loss: 0.6125 - accuracy: 0.6719 - val_loss: 0.6677 - val_accuracy: 0.5822
Epoch 43/500
256/256 - 35s - loss: 0.6111 - accuracy: 0.6786 - val_loss: 0.6678 - val_accuracy: 0.5822
Epoch 44/500
256/256 - 35s - loss: 0.6076 - accuracy: 0.6827 - val_loss: 0.6677 - val_accuracy: 0.5792
Epoch 45/500
256/256 - 35s - loss: 0.6042 - accuracy: 0.6829 - val_loss: 0.6675 - val_accuracy: 0.5851
Epoch 46/500
256/256 - 35s - loss: 0.5973 - accuracy: 0.6873 - val_loss: 0.6668 - val_accuracy: 0.5842
Epoch 47/500
256/256 - 35s - loss: 0.5788 - accuracy: 0.6961 - val_loss: 0.6662 - val_accuracy: 0.5881
Epoch 48/500
256/256 - 35s - loss: 0.5607 - accuracy: 0.7099 - val_loss: 0.6666 - val_accuracy: 0.5891
Epoch 49/500
256/256 - 35s - loss: 0.5694 - accuracy: 0.6978 - val_loss: 0.6661 - val_accuracy: 0.5871
Epoch 50/500
256/256 - 35s - loss: 0.5564 - accuracy: 0.7178 - val_loss: 0.6662 - val_accuracy: 0.5921
Epoch 51/500
256/256 - 35s - loss: 0.5465 - accuracy: 0.7251 - val_loss: 0.6658 - val_accuracy: 0.5970
Epoch 52/500
256/256 - 35s - loss: 0.5341 - accuracy: 0.7330 - val_loss: 0.6656 - val_accuracy: 0.5980
Epoch 53/500
256/256 - 35s - loss: 0.5446 - accuracy: 0.7215 - val_loss: 0.6654 - val_accuracy: 0.5990
Epoch 54/500
256/256 - 35s - loss: 0.5252 - accuracy: 0.7380 - val_loss: 0.6652 - val_accuracy: 0.6010
Epoch 55/500
256/256 - 35s - loss: 0.5144 - accuracy: 0.7430 - val_loss: 0.6663 - val_accuracy: 0.6000
Epoch 56/500
256/256 - 35s - loss: 0.5134 - accuracy: 0.7435 - val_loss: 0.6662 - val_accuracy: 0.6020
Epoch 57/500
256/256 - 35s - loss: 0.4980 - accuracy: 0.7530 - val_loss: 0.6671 - val_accuracy: 0.6069
Epoch 58/500
256/256 - 35s - loss: 0.4933 - accuracy: 0.7587 - val_loss: 0.6671 - val_accuracy: 0.6010
Epoch 59/500
256/256 - 35s - loss: 0.4910 - accuracy: 0.7655 - val_loss: 0.6678 - val_accuracy: 0.6040
Epoch 60/500
256/256 - 35s - loss: 0.4715 - accuracy: 0.7714 - val_loss: 0.6684 - val_accuracy: 0.6040
Epoch 61/500
256/256 - 35s - loss: 0.4661 - accuracy: 0.7732 - val_loss: 0.6699 - val_accuracy: 0.6099
Epoch 62/500
256/256 - 35s - loss: 0.4680 - accuracy: 0.7757 - val_loss: 0.6708 - val_accuracy: 0.6040
Epoch 63/500
256/256 - 35s - loss: 0.4478 - accuracy: 0.7875 - val_loss: 0.6702 - val_accuracy: 0.6030
Epoch 64/500
256/256 - 35s - loss: 0.4379 - accuracy: 0.7911 - val_loss: 0.6716 - val_accuracy: 0.6059
Epoch 65/500
256/256 - 35s - loss: 0.4296 - accuracy: 0.7984 - val_loss: 0.6725 - val_accuracy: 0.6020
Epoch 66/500
256/256 - 35s - loss: 0.4150 - accuracy: 0.8132 - val_loss: 0.6743 - val_accuracy: 0.6050
Epoch 67/500
256/256 - 35s - loss: 0.4163 - accuracy: 0.8070 - val_loss: 0.6753 - val_accuracy: 0.6059
Epoch 68/500
256/256 - 35s - loss: 0.4063 - accuracy: 0.8130 - val_loss: 0.6760 - val_accuracy: 0.6069
Epoch 69/500
256/256 - 35s - loss: 0.3993 - accuracy: 0.8208 - val_loss: 0.6797 - val_accuracy: 0.6099
Epoch 70/500
256/256 - 35s - loss: 0.3836 - accuracy: 0.8285 - val_loss: 0.6814 - val_accuracy: 0.6089
Epoch 71/500
256/256 - 35s - loss: 0.3755 - accuracy: 0.8328 - val_loss: 0.6828 - val_accuracy: 0.6099
Epoch 72/500
256/256 - 35s - loss: 0.3789 - accuracy: 0.8326 - val_loss: 0.6856 - val_accuracy: 0.6079
Epoch 73/500
256/256 - 35s - loss: 0.3680 - accuracy: 0.8332 - val_loss: 0.6885 - val_accuracy: 0.6099
Epoch 74/500
256/256 - 35s - loss: 0.3658 - accuracy: 0.8367 - val_loss: 0.6905 - val_accuracy: 0.6109
Epoch 75/500
256/256 - 35s - loss: 0.3490 - accuracy: 0.8459 - val_loss: 0.6937 - val_accuracy: 0.6129
Epoch 76/500
256/256 - 35s - loss: 0.3283 - accuracy: 0.8583 - val_loss: 0.6968 - val_accuracy: 0.6119
Epoch 77/500
256/256 - 35s - loss: 0.3320 - accuracy: 0.8552 - val_loss: 0.6994 - val_accuracy: 0.6119
Epoch 78/500
256/256 - 35s - loss: 0.3245 - accuracy: 0.8606 - val_loss: 0.7015 - val_accuracy: 0.6119
Epoch 79/500
256/256 - 35s - loss: 0.3105 - accuracy: 0.8682 - val_loss: 0.7048 - val_accuracy: 0.6129
Epoch 80/500
256/256 - 35s - loss: 0.3088 - accuracy: 0.8667 - val_loss: 0.7086 - val_accuracy: 0.6089
Epoch 81/500
256/256 - 35s - loss: 0.2952 - accuracy: 0.8749 - val_loss: 0.7105 - val_accuracy: 0.6129
Epoch 82/500
256/256 - 35s - loss: 0.2908 - accuracy: 0.8776 - val_loss: 0.7143 - val_accuracy: 0.6119
Epoch 83/500
256/256 - 35s - loss: 0.2820 - accuracy: 0.8798 - val_loss: 0.7170 - val_accuracy: 0.6168
Epoch 84/500
256/256 - 35s - loss: 0.2844 - accuracy: 0.8780 - val_loss: 0.7206 - val_accuracy: 0.6168
Epoch 85/500
256/256 - 35s - loss: 0.2784 - accuracy: 0.8815 - val_loss: 0.7239 - val_accuracy: 0.6139
Epoch 86/500
256/256 - 35s - loss: 0.2609 - accuracy: 0.8939 - val_loss: 0.7296 - val_accuracy: 0.6099
Epoch 87/500
256/256 - 35s - loss: 0.2623 - accuracy: 0.8934 - val_loss: 0.7326 - val_accuracy: 0.6129
Epoch 88/500
256/256 - 35s - loss: 0.2442 - accuracy: 0.9036 - val_loss: 0.7347 - val_accuracy: 0.6158
Epoch 89/500
256/256 - 35s - loss: 0.2502 - accuracy: 0.8982 - val_loss: 0.7417 - val_accuracy: 0.6099
Epoch 90/500
256/256 - 35s - loss: 0.2375 - accuracy: 0.9003 - val_loss: 0.7470 - val_accuracy: 0.6129
Epoch 91/500
256/256 - 35s - loss: 0.2291 - accuracy: 0.9108 - val_loss: 0.7506 - val_accuracy: 0.6158
Epoch 92/500
256/256 - 35s - loss: 0.2318 - accuracy: 0.9042 - val_loss: 0.7540 - val_accuracy: 0.6158
Epoch 93/500
256/256 - 35s - loss: 0.2250 - accuracy: 0.9097 - val_loss: 0.7574 - val_accuracy: 0.6178
Epoch 94/500
256/256 - 35s - loss: 0.2132 - accuracy: 0.9160 - val_loss: 0.7616 - val_accuracy: 0.6158
Epoch 95/500
256/256 - 35s - loss: 0.2239 - accuracy: 0.9077 - val_loss: 0.7655 - val_accuracy: 0.6168
Epoch 96/500
256/256 - 35s - loss: 0.2025 - accuracy: 0.9240 - val_loss: 0.7729 - val_accuracy: 0.6139
Epoch 97/500
256/256 - 35s - loss: 0.2001 - accuracy: 0.9211 - val_loss: 0.7768 - val_accuracy: 0.6158
Epoch 98/500
256/256 - 35s - loss: 0.2028 - accuracy: 0.9167 - val_loss: 0.7784 - val_accuracy: 0.6168
Epoch 99/500
256/256 - 35s - loss: 0.1923 - accuracy: 0.9249 - val_loss: 0.7850 - val_accuracy: 0.6149
Epoch 100/500
256/256 - 35s - loss: 0.1867 - accuracy: 0.9266 - val_loss: 0.7895 - val_accuracy: 0.6188
Epoch 101/500
256/256 - 35s - loss: 0.1830 - accuracy: 0.9298 - val_loss: 0.7945 - val_accuracy: 0.6188
Epoch 102/500
256/256 - 35s - loss: 0.1832 - accuracy: 0.9332 - val_loss: 0.7985 - val_accuracy: 0.6178
Epoch 103/500
256/256 - 35s - loss: 0.1646 - accuracy: 0.9393 - val_loss: 0.8046 - val_accuracy: 0.6168
Epoch 104/500
256/256 - 35s - loss: 0.1616 - accuracy: 0.9357 - val_loss: 0.8082 - val_accuracy: 0.6178
Epoch 105/500
256/256 - 35s - loss: 0.1659 - accuracy: 0.9362 - val_loss: 0.8115 - val_accuracy: 0.6149
Epoch 106/500
256/256 - 35s - loss: 0.1657 - accuracy: 0.9360 - val_loss: 0.8182 - val_accuracy: 0.6149
Epoch 107/500
256/256 - 35s - loss: 0.1502 - accuracy: 0.9433 - val_loss: 0.8258 - val_accuracy: 0.6178
Epoch 108/500
256/256 - 35s - loss: 0.1545 - accuracy: 0.9416 - val_loss: 0.8300 - val_accuracy: 0.6168
Epoch 109/500
256/256 - 35s - loss: 0.1440 - accuracy: 0.9462 - val_loss: 0.8357 - val_accuracy: 0.6139
Epoch 110/500
256/256 - 35s - loss: 0.1476 - accuracy: 0.9437 - val_loss: 0.8413 - val_accuracy: 0.6168
Epoch 111/500
256/256 - 35s - loss: 0.1495 - accuracy: 0.9449 - val_loss: 0.8462 - val_accuracy: 0.6178
Epoch 112/500
256/256 - 35s - loss: 0.1322 - accuracy: 0.9519 - val_loss: 0.8471 - val_accuracy: 0.6188
Epoch 113/500
256/256 - 35s - loss: 0.1329 - accuracy: 0.9520 - val_loss: 0.8546 - val_accuracy: 0.6168
Epoch 114/500
256/256 - 35s - loss: 0.1298 - accuracy: 0.9509 - val_loss: 0.8579 - val_accuracy: 0.6208
Epoch 115/500
256/256 - 35s - loss: 0.1281 - accuracy: 0.9508 - val_loss: 0.8630 - val_accuracy: 0.6208
Epoch 116/500
256/256 - 35s - loss: 0.1247 - accuracy: 0.9556 - val_loss: 0.8678 - val_accuracy: 0.6188
Epoch 117/500
256/256 - 35s - loss: 0.1254 - accuracy: 0.9547 - val_loss: 0.8691 - val_accuracy: 0.6198
Epoch 118/500
256/256 - 35s - loss: 0.1239 - accuracy: 0.9552 - val_loss: 0.8759 - val_accuracy: 0.6178
Epoch 119/500
256/256 - 35s - loss: 0.1134 - accuracy: 0.9602 - val_loss: 0.8791 - val_accuracy: 0.6158
Epoch 120/500
256/256 - 35s - loss: 0.1120 - accuracy: 0.9596 - val_loss: 0.8862 - val_accuracy: 0.6208
Epoch 121/500
256/256 - 35s - loss: 0.1071 - accuracy: 0.9616 - val_loss: 0.8897 - val_accuracy: 0.6178
Epoch 122/500
256/256 - 35s - loss: 0.1041 - accuracy: 0.9634 - val_loss: 0.8976 - val_accuracy: 0.6208
Epoch 123/500
256/256 - 35s - loss: 0.1097 - accuracy: 0.9621 - val_loss: 0.8958 - val_accuracy: 0.6208
Epoch 124/500
256/256 - 35s - loss: 0.1034 - accuracy: 0.9622 - val_loss: 0.9038 - val_accuracy: 0.6208
Epoch 125/500
256/256 - 35s - loss: 0.1021 - accuracy: 0.9646 - val_loss: 0.9074 - val_accuracy: 0.6218
Epoch 126/500
256/256 - 35s - loss: 0.1076 - accuracy: 0.9608 - val_loss: 0.9163 - val_accuracy: 0.6208
Epoch 127/500
256/256 - 35s - loss: 0.0945 - accuracy: 0.9676 - val_loss: 0.9192 - val_accuracy: 0.6248
Epoch 128/500
256/256 - 35s - loss: 0.0860 - accuracy: 0.9705 - val_loss: 0.9256 - val_accuracy: 0.6218
Epoch 129/500
256/256 - 35s - loss: 0.0924 - accuracy: 0.9672 - val_loss: 0.9284 - val_accuracy: 0.6228
Epoch 130/500
256/256 - 35s - loss: 0.0871 - accuracy: 0.9687 - val_loss: 0.9329 - val_accuracy: 0.6178
Epoch 131/500
256/256 - 35s - loss: 0.0873 - accuracy: 0.9709 - val_loss: 0.9393 - val_accuracy: 0.6188
Epoch 132/500
256/256 - 35s - loss: 0.0796 - accuracy: 0.9739 - val_loss: 0.9401 - val_accuracy: 0.6198
Epoch 133/500
256/256 - 35s - loss: 0.0908 - accuracy: 0.9670 - val_loss: 0.9495 - val_accuracy: 0.6178
Epoch 134/500
256/256 - 35s - loss: 0.0835 - accuracy: 0.9720 - val_loss: 0.9519 - val_accuracy: 0.6248
Epoch 135/500
256/256 - 35s - loss: 0.0824 - accuracy: 0.9699 - val_loss: 0.9594 - val_accuracy: 0.6218
Epoch 136/500
256/256 - 35s - loss: 0.0711 - accuracy: 0.9771 - val_loss: 0.9620 - val_accuracy: 0.6228
Epoch 137/500
256/256 - 35s - loss: 0.0814 - accuracy: 0.9703 - val_loss: 0.9691 - val_accuracy: 0.6228
Epoch 138/500
256/256 - 35s - loss: 0.0728 - accuracy: 0.9756 - val_loss: 0.9757 - val_accuracy: 0.6178
Epoch 139/500
256/256 - 35s - loss: 0.0754 - accuracy: 0.9742 - val_loss: 0.9803 - val_accuracy: 0.6178
Epoch 140/500
256/256 - 35s - loss: 0.0702 - accuracy: 0.9758 - val_loss: 0.9844 - val_accuracy: 0.6158
Epoch 141/500
256/256 - 35s - loss: 0.0752 - accuracy: 0.9741 - val_loss: 0.9897 - val_accuracy: 0.6228
Epoch 142/500
256/256 - 35s - loss: 0.0762 - accuracy: 0.9721 - val_loss: 0.9885 - val_accuracy: 0.6218
Epoch 143/500
256/256 - 35s - loss: 0.0677 - accuracy: 0.9759 - val_loss: 0.9973 - val_accuracy: 0.6228
Epoch 144/500
256/256 - 35s - loss: 0.0680 - accuracy: 0.9764 - val_loss: 0.9950 - val_accuracy: 0.6238
Epoch 145/500
256/256 - 35s - loss: 0.0651 - accuracy: 0.9786 - val_loss: 1.0014 - val_accuracy: 0.6248
Epoch 146/500
256/256 - 35s - loss: 0.0650 - accuracy: 0.9778 - val_loss: 1.0086 - val_accuracy: 0.6218
Epoch 147/500
256/256 - 35s - loss: 0.0621 - accuracy: 0.9801 - val_loss: 1.0133 - val_accuracy: 0.6218
========================================
save_weights
h5_weights/H1.po/onehot_embedding_cnn_one_branch.h5
========================================

end time >>> Sun Oct  3 07:46:38 2021

end time >>> Sun Oct  3 07:46:38 2021

end time >>> Sun Oct  3 07:46:38 2021

end time >>> Sun Oct  3 07:46:38 2021

end time >>> Sun Oct  3 07:46:38 2021












args.model = onehot_embedding_cnn_one_branch
time used = 5191.344929695129


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Sun Oct  3 07:46:40 2021

begin time >>> Sun Oct  3 07:46:40 2021

begin time >>> Sun Oct  3 07:46:40 2021

begin time >>> Sun Oct  3 07:46:40 2021

begin time >>> Sun Oct  3 07:46:40 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_two_branch
args.type = train
args.name = H1.po
args.length = 10001
===========================


-> h5_weights/H1.po folder already exist. pass.
-> result/H1.po/onehot_cnn_one_branch folder already exist. pass.
-> result/H1.po/onehot_cnn_two_branch folder already exist. pass.
-> result/H1.po/onehot_embedding_dense folder already exist. pass.
-> result/H1.po/onehot_dense folder already exist. pass.
-> result/H1.po/onehot_resnet18 folder already exist. pass.
-> result/H1.po/onehot_resnet34 folder already exist. pass.
-> result/H1.po/embedding_cnn_one_branch folder already exist. pass.
-> result/H1.po/embedding_cnn_two_branch folder already exist. pass.
-> result/H1.po/embedding_dense folder already exist. pass.
-> result/H1.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/H1.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
H1.po
########################################

########################################
model_name
onehot_embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
256/256 - 36s - loss: 0.9115 - accuracy: 0.4994 - val_loss: 0.7252 - val_accuracy: 0.4822
Epoch 2/500
256/256 - 35s - loss: 0.8850 - accuracy: 0.4970 - val_loss: 0.7421 - val_accuracy: 0.4822
Epoch 3/500
256/256 - 35s - loss: 0.8636 - accuracy: 0.5083 - val_loss: 0.7038 - val_accuracy: 0.5139
Epoch 4/500
256/256 - 35s - loss: 0.8438 - accuracy: 0.5184 - val_loss: 0.6983 - val_accuracy: 0.5238
Epoch 5/500
256/256 - 35s - loss: 0.8400 - accuracy: 0.5200 - val_loss: 0.6976 - val_accuracy: 0.5366
Epoch 6/500
256/256 - 35s - loss: 0.8434 - accuracy: 0.5125 - val_loss: 0.6958 - val_accuracy: 0.5317
Epoch 7/500
256/256 - 35s - loss: 0.8353 - accuracy: 0.5200 - val_loss: 0.6960 - val_accuracy: 0.5406
Epoch 8/500
256/256 - 35s - loss: 0.8212 - accuracy: 0.5254 - val_loss: 0.6952 - val_accuracy: 0.5475
Epoch 9/500
256/256 - 35s - loss: 0.8076 - accuracy: 0.5321 - val_loss: 0.6938 - val_accuracy: 0.5455
Epoch 10/500
256/256 - 35s - loss: 0.8030 - accuracy: 0.5335 - val_loss: 0.6931 - val_accuracy: 0.5515
Epoch 11/500
256/256 - 35s - loss: 0.8068 - accuracy: 0.5280 - val_loss: 0.6921 - val_accuracy: 0.5554
Epoch 12/500
256/256 - 35s - loss: 0.7920 - accuracy: 0.5408 - val_loss: 0.6913 - val_accuracy: 0.5485
Epoch 13/500
256/256 - 35s - loss: 0.7815 - accuracy: 0.5549 - val_loss: 0.6899 - val_accuracy: 0.5545
Epoch 14/500
256/256 - 35s - loss: 0.7825 - accuracy: 0.5478 - val_loss: 0.6887 - val_accuracy: 0.5515
Epoch 15/500
256/256 - 35s - loss: 0.7887 - accuracy: 0.5417 - val_loss: 0.6883 - val_accuracy: 0.5604
Epoch 16/500
256/256 - 35s - loss: 0.7657 - accuracy: 0.5602 - val_loss: 0.6875 - val_accuracy: 0.5614
Epoch 17/500
256/256 - 35s - loss: 0.7556 - accuracy: 0.5637 - val_loss: 0.6864 - val_accuracy: 0.5594
Epoch 18/500
256/256 - 35s - loss: 0.7600 - accuracy: 0.5609 - val_loss: 0.6859 - val_accuracy: 0.5574
Epoch 19/500
256/256 - 35s - loss: 0.7562 - accuracy: 0.5690 - val_loss: 0.6861 - val_accuracy: 0.5644
Epoch 20/500
256/256 - 35s - loss: 0.7476 - accuracy: 0.5796 - val_loss: 0.6852 - val_accuracy: 0.5604
Epoch 21/500
256/256 - 35s - loss: 0.7397 - accuracy: 0.5822 - val_loss: 0.6851 - val_accuracy: 0.5594
Epoch 22/500
256/256 - 35s - loss: 0.7422 - accuracy: 0.5762 - val_loss: 0.6827 - val_accuracy: 0.5653
Epoch 23/500
256/256 - 35s - loss: 0.7271 - accuracy: 0.5845 - val_loss: 0.6826 - val_accuracy: 0.5644
Epoch 24/500
256/256 - 35s - loss: 0.7177 - accuracy: 0.5895 - val_loss: 0.6810 - val_accuracy: 0.5604
Epoch 25/500
256/256 - 35s - loss: 0.7185 - accuracy: 0.5928 - val_loss: 0.6801 - val_accuracy: 0.5653
Epoch 26/500
256/256 - 35s - loss: 0.7098 - accuracy: 0.6052 - val_loss: 0.6795 - val_accuracy: 0.5624
Epoch 27/500
256/256 - 35s - loss: 0.7075 - accuracy: 0.6003 - val_loss: 0.6790 - val_accuracy: 0.5733
Epoch 28/500
256/256 - 35s - loss: 0.7076 - accuracy: 0.6002 - val_loss: 0.6788 - val_accuracy: 0.5713
Epoch 29/500
256/256 - 35s - loss: 0.6967 - accuracy: 0.6127 - val_loss: 0.6799 - val_accuracy: 0.5673
Epoch 30/500
256/256 - 35s - loss: 0.6834 - accuracy: 0.6275 - val_loss: 0.6790 - val_accuracy: 0.5683
Epoch 31/500
256/256 - 35s - loss: 0.6864 - accuracy: 0.6220 - val_loss: 0.6768 - val_accuracy: 0.5772
Epoch 32/500
256/256 - 35s - loss: 0.6906 - accuracy: 0.6175 - val_loss: 0.6767 - val_accuracy: 0.5733
Epoch 33/500
256/256 - 35s - loss: 0.6663 - accuracy: 0.6342 - val_loss: 0.6765 - val_accuracy: 0.5782
Epoch 34/500
256/256 - 35s - loss: 0.6544 - accuracy: 0.6434 - val_loss: 0.6746 - val_accuracy: 0.5842
Epoch 35/500
256/256 - 35s - loss: 0.6504 - accuracy: 0.6468 - val_loss: 0.6754 - val_accuracy: 0.5772
Epoch 36/500
256/256 - 35s - loss: 0.6386 - accuracy: 0.6528 - val_loss: 0.6740 - val_accuracy: 0.5822
Epoch 37/500
256/256 - 35s - loss: 0.6390 - accuracy: 0.6538 - val_loss: 0.6728 - val_accuracy: 0.5812
Epoch 38/500
256/256 - 35s - loss: 0.6409 - accuracy: 0.6592 - val_loss: 0.6716 - val_accuracy: 0.5842
Epoch 39/500
256/256 - 35s - loss: 0.6270 - accuracy: 0.6674 - val_loss: 0.6709 - val_accuracy: 0.5861
Epoch 40/500
256/256 - 35s - loss: 0.6090 - accuracy: 0.6762 - val_loss: 0.6737 - val_accuracy: 0.5752
Epoch 41/500
256/256 - 35s - loss: 0.6008 - accuracy: 0.6834 - val_loss: 0.6710 - val_accuracy: 0.5842
Epoch 42/500
256/256 - 35s - loss: 0.6039 - accuracy: 0.6824 - val_loss: 0.6708 - val_accuracy: 0.5891
Epoch 43/500
256/256 - 35s - loss: 0.5934 - accuracy: 0.6830 - val_loss: 0.6696 - val_accuracy: 0.5901
Epoch 44/500
256/256 - 35s - loss: 0.5826 - accuracy: 0.6976 - val_loss: 0.6707 - val_accuracy: 0.5901
Epoch 45/500
256/256 - 35s - loss: 0.5748 - accuracy: 0.7077 - val_loss: 0.6681 - val_accuracy: 0.5881
Epoch 46/500
256/256 - 35s - loss: 0.5712 - accuracy: 0.7150 - val_loss: 0.6698 - val_accuracy: 0.5842
Epoch 47/500
256/256 - 35s - loss: 0.5680 - accuracy: 0.7077 - val_loss: 0.6691 - val_accuracy: 0.5950
Epoch 48/500
256/256 - 35s - loss: 0.5546 - accuracy: 0.7181 - val_loss: 0.6695 - val_accuracy: 0.5941
Epoch 49/500
256/256 - 35s - loss: 0.5354 - accuracy: 0.7309 - val_loss: 0.6695 - val_accuracy: 0.5970
Epoch 50/500
256/256 - 35s - loss: 0.5327 - accuracy: 0.7293 - val_loss: 0.6704 - val_accuracy: 0.5921
Epoch 51/500
256/256 - 35s - loss: 0.5271 - accuracy: 0.7376 - val_loss: 0.6706 - val_accuracy: 0.5950
Epoch 52/500
256/256 - 35s - loss: 0.5174 - accuracy: 0.7453 - val_loss: 0.6710 - val_accuracy: 0.5970
Epoch 53/500
256/256 - 35s - loss: 0.5067 - accuracy: 0.7530 - val_loss: 0.6718 - val_accuracy: 0.5950
Epoch 54/500
256/256 - 35s - loss: 0.4931 - accuracy: 0.7604 - val_loss: 0.6735 - val_accuracy: 0.5960
Epoch 55/500
256/256 - 35s - loss: 0.4926 - accuracy: 0.7623 - val_loss: 0.6707 - val_accuracy: 0.6079
Epoch 56/500
256/256 - 35s - loss: 0.4799 - accuracy: 0.7759 - val_loss: 0.6745 - val_accuracy: 0.6010
Epoch 57/500
256/256 - 35s - loss: 0.4674 - accuracy: 0.7793 - val_loss: 0.6735 - val_accuracy: 0.6050
Epoch 58/500
256/256 - 35s - loss: 0.4519 - accuracy: 0.7884 - val_loss: 0.6743 - val_accuracy: 0.6050
Epoch 59/500
256/256 - 35s - loss: 0.4507 - accuracy: 0.7852 - val_loss: 0.6769 - val_accuracy: 0.6040
Epoch 60/500
256/256 - 35s - loss: 0.4439 - accuracy: 0.7916 - val_loss: 0.6797 - val_accuracy: 0.6079
Epoch 61/500
256/256 - 35s - loss: 0.4315 - accuracy: 0.7982 - val_loss: 0.6796 - val_accuracy: 0.6059
Epoch 62/500
256/256 - 35s - loss: 0.4128 - accuracy: 0.8086 - val_loss: 0.6825 - val_accuracy: 0.6059
Epoch 63/500
256/256 - 35s - loss: 0.4065 - accuracy: 0.8128 - val_loss: 0.6830 - val_accuracy: 0.6069
Epoch 64/500
256/256 - 35s - loss: 0.4008 - accuracy: 0.8191 - val_loss: 0.6874 - val_accuracy: 0.6040
Epoch 65/500
256/256 - 35s - loss: 0.4001 - accuracy: 0.8152 - val_loss: 0.6880 - val_accuracy: 0.6040
Epoch 66/500
256/256 - 35s - loss: 0.3927 - accuracy: 0.8202 - val_loss: 0.6905 - val_accuracy: 0.6069
Epoch 67/500
256/256 - 35s - loss: 0.3781 - accuracy: 0.8317 - val_loss: 0.6929 - val_accuracy: 0.6050
Epoch 68/500
256/256 - 35s - loss: 0.3696 - accuracy: 0.8387 - val_loss: 0.6953 - val_accuracy: 0.6089
Epoch 69/500
256/256 - 35s - loss: 0.3602 - accuracy: 0.8421 - val_loss: 0.6975 - val_accuracy: 0.6109
Epoch 70/500
256/256 - 35s - loss: 0.3435 - accuracy: 0.8549 - val_loss: 0.7026 - val_accuracy: 0.6089
Epoch 71/500
256/256 - 35s - loss: 0.3420 - accuracy: 0.8506 - val_loss: 0.7019 - val_accuracy: 0.6059
Epoch 72/500
256/256 - 35s - loss: 0.3337 - accuracy: 0.8538 - val_loss: 0.7087 - val_accuracy: 0.6050
Epoch 73/500
256/256 - 35s - loss: 0.3266 - accuracy: 0.8593 - val_loss: 0.7064 - val_accuracy: 0.6099
Epoch 74/500
256/256 - 35s - loss: 0.3199 - accuracy: 0.8646 - val_loss: 0.7123 - val_accuracy: 0.6030
Epoch 75/500
256/256 - 35s - loss: 0.3113 - accuracy: 0.8719 - val_loss: 0.7148 - val_accuracy: 0.6079
Epoch 76/500
256/256 - 35s - loss: 0.2939 - accuracy: 0.8760 - val_loss: 0.7183 - val_accuracy: 0.6059
Epoch 77/500
256/256 - 35s - loss: 0.2838 - accuracy: 0.8818 - val_loss: 0.7232 - val_accuracy: 0.6099
Epoch 78/500
256/256 - 35s - loss: 0.2825 - accuracy: 0.8832 - val_loss: 0.7263 - val_accuracy: 0.6099
Epoch 79/500
256/256 - 35s - loss: 0.2776 - accuracy: 0.8832 - val_loss: 0.7307 - val_accuracy: 0.6099
Epoch 80/500
256/256 - 35s - loss: 0.2615 - accuracy: 0.8888 - val_loss: 0.7349 - val_accuracy: 0.6139
Epoch 81/500
256/256 - 35s - loss: 0.2646 - accuracy: 0.8910 - val_loss: 0.7386 - val_accuracy: 0.6139
Epoch 82/500
256/256 - 35s - loss: 0.2539 - accuracy: 0.8959 - val_loss: 0.7446 - val_accuracy: 0.6168
Epoch 83/500
256/256 - 35s - loss: 0.2454 - accuracy: 0.9011 - val_loss: 0.7463 - val_accuracy: 0.6099
Epoch 84/500
256/256 - 35s - loss: 0.2414 - accuracy: 0.9005 - val_loss: 0.7493 - val_accuracy: 0.6089
Epoch 85/500
256/256 - 35s - loss: 0.2285 - accuracy: 0.9072 - val_loss: 0.7536 - val_accuracy: 0.6109
Epoch 86/500
256/256 - 35s - loss: 0.2301 - accuracy: 0.9080 - val_loss: 0.7600 - val_accuracy: 0.6149
Epoch 87/500
256/256 - 35s - loss: 0.2227 - accuracy: 0.9126 - val_loss: 0.7673 - val_accuracy: 0.6129
Epoch 88/500
256/256 - 35s - loss: 0.2093 - accuracy: 0.9163 - val_loss: 0.7759 - val_accuracy: 0.6139
Epoch 89/500
256/256 - 35s - loss: 0.2067 - accuracy: 0.9192 - val_loss: 0.7730 - val_accuracy: 0.6158
Epoch 90/500
256/256 - 35s - loss: 0.2042 - accuracy: 0.9198 - val_loss: 0.7769 - val_accuracy: 0.6168
Epoch 91/500
256/256 - 35s - loss: 0.2055 - accuracy: 0.9193 - val_loss: 0.7847 - val_accuracy: 0.6178
Epoch 92/500
256/256 - 35s - loss: 0.1919 - accuracy: 0.9264 - val_loss: 0.7887 - val_accuracy: 0.6188
Epoch 93/500
256/256 - 35s - loss: 0.1915 - accuracy: 0.9266 - val_loss: 0.7985 - val_accuracy: 0.6149
Epoch 94/500
256/256 - 35s - loss: 0.1814 - accuracy: 0.9293 - val_loss: 0.8040 - val_accuracy: 0.6139
Epoch 95/500
256/256 - 35s - loss: 0.1815 - accuracy: 0.9294 - val_loss: 0.8034 - val_accuracy: 0.6139
Epoch 96/500
256/256 - 35s - loss: 0.1739 - accuracy: 0.9331 - val_loss: 0.8097 - val_accuracy: 0.6158
Epoch 97/500
256/256 - 35s - loss: 0.1746 - accuracy: 0.9331 - val_loss: 0.8139 - val_accuracy: 0.6149
Epoch 98/500
256/256 - 35s - loss: 0.1577 - accuracy: 0.9428 - val_loss: 0.8185 - val_accuracy: 0.6228
Epoch 99/500
256/256 - 35s - loss: 0.1582 - accuracy: 0.9377 - val_loss: 0.8226 - val_accuracy: 0.6218
Epoch 100/500
256/256 - 35s - loss: 0.1536 - accuracy: 0.9397 - val_loss: 0.8307 - val_accuracy: 0.6208
Epoch 101/500
256/256 - 35s - loss: 0.1397 - accuracy: 0.9479 - val_loss: 0.8371 - val_accuracy: 0.6158
Epoch 102/500
256/256 - 35s - loss: 0.1425 - accuracy: 0.9460 - val_loss: 0.8399 - val_accuracy: 0.6178
Epoch 103/500
256/256 - 35s - loss: 0.1414 - accuracy: 0.9485 - val_loss: 0.8455 - val_accuracy: 0.6208
Epoch 104/500
256/256 - 35s - loss: 0.1390 - accuracy: 0.9498 - val_loss: 0.8498 - val_accuracy: 0.6208
Epoch 105/500
256/256 - 35s - loss: 0.1331 - accuracy: 0.9532 - val_loss: 0.8575 - val_accuracy: 0.6178
Epoch 106/500
256/256 - 35s - loss: 0.1285 - accuracy: 0.9520 - val_loss: 0.8653 - val_accuracy: 0.6158
Epoch 107/500
256/256 - 35s - loss: 0.1309 - accuracy: 0.9524 - val_loss: 0.8669 - val_accuracy: 0.6168
Epoch 108/500
256/256 - 35s - loss: 0.1256 - accuracy: 0.9529 - val_loss: 0.8750 - val_accuracy: 0.6168
Epoch 109/500
256/256 - 35s - loss: 0.1205 - accuracy: 0.9552 - val_loss: 0.8812 - val_accuracy: 0.6158
Epoch 110/500
256/256 - 35s - loss: 0.1150 - accuracy: 0.9588 - val_loss: 0.8844 - val_accuracy: 0.6188
Epoch 111/500
256/256 - 35s - loss: 0.1135 - accuracy: 0.9608 - val_loss: 0.8827 - val_accuracy: 0.6238
Epoch 112/500
256/256 - 35s - loss: 0.1104 - accuracy: 0.9603 - val_loss: 0.8879 - val_accuracy: 0.6257
Epoch 113/500
256/256 - 35s - loss: 0.1126 - accuracy: 0.9594 - val_loss: 0.8992 - val_accuracy: 0.6198
Epoch 114/500
256/256 - 35s - loss: 0.1106 - accuracy: 0.9605 - val_loss: 0.9011 - val_accuracy: 0.6188
Epoch 115/500
256/256 - 35s - loss: 0.1023 - accuracy: 0.9618 - val_loss: 0.9082 - val_accuracy: 0.6218
Epoch 116/500
256/256 - 35s - loss: 0.0947 - accuracy: 0.9671 - val_loss: 0.9101 - val_accuracy: 0.6228
Epoch 117/500
256/256 - 35s - loss: 0.0991 - accuracy: 0.9649 - val_loss: 0.9143 - val_accuracy: 0.6228
Epoch 118/500
256/256 - 35s - loss: 0.0986 - accuracy: 0.9643 - val_loss: 0.9238 - val_accuracy: 0.6188
Epoch 119/500
256/256 - 35s - loss: 0.0836 - accuracy: 0.9723 - val_loss: 0.9221 - val_accuracy: 0.6208
Epoch 120/500
256/256 - 35s - loss: 0.0872 - accuracy: 0.9701 - val_loss: 0.9274 - val_accuracy: 0.6188
Epoch 121/500
256/256 - 35s - loss: 0.0892 - accuracy: 0.9684 - val_loss: 0.9362 - val_accuracy: 0.6228
Epoch 122/500
256/256 - 35s - loss: 0.0814 - accuracy: 0.9733 - val_loss: 0.9455 - val_accuracy: 0.6168
Epoch 123/500
256/256 - 35s - loss: 0.0874 - accuracy: 0.9696 - val_loss: 0.9489 - val_accuracy: 0.6198
Epoch 124/500
256/256 - 35s - loss: 0.0840 - accuracy: 0.9701 - val_loss: 0.9568 - val_accuracy: 0.6168
Epoch 125/500
256/256 - 35s - loss: 0.0773 - accuracy: 0.9755 - val_loss: 0.9560 - val_accuracy: 0.6228
Epoch 126/500
256/256 - 35s - loss: 0.0757 - accuracy: 0.9743 - val_loss: 0.9598 - val_accuracy: 0.6228
Epoch 127/500
256/256 - 35s - loss: 0.0773 - accuracy: 0.9728 - val_loss: 0.9687 - val_accuracy: 0.6208
Epoch 128/500
256/256 - 35s - loss: 0.0721 - accuracy: 0.9753 - val_loss: 0.9745 - val_accuracy: 0.6208
Epoch 129/500
256/256 - 35s - loss: 0.0680 - accuracy: 0.9776 - val_loss: 0.9763 - val_accuracy: 0.6257
Epoch 130/500
256/256 - 35s - loss: 0.0622 - accuracy: 0.9798 - val_loss: 0.9817 - val_accuracy: 0.6208
Epoch 131/500
256/256 - 35s - loss: 0.0635 - accuracy: 0.9782 - val_loss: 0.9887 - val_accuracy: 0.6208
Epoch 132/500
256/256 - 35s - loss: 0.0661 - accuracy: 0.9767 - val_loss: 0.9866 - val_accuracy: 0.6257
========================================
save_weights
h5_weights/H1.po/onehot_embedding_cnn_two_branch.h5
========================================

end time >>> Sun Oct  3 09:03:57 2021

end time >>> Sun Oct  3 09:03:57 2021

end time >>> Sun Oct  3 09:03:57 2021

end time >>> Sun Oct  3 09:03:57 2021

end time >>> Sun Oct  3 09:03:57 2021












args.model = onehot_embedding_cnn_two_branch
time used = 4637.068001508713


