************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 03:58:05 2021

begin time >>> Mon Oct  4 03:58:05 2021

begin time >>> Mon Oct  4 03:58:05 2021

begin time >>> Mon Oct  4 03:58:05 2021

begin time >>> Mon Oct  4 03:58:05 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_dense
args.type = train
args.name = TH1.po
args.length = 10001
===========================


-> h5_weights/TH1.po folder already exist. pass.
-> result/TH1.po/onehot_cnn_one_branch folder already exist. pass.
-> result/TH1.po/onehot_cnn_two_branch folder already exist. pass.
-> result/TH1.po/onehot_embedding_dense folder already exist. pass.
-> result/TH1.po/onehot_dense folder already exist. pass.
-> result/TH1.po/onehot_resnet18 folder already exist. pass.
-> result/TH1.po/onehot_resnet34 folder already exist. pass.
-> result/TH1.po/embedding_cnn_one_branch folder already exist. pass.
-> result/TH1.po/embedding_cnn_two_branch folder already exist. pass.
-> result/TH1.po/embedding_dense folder already exist. pass.
-> result/TH1.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/TH1.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
TH1.po
########################################

########################################
model_name
onehot_embedding_dense
########################################

Found 9224 images belonging to 2 classes.
Found 1140 images belonging to 2 classes.
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 20002, 5, 1, 8)    32776     
_________________________________________________________________
flatten (Flatten)            (None, 800080)            0         
_________________________________________________________________
batch_normalization (BatchNo (None, 800080)            3200320   
_________________________________________________________________
dense (Dense)                (None, 512)               409641472 
_________________________________________________________________
batch_normalization_1 (Batch (None, 512)               2048      
_________________________________________________________________
activation (Activation)      (None, 512)               0         
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_3 (Batch (None, 512)               2048      
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 512)               262656    
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_3 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 413,671,754
Trainable params: 412,067,498
Non-trainable params: 1,604,256
_________________________________________________________________
Epoch 1/500
288/288 - 136s - loss: 0.7432 - accuracy: 0.5769 - val_loss: 0.7899 - val_accuracy: 0.5000
Epoch 2/500
288/288 - 53s - loss: 0.6161 - accuracy: 0.6796 - val_loss: 1.0485 - val_accuracy: 0.5027
Epoch 3/500
288/288 - 53s - loss: 0.5226 - accuracy: 0.7483 - val_loss: 1.1374 - val_accuracy: 0.5429
Epoch 4/500
288/288 - 54s - loss: 0.4063 - accuracy: 0.8245 - val_loss: 1.3949 - val_accuracy: 0.5607
Epoch 5/500
288/288 - 53s - loss: 0.3135 - accuracy: 0.8702 - val_loss: 1.6875 - val_accuracy: 0.5625
Epoch 6/500
288/288 - 54s - loss: 0.2490 - accuracy: 0.9000 - val_loss: 1.7942 - val_accuracy: 0.5661
Epoch 7/500
288/288 - 54s - loss: 0.2039 - accuracy: 0.9183 - val_loss: 1.9280 - val_accuracy: 0.5875
Epoch 8/500
288/288 - 53s - loss: 0.1635 - accuracy: 0.9361 - val_loss: 2.0109 - val_accuracy: 0.5839
Epoch 9/500
288/288 - 52s - loss: 0.1344 - accuracy: 0.9505 - val_loss: 2.1282 - val_accuracy: 0.5821
Epoch 10/500
288/288 - 54s - loss: 0.1189 - accuracy: 0.9546 - val_loss: 2.1266 - val_accuracy: 0.6009
Epoch 11/500
288/288 - 52s - loss: 0.1055 - accuracy: 0.9606 - val_loss: 2.1638 - val_accuracy: 0.6009
Epoch 12/500
288/288 - 52s - loss: 0.0847 - accuracy: 0.9683 - val_loss: 2.1913 - val_accuracy: 0.5973
Epoch 13/500
288/288 - 52s - loss: 0.0823 - accuracy: 0.9687 - val_loss: 2.3236 - val_accuracy: 0.5911
Epoch 14/500
288/288 - 54s - loss: 0.0755 - accuracy: 0.9719 - val_loss: 2.2905 - val_accuracy: 0.6036
Epoch 15/500
288/288 - 52s - loss: 0.0734 - accuracy: 0.9730 - val_loss: 2.3619 - val_accuracy: 0.5991
Epoch 16/500
288/288 - 52s - loss: 0.0616 - accuracy: 0.9775 - val_loss: 2.4178 - val_accuracy: 0.5946
Epoch 17/500
288/288 - 53s - loss: 0.0629 - accuracy: 0.9756 - val_loss: 2.4363 - val_accuracy: 0.5920
Epoch 18/500
288/288 - 53s - loss: 0.0696 - accuracy: 0.9744 - val_loss: 2.4610 - val_accuracy: 0.6009
Epoch 19/500
288/288 - 54s - loss: 0.0654 - accuracy: 0.9786 - val_loss: 2.3475 - val_accuracy: 0.6062
Epoch 20/500
288/288 - 54s - loss: 0.0583 - accuracy: 0.9807 - val_loss: 2.3432 - val_accuracy: 0.6187
Epoch 21/500
288/288 - 53s - loss: 0.0568 - accuracy: 0.9791 - val_loss: 2.3154 - val_accuracy: 0.6259
Epoch 22/500
288/288 - 53s - loss: 0.0497 - accuracy: 0.9813 - val_loss: 2.3736 - val_accuracy: 0.6214
Epoch 23/500
288/288 - 53s - loss: 0.0588 - accuracy: 0.9818 - val_loss: 2.3085 - val_accuracy: 0.6277
Epoch 24/500
288/288 - 54s - loss: 0.0525 - accuracy: 0.9832 - val_loss: 2.2688 - val_accuracy: 0.6339
Epoch 25/500
288/288 - 52s - loss: 0.0509 - accuracy: 0.9817 - val_loss: 2.2851 - val_accuracy: 0.6304
Epoch 26/500
288/288 - 52s - loss: 0.0526 - accuracy: 0.9826 - val_loss: 2.3123 - val_accuracy: 0.6277
Epoch 27/500
288/288 - 53s - loss: 0.0452 - accuracy: 0.9856 - val_loss: 2.2641 - val_accuracy: 0.6348
Epoch 28/500
288/288 - 53s - loss: 0.0471 - accuracy: 0.9832 - val_loss: 2.1931 - val_accuracy: 0.6429
Epoch 29/500
288/288 - 53s - loss: 0.0425 - accuracy: 0.9854 - val_loss: 2.1930 - val_accuracy: 0.6491
Epoch 30/500
288/288 - 52s - loss: 0.0388 - accuracy: 0.9874 - val_loss: 2.2434 - val_accuracy: 0.6330
Epoch 31/500
288/288 - 53s - loss: 0.0302 - accuracy: 0.9899 - val_loss: 2.2208 - val_accuracy: 0.6393
Epoch 32/500
288/288 - 53s - loss: 0.0326 - accuracy: 0.9889 - val_loss: 2.2575 - val_accuracy: 0.6348
Epoch 33/500
288/288 - 52s - loss: 0.0351 - accuracy: 0.9873 - val_loss: 2.2911 - val_accuracy: 0.6375
Epoch 34/500
288/288 - 52s - loss: 0.0309 - accuracy: 0.9891 - val_loss: 2.3085 - val_accuracy: 0.6473
Epoch 35/500
288/288 - 52s - loss: 0.0292 - accuracy: 0.9897 - val_loss: 2.3511 - val_accuracy: 0.6393
Epoch 36/500
288/288 - 52s - loss: 0.0266 - accuracy: 0.9908 - val_loss: 2.2919 - val_accuracy: 0.6411
Epoch 37/500
288/288 - 52s - loss: 0.0256 - accuracy: 0.9905 - val_loss: 2.3684 - val_accuracy: 0.6366
Epoch 38/500
288/288 - 52s - loss: 0.0335 - accuracy: 0.9887 - val_loss: 2.3504 - val_accuracy: 0.6438
Epoch 39/500
288/288 - 53s - loss: 0.0361 - accuracy: 0.9879 - val_loss: 2.3701 - val_accuracy: 0.6357
========================================
save_weights
h5_weights/TH1.po/onehot_embedding_dense.h5
========================================

end time >>> Mon Oct  4 04:34:15 2021

end time >>> Mon Oct  4 04:34:15 2021

end time >>> Mon Oct  4 04:34:15 2021

end time >>> Mon Oct  4 04:34:15 2021

end time >>> Mon Oct  4 04:34:15 2021












args.model = onehot_embedding_dense
time used = 2170.6501257419586


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 04:34:16 2021

begin time >>> Mon Oct  4 04:34:16 2021

begin time >>> Mon Oct  4 04:34:16 2021

begin time >>> Mon Oct  4 04:34:16 2021

begin time >>> Mon Oct  4 04:34:16 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_one_branch
args.type = train
args.name = TH1.po
args.length = 10001
===========================


-> h5_weights/TH1.po folder already exist. pass.
-> result/TH1.po/onehot_cnn_one_branch folder already exist. pass.
-> result/TH1.po/onehot_cnn_two_branch folder already exist. pass.
-> result/TH1.po/onehot_embedding_dense folder already exist. pass.
-> result/TH1.po/onehot_dense folder already exist. pass.
-> result/TH1.po/onehot_resnet18 folder already exist. pass.
-> result/TH1.po/onehot_resnet34 folder already exist. pass.
-> result/TH1.po/embedding_cnn_one_branch folder already exist. pass.
-> result/TH1.po/embedding_cnn_two_branch folder already exist. pass.
-> result/TH1.po/embedding_dense folder already exist. pass.
-> result/TH1.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/TH1.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
TH1.po
########################################

########################################
model_name
onehot_embedding_cnn_one_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 20002, 100)   0           embedding[0][0]                  
                                                                 embedding_1[0][0]                
__________________________________________________________________________________________________
sequential (Sequential)         (None, 155, 64)      205888      concatenate[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 9920)         0           sequential[0][0]                 
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9920)         39680       flatten[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9920)         0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5079552     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 512)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,411,785
Trainable params: 6,389,385
Non-trainable params: 22,400
__________________________________________________________________________________________________
Epoch 1/500
289/289 - 40s - loss: 0.8836 - accuracy: 0.5019 - val_loss: 0.6983 - val_accuracy: 0.5031
Epoch 2/500
289/289 - 39s - loss: 0.8693 - accuracy: 0.5062 - val_loss: 0.6992 - val_accuracy: 0.5118
Epoch 3/500
289/289 - 39s - loss: 0.8558 - accuracy: 0.5068 - val_loss: 0.6994 - val_accuracy: 0.5259
Epoch 4/500
289/289 - 39s - loss: 0.8545 - accuracy: 0.5159 - val_loss: 0.6966 - val_accuracy: 0.5372
Epoch 5/500
289/289 - 39s - loss: 0.8308 - accuracy: 0.5266 - val_loss: 0.6947 - val_accuracy: 0.5504
Epoch 6/500
289/289 - 39s - loss: 0.8314 - accuracy: 0.5230 - val_loss: 0.6924 - val_accuracy: 0.5513
Epoch 7/500
289/289 - 39s - loss: 0.8226 - accuracy: 0.5216 - val_loss: 0.6909 - val_accuracy: 0.5460
Epoch 8/500
289/289 - 39s - loss: 0.8229 - accuracy: 0.5276 - val_loss: 0.6888 - val_accuracy: 0.5486
Epoch 9/500
289/289 - 39s - loss: 0.8046 - accuracy: 0.5343 - val_loss: 0.6865 - val_accuracy: 0.5521
Epoch 10/500
289/289 - 39s - loss: 0.8026 - accuracy: 0.5407 - val_loss: 0.6852 - val_accuracy: 0.5539
Epoch 11/500
289/289 - 39s - loss: 0.7965 - accuracy: 0.5430 - val_loss: 0.6841 - val_accuracy: 0.5592
Epoch 12/500
289/289 - 39s - loss: 0.7891 - accuracy: 0.5521 - val_loss: 0.6825 - val_accuracy: 0.5662
Epoch 13/500
289/289 - 39s - loss: 0.7858 - accuracy: 0.5423 - val_loss: 0.6808 - val_accuracy: 0.5627
Epoch 14/500
289/289 - 39s - loss: 0.7785 - accuracy: 0.5503 - val_loss: 0.6793 - val_accuracy: 0.5706
Epoch 15/500
289/289 - 39s - loss: 0.7722 - accuracy: 0.5564 - val_loss: 0.6782 - val_accuracy: 0.5574
Epoch 16/500
289/289 - 39s - loss: 0.7853 - accuracy: 0.5554 - val_loss: 0.6773 - val_accuracy: 0.5618
Epoch 17/500
289/289 - 39s - loss: 0.7576 - accuracy: 0.5664 - val_loss: 0.6759 - val_accuracy: 0.5706
Epoch 18/500
289/289 - 39s - loss: 0.7528 - accuracy: 0.5713 - val_loss: 0.6749 - val_accuracy: 0.5706
Epoch 19/500
289/289 - 39s - loss: 0.7519 - accuracy: 0.5619 - val_loss: 0.6738 - val_accuracy: 0.5758
Epoch 20/500
289/289 - 40s - loss: 0.7398 - accuracy: 0.5802 - val_loss: 0.6723 - val_accuracy: 0.5714
Epoch 21/500
289/289 - 39s - loss: 0.7374 - accuracy: 0.5841 - val_loss: 0.6713 - val_accuracy: 0.5767
Epoch 22/500
289/289 - 39s - loss: 0.7440 - accuracy: 0.5742 - val_loss: 0.6705 - val_accuracy: 0.5802
Epoch 23/500
289/289 - 39s - loss: 0.7304 - accuracy: 0.5858 - val_loss: 0.6697 - val_accuracy: 0.5811
Epoch 24/500
289/289 - 39s - loss: 0.7275 - accuracy: 0.5891 - val_loss: 0.6685 - val_accuracy: 0.5898
Epoch 25/500
289/289 - 39s - loss: 0.7184 - accuracy: 0.5931 - val_loss: 0.6675 - val_accuracy: 0.5960
Epoch 26/500
289/289 - 39s - loss: 0.7182 - accuracy: 0.5907 - val_loss: 0.6665 - val_accuracy: 0.5995
Epoch 27/500
289/289 - 39s - loss: 0.6951 - accuracy: 0.6095 - val_loss: 0.6655 - val_accuracy: 0.6030
Epoch 28/500
289/289 - 39s - loss: 0.7009 - accuracy: 0.6053 - val_loss: 0.6650 - val_accuracy: 0.6047
Epoch 29/500
289/289 - 39s - loss: 0.6899 - accuracy: 0.6153 - val_loss: 0.6641 - val_accuracy: 0.6065
Epoch 30/500
289/289 - 39s - loss: 0.6912 - accuracy: 0.6121 - val_loss: 0.6633 - val_accuracy: 0.6021
Epoch 31/500
289/289 - 39s - loss: 0.6923 - accuracy: 0.6153 - val_loss: 0.6618 - val_accuracy: 0.6082
Epoch 32/500
289/289 - 39s - loss: 0.6705 - accuracy: 0.6256 - val_loss: 0.6613 - val_accuracy: 0.6161
Epoch 33/500
289/289 - 39s - loss: 0.6703 - accuracy: 0.6258 - val_loss: 0.6605 - val_accuracy: 0.6126
Epoch 34/500
289/289 - 40s - loss: 0.6604 - accuracy: 0.6425 - val_loss: 0.6599 - val_accuracy: 0.6126
Epoch 35/500
289/289 - 39s - loss: 0.6547 - accuracy: 0.6424 - val_loss: 0.6593 - val_accuracy: 0.6188
Epoch 36/500
289/289 - 39s - loss: 0.6524 - accuracy: 0.6497 - val_loss: 0.6583 - val_accuracy: 0.6135
Epoch 37/500
289/289 - 39s - loss: 0.6360 - accuracy: 0.6564 - val_loss: 0.6588 - val_accuracy: 0.6152
Epoch 38/500
289/289 - 39s - loss: 0.6278 - accuracy: 0.6628 - val_loss: 0.6578 - val_accuracy: 0.6214
Epoch 39/500
289/289 - 39s - loss: 0.6281 - accuracy: 0.6616 - val_loss: 0.6573 - val_accuracy: 0.6205
Epoch 40/500
289/289 - 39s - loss: 0.6252 - accuracy: 0.6644 - val_loss: 0.6566 - val_accuracy: 0.6188
Epoch 41/500
289/289 - 39s - loss: 0.6094 - accuracy: 0.6761 - val_loss: 0.6569 - val_accuracy: 0.6214
Epoch 42/500
289/289 - 39s - loss: 0.6118 - accuracy: 0.6783 - val_loss: 0.6564 - val_accuracy: 0.6196
Epoch 43/500
289/289 - 39s - loss: 0.6000 - accuracy: 0.6898 - val_loss: 0.6566 - val_accuracy: 0.6179
Epoch 44/500
289/289 - 39s - loss: 0.5854 - accuracy: 0.6966 - val_loss: 0.6564 - val_accuracy: 0.6196
Epoch 45/500
289/289 - 39s - loss: 0.5817 - accuracy: 0.6975 - val_loss: 0.6560 - val_accuracy: 0.6205
Epoch 46/500
289/289 - 39s - loss: 0.5700 - accuracy: 0.7035 - val_loss: 0.6561 - val_accuracy: 0.6231
Epoch 47/500
289/289 - 39s - loss: 0.5688 - accuracy: 0.7090 - val_loss: 0.6566 - val_accuracy: 0.6161
Epoch 48/500
289/289 - 39s - loss: 0.5560 - accuracy: 0.7167 - val_loss: 0.6571 - val_accuracy: 0.6258
Epoch 49/500
289/289 - 39s - loss: 0.5507 - accuracy: 0.7222 - val_loss: 0.6571 - val_accuracy: 0.6258
Epoch 50/500
289/289 - 39s - loss: 0.5340 - accuracy: 0.7344 - val_loss: 0.6577 - val_accuracy: 0.6275
Epoch 51/500
289/289 - 39s - loss: 0.5436 - accuracy: 0.7297 - val_loss: 0.6593 - val_accuracy: 0.6275
Epoch 52/500
289/289 - 39s - loss: 0.5323 - accuracy: 0.7375 - val_loss: 0.6593 - val_accuracy: 0.6284
Epoch 53/500
289/289 - 39s - loss: 0.5147 - accuracy: 0.7482 - val_loss: 0.6616 - val_accuracy: 0.6266
Epoch 54/500
289/289 - 39s - loss: 0.5101 - accuracy: 0.7525 - val_loss: 0.6620 - val_accuracy: 0.6293
Epoch 55/500
289/289 - 39s - loss: 0.4900 - accuracy: 0.7643 - val_loss: 0.6632 - val_accuracy: 0.6249
Epoch 56/500
289/289 - 39s - loss: 0.4960 - accuracy: 0.7602 - val_loss: 0.6642 - val_accuracy: 0.6293
Epoch 57/500
289/289 - 39s - loss: 0.4828 - accuracy: 0.7698 - val_loss: 0.6666 - val_accuracy: 0.6223
Epoch 58/500
289/289 - 39s - loss: 0.4807 - accuracy: 0.7711 - val_loss: 0.6685 - val_accuracy: 0.6258
Epoch 59/500
289/289 - 39s - loss: 0.4764 - accuracy: 0.7707 - val_loss: 0.6707 - val_accuracy: 0.6266
Epoch 60/500
289/289 - 39s - loss: 0.4614 - accuracy: 0.7815 - val_loss: 0.6711 - val_accuracy: 0.6275
Epoch 61/500
289/289 - 39s - loss: 0.4596 - accuracy: 0.7803 - val_loss: 0.6730 - val_accuracy: 0.6275
Epoch 62/500
289/289 - 39s - loss: 0.4419 - accuracy: 0.7970 - val_loss: 0.6749 - val_accuracy: 0.6240
Epoch 63/500
289/289 - 39s - loss: 0.4406 - accuracy: 0.7979 - val_loss: 0.6781 - val_accuracy: 0.6284
Epoch 64/500
289/289 - 39s - loss: 0.4219 - accuracy: 0.8028 - val_loss: 0.6795 - val_accuracy: 0.6284
Epoch 65/500
289/289 - 39s - loss: 0.4216 - accuracy: 0.8049 - val_loss: 0.6823 - val_accuracy: 0.6328
Epoch 66/500
289/289 - 39s - loss: 0.4006 - accuracy: 0.8151 - val_loss: 0.6836 - val_accuracy: 0.6354
Epoch 67/500
289/289 - 39s - loss: 0.4004 - accuracy: 0.8188 - val_loss: 0.6848 - val_accuracy: 0.6372
Epoch 68/500
289/289 - 39s - loss: 0.3957 - accuracy: 0.8212 - val_loss: 0.6883 - val_accuracy: 0.6398
Epoch 69/500
289/289 - 39s - loss: 0.3851 - accuracy: 0.8293 - val_loss: 0.6920 - val_accuracy: 0.6363
Epoch 70/500
289/289 - 39s - loss: 0.3816 - accuracy: 0.8294 - val_loss: 0.6959 - val_accuracy: 0.6363
Epoch 71/500
289/289 - 39s - loss: 0.3746 - accuracy: 0.8287 - val_loss: 0.6982 - val_accuracy: 0.6354
Epoch 72/500
289/289 - 39s - loss: 0.3671 - accuracy: 0.8363 - val_loss: 0.7011 - val_accuracy: 0.6354
Epoch 73/500
289/289 - 39s - loss: 0.3636 - accuracy: 0.8363 - val_loss: 0.7059 - val_accuracy: 0.6380
Epoch 74/500
289/289 - 39s - loss: 0.3410 - accuracy: 0.8485 - val_loss: 0.7089 - val_accuracy: 0.6398
Epoch 75/500
289/289 - 39s - loss: 0.3415 - accuracy: 0.8502 - val_loss: 0.7120 - val_accuracy: 0.6354
Epoch 76/500
289/289 - 39s - loss: 0.3383 - accuracy: 0.8524 - val_loss: 0.7153 - val_accuracy: 0.6380
Epoch 77/500
289/289 - 39s - loss: 0.3240 - accuracy: 0.8589 - val_loss: 0.7200 - val_accuracy: 0.6415
Epoch 78/500
289/289 - 39s - loss: 0.3186 - accuracy: 0.8605 - val_loss: 0.7224 - val_accuracy: 0.6354
Epoch 79/500
289/289 - 39s - loss: 0.3173 - accuracy: 0.8626 - val_loss: 0.7279 - val_accuracy: 0.6372
Epoch 80/500
289/289 - 39s - loss: 0.3059 - accuracy: 0.8727 - val_loss: 0.7313 - val_accuracy: 0.6424
Epoch 81/500
289/289 - 39s - loss: 0.3032 - accuracy: 0.8670 - val_loss: 0.7344 - val_accuracy: 0.6372
Epoch 82/500
289/289 - 39s - loss: 0.2962 - accuracy: 0.8712 - val_loss: 0.7376 - val_accuracy: 0.6380
Epoch 83/500
289/289 - 39s - loss: 0.2902 - accuracy: 0.8785 - val_loss: 0.7444 - val_accuracy: 0.6380
Epoch 84/500
289/289 - 39s - loss: 0.2787 - accuracy: 0.8858 - val_loss: 0.7487 - val_accuracy: 0.6354
Epoch 85/500
289/289 - 39s - loss: 0.2787 - accuracy: 0.8833 - val_loss: 0.7521 - val_accuracy: 0.6398
Epoch 86/500
289/289 - 39s - loss: 0.2634 - accuracy: 0.8941 - val_loss: 0.7559 - val_accuracy: 0.6407
Epoch 87/500
289/289 - 39s - loss: 0.2651 - accuracy: 0.8905 - val_loss: 0.7609 - val_accuracy: 0.6380
Epoch 88/500
289/289 - 39s - loss: 0.2582 - accuracy: 0.8899 - val_loss: 0.7660 - val_accuracy: 0.6363
Epoch 89/500
289/289 - 39s - loss: 0.2560 - accuracy: 0.8942 - val_loss: 0.7692 - val_accuracy: 0.6407
Epoch 90/500
289/289 - 39s - loss: 0.2502 - accuracy: 0.8969 - val_loss: 0.7743 - val_accuracy: 0.6337
Epoch 91/500
289/289 - 39s - loss: 0.2438 - accuracy: 0.9007 - val_loss: 0.7784 - val_accuracy: 0.6372
Epoch 92/500
289/289 - 39s - loss: 0.2436 - accuracy: 0.8984 - val_loss: 0.7833 - val_accuracy: 0.6345
Epoch 93/500
289/289 - 39s - loss: 0.2322 - accuracy: 0.9058 - val_loss: 0.7873 - val_accuracy: 0.6415
Epoch 94/500
289/289 - 39s - loss: 0.2263 - accuracy: 0.9066 - val_loss: 0.7912 - val_accuracy: 0.6407
Epoch 95/500
289/289 - 39s - loss: 0.2173 - accuracy: 0.9105 - val_loss: 0.7973 - val_accuracy: 0.6389
Epoch 96/500
289/289 - 39s - loss: 0.2262 - accuracy: 0.9038 - val_loss: 0.8009 - val_accuracy: 0.6389
Epoch 97/500
289/289 - 39s - loss: 0.2117 - accuracy: 0.9140 - val_loss: 0.8069 - val_accuracy: 0.6433
Epoch 98/500
289/289 - 39s - loss: 0.2000 - accuracy: 0.9204 - val_loss: 0.8141 - val_accuracy: 0.6407
Epoch 99/500
289/289 - 39s - loss: 0.1984 - accuracy: 0.9204 - val_loss: 0.8168 - val_accuracy: 0.6398
Epoch 100/500
289/289 - 39s - loss: 0.1907 - accuracy: 0.9257 - val_loss: 0.8219 - val_accuracy: 0.6407
Epoch 101/500
289/289 - 39s - loss: 0.1866 - accuracy: 0.9264 - val_loss: 0.8267 - val_accuracy: 0.6424
Epoch 102/500
289/289 - 39s - loss: 0.1860 - accuracy: 0.9289 - val_loss: 0.8330 - val_accuracy: 0.6450
Epoch 103/500
289/289 - 39s - loss: 0.1818 - accuracy: 0.9304 - val_loss: 0.8383 - val_accuracy: 0.6477
Epoch 104/500
289/289 - 39s - loss: 0.1824 - accuracy: 0.9326 - val_loss: 0.8456 - val_accuracy: 0.6450
Epoch 105/500
289/289 - 39s - loss: 0.1728 - accuracy: 0.9326 - val_loss: 0.8512 - val_accuracy: 0.6424
Epoch 106/500
289/289 - 39s - loss: 0.1720 - accuracy: 0.9322 - val_loss: 0.8553 - val_accuracy: 0.6433
Epoch 107/500
289/289 - 39s - loss: 0.1657 - accuracy: 0.9343 - val_loss: 0.8635 - val_accuracy: 0.6407
Epoch 108/500
289/289 - 39s - loss: 0.1613 - accuracy: 0.9396 - val_loss: 0.8670 - val_accuracy: 0.6372
Epoch 109/500
289/289 - 39s - loss: 0.1631 - accuracy: 0.9354 - val_loss: 0.8710 - val_accuracy: 0.6407
Epoch 110/500
289/289 - 39s - loss: 0.1651 - accuracy: 0.9355 - val_loss: 0.8788 - val_accuracy: 0.6389
Epoch 111/500
289/289 - 39s - loss: 0.1506 - accuracy: 0.9406 - val_loss: 0.8824 - val_accuracy: 0.6389
Epoch 112/500
289/289 - 39s - loss: 0.1520 - accuracy: 0.9407 - val_loss: 0.8868 - val_accuracy: 0.6389
Epoch 113/500
289/289 - 39s - loss: 0.1442 - accuracy: 0.9433 - val_loss: 0.8959 - val_accuracy: 0.6354
Epoch 114/500
289/289 - 39s - loss: 0.1452 - accuracy: 0.9409 - val_loss: 0.8991 - val_accuracy: 0.6415
Epoch 115/500
289/289 - 39s - loss: 0.1462 - accuracy: 0.9450 - val_loss: 0.9042 - val_accuracy: 0.6398
Epoch 116/500
289/289 - 39s - loss: 0.1447 - accuracy: 0.9457 - val_loss: 0.9090 - val_accuracy: 0.6398
Epoch 117/500
289/289 - 39s - loss: 0.1293 - accuracy: 0.9515 - val_loss: 0.9133 - val_accuracy: 0.6398
Epoch 118/500
289/289 - 39s - loss: 0.1322 - accuracy: 0.9511 - val_loss: 0.9193 - val_accuracy: 0.6345
Epoch 119/500
289/289 - 39s - loss: 0.1271 - accuracy: 0.9528 - val_loss: 0.9234 - val_accuracy: 0.6372
Epoch 120/500
289/289 - 39s - loss: 0.1299 - accuracy: 0.9499 - val_loss: 0.9291 - val_accuracy: 0.6398
Epoch 121/500
289/289 - 39s - loss: 0.1279 - accuracy: 0.9507 - val_loss: 0.9368 - val_accuracy: 0.6389
Epoch 122/500
289/289 - 39s - loss: 0.1197 - accuracy: 0.9554 - val_loss: 0.9421 - val_accuracy: 0.6389
Epoch 123/500
289/289 - 39s - loss: 0.1169 - accuracy: 0.9538 - val_loss: 0.9477 - val_accuracy: 0.6363
========================================
save_weights
h5_weights/TH1.po/onehot_embedding_cnn_one_branch.h5
========================================

end time >>> Mon Oct  4 05:55:17 2021

end time >>> Mon Oct  4 05:55:17 2021

end time >>> Mon Oct  4 05:55:17 2021

end time >>> Mon Oct  4 05:55:17 2021

end time >>> Mon Oct  4 05:55:17 2021












args.model = onehot_embedding_cnn_one_branch
time used = 4860.878058195114


************************************
************************************

    Welcome to use DeepChromeHiC

************************************
************************************

begin time >>> Mon Oct  4 05:55:18 2021

begin time >>> Mon Oct  4 05:55:18 2021

begin time >>> Mon Oct  4 05:55:18 2021

begin time >>> Mon Oct  4 05:55:18 2021

begin time >>> Mon Oct  4 05:55:18 2021



If it is the first time to use, please preprocess the data first.
Use the command: python3 DeepChromeHiC.py -p true -n [gene name]
For example: python3 DeepChromeHiC.py -p true -n AD2.po


=== Below is your input ===

args.model = onehot_embedding_cnn_two_branch
args.type = train
args.name = TH1.po
args.length = 10001
===========================


-> h5_weights/TH1.po folder already exist. pass.
-> result/TH1.po/onehot_cnn_one_branch folder already exist. pass.
-> result/TH1.po/onehot_cnn_two_branch folder already exist. pass.
-> result/TH1.po/onehot_embedding_dense folder already exist. pass.
-> result/TH1.po/onehot_dense folder already exist. pass.
-> result/TH1.po/onehot_resnet18 folder already exist. pass.
-> result/TH1.po/onehot_resnet34 folder already exist. pass.
-> result/TH1.po/embedding_cnn_one_branch folder already exist. pass.
-> result/TH1.po/embedding_cnn_two_branch folder already exist. pass.
-> result/TH1.po/embedding_dense folder already exist. pass.
-> result/TH1.po/onehot_embedding_cnn_one_branch folder already exist. pass.
-> result/TH1.po/onehot_embedding_cnn_two_branch folder already exist. pass.
########################################
gen_name
TH1.po
########################################

########################################
model_name
onehot_embedding_cnn_two_branch
########################################

Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 10001)]      0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 10001, 100)   409700      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 10001, 100)   409700      input_2[0][0]                    
__________________________________________________________________________________________________
sequential (Sequential)         (None, 77, 64)       205888      embedding[0][0]                  
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 77, 64)       205888      embedding_1[0][0]                
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4928)         0           sequential[0][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4928)         0           sequential_1[0][0]               
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 9856)         0           flatten[0][0]                    
                                                                 flatten_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9856)         39424       concatenate[0][0]                
__________________________________________________________________________________________________
dropout (Dropout)               (None, 9856)         0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dense (Dense)                   (None, 512)          5046784     dropout[0][0]                    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 512)          2048        dense[0][0]                      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 512)          2048        dense_1[0][0]                    
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  
==================================================================================================
Total params: 6,584,649
Trainable params: 6,561,865
Non-trainable params: 22,784
__________________________________________________________________________________________________
Epoch 1/500
289/289 - 40s - loss: 0.8762 - accuracy: 0.5115 - val_loss: 0.6949 - val_accuracy: 0.5031
Epoch 2/500
289/289 - 39s - loss: 0.8793 - accuracy: 0.4990 - val_loss: 0.7009 - val_accuracy: 0.5022
Epoch 3/500
289/289 - 39s - loss: 0.8574 - accuracy: 0.5112 - val_loss: 0.7118 - val_accuracy: 0.4829
Epoch 4/500
289/289 - 40s - loss: 0.8429 - accuracy: 0.5184 - val_loss: 0.7115 - val_accuracy: 0.4838
Epoch 5/500
289/289 - 39s - loss: 0.8320 - accuracy: 0.5203 - val_loss: 0.7091 - val_accuracy: 0.4873
Epoch 6/500
289/289 - 39s - loss: 0.8249 - accuracy: 0.5240 - val_loss: 0.7069 - val_accuracy: 0.5004
Epoch 7/500
289/289 - 39s - loss: 0.8179 - accuracy: 0.5250 - val_loss: 0.7064 - val_accuracy: 0.5039
Epoch 8/500
289/289 - 39s - loss: 0.8140 - accuracy: 0.5326 - val_loss: 0.7039 - val_accuracy: 0.5039
Epoch 9/500
289/289 - 39s - loss: 0.8189 - accuracy: 0.5220 - val_loss: 0.7026 - val_accuracy: 0.5092
Epoch 10/500
289/289 - 39s - loss: 0.7945 - accuracy: 0.5411 - val_loss: 0.7018 - val_accuracy: 0.5206
Epoch 11/500
289/289 - 39s - loss: 0.7872 - accuracy: 0.5420 - val_loss: 0.7003 - val_accuracy: 0.5188
Epoch 12/500
289/289 - 39s - loss: 0.7760 - accuracy: 0.5525 - val_loss: 0.6993 - val_accuracy: 0.5259
Epoch 13/500
289/289 - 39s - loss: 0.7815 - accuracy: 0.5517 - val_loss: 0.6973 - val_accuracy: 0.5250
Epoch 14/500
289/289 - 39s - loss: 0.7811 - accuracy: 0.5491 - val_loss: 0.6960 - val_accuracy: 0.5337
Epoch 15/500
289/289 - 39s - loss: 0.7767 - accuracy: 0.5531 - val_loss: 0.6953 - val_accuracy: 0.5302
Epoch 16/500
289/289 - 39s - loss: 0.7571 - accuracy: 0.5654 - val_loss: 0.6941 - val_accuracy: 0.5320
Epoch 17/500
289/289 - 39s - loss: 0.7442 - accuracy: 0.5783 - val_loss: 0.6928 - val_accuracy: 0.5337
Epoch 18/500
289/289 - 39s - loss: 0.7467 - accuracy: 0.5686 - val_loss: 0.6911 - val_accuracy: 0.5390
Epoch 19/500
289/289 - 39s - loss: 0.7409 - accuracy: 0.5740 - val_loss: 0.6911 - val_accuracy: 0.5337
Epoch 20/500
289/289 - 39s - loss: 0.7326 - accuracy: 0.5850 - val_loss: 0.6905 - val_accuracy: 0.5329
Epoch 21/500
289/289 - 39s - loss: 0.7300 - accuracy: 0.5864 - val_loss: 0.6890 - val_accuracy: 0.5355
Epoch 22/500
289/289 - 39s - loss: 0.7269 - accuracy: 0.5858 - val_loss: 0.6883 - val_accuracy: 0.5381
Epoch 23/500
289/289 - 39s - loss: 0.7145 - accuracy: 0.5999 - val_loss: 0.6875 - val_accuracy: 0.5381
Epoch 24/500
289/289 - 39s - loss: 0.7107 - accuracy: 0.5990 - val_loss: 0.6870 - val_accuracy: 0.5355
Epoch 25/500
289/289 - 39s - loss: 0.6978 - accuracy: 0.6129 - val_loss: 0.6857 - val_accuracy: 0.5451
Epoch 26/500
289/289 - 39s - loss: 0.6907 - accuracy: 0.6164 - val_loss: 0.6853 - val_accuracy: 0.5443
Epoch 27/500
289/289 - 39s - loss: 0.6900 - accuracy: 0.6134 - val_loss: 0.6840 - val_accuracy: 0.5495
Epoch 28/500
289/289 - 39s - loss: 0.6768 - accuracy: 0.6256 - val_loss: 0.6840 - val_accuracy: 0.5504
Epoch 29/500
289/289 - 39s - loss: 0.6644 - accuracy: 0.6320 - val_loss: 0.6841 - val_accuracy: 0.5504
Epoch 30/500
289/289 - 39s - loss: 0.6752 - accuracy: 0.6317 - val_loss: 0.6839 - val_accuracy: 0.5486
Epoch 31/500
289/289 - 39s - loss: 0.6666 - accuracy: 0.6347 - val_loss: 0.6841 - val_accuracy: 0.5539
Epoch 32/500
289/289 - 39s - loss: 0.6552 - accuracy: 0.6399 - val_loss: 0.6824 - val_accuracy: 0.5539
Epoch 33/500
289/289 - 39s - loss: 0.6505 - accuracy: 0.6445 - val_loss: 0.6820 - val_accuracy: 0.5557
Epoch 34/500
289/289 - 39s - loss: 0.6447 - accuracy: 0.6490 - val_loss: 0.6826 - val_accuracy: 0.5583
Epoch 35/500
289/289 - 39s - loss: 0.6340 - accuracy: 0.6568 - val_loss: 0.6813 - val_accuracy: 0.5662
Epoch 36/500
289/289 - 39s - loss: 0.6263 - accuracy: 0.6610 - val_loss: 0.6815 - val_accuracy: 0.5653
Epoch 37/500
289/289 - 39s - loss: 0.6155 - accuracy: 0.6738 - val_loss: 0.6820 - val_accuracy: 0.5679
Epoch 38/500
289/289 - 39s - loss: 0.6193 - accuracy: 0.6688 - val_loss: 0.6825 - val_accuracy: 0.5714
Epoch 39/500
289/289 - 39s - loss: 0.6029 - accuracy: 0.6791 - val_loss: 0.6826 - val_accuracy: 0.5741
Epoch 40/500
289/289 - 39s - loss: 0.5955 - accuracy: 0.6842 - val_loss: 0.6816 - val_accuracy: 0.5732
Epoch 41/500
289/289 - 39s - loss: 0.5798 - accuracy: 0.6963 - val_loss: 0.6813 - val_accuracy: 0.5767
Epoch 42/500
289/289 - 39s - loss: 0.5679 - accuracy: 0.7070 - val_loss: 0.6831 - val_accuracy: 0.5767
Epoch 43/500
289/289 - 39s - loss: 0.5645 - accuracy: 0.7114 - val_loss: 0.6823 - val_accuracy: 0.5776
Epoch 44/500
289/289 - 39s - loss: 0.5567 - accuracy: 0.7222 - val_loss: 0.6839 - val_accuracy: 0.5802
Epoch 45/500
289/289 - 39s - loss: 0.5542 - accuracy: 0.7203 - val_loss: 0.6845 - val_accuracy: 0.5767
Epoch 46/500
289/289 - 39s - loss: 0.5449 - accuracy: 0.7229 - val_loss: 0.6843 - val_accuracy: 0.5811
Epoch 47/500
289/289 - 39s - loss: 0.5204 - accuracy: 0.7403 - val_loss: 0.6853 - val_accuracy: 0.5819
Epoch 48/500
289/289 - 39s - loss: 0.5176 - accuracy: 0.7462 - val_loss: 0.6866 - val_accuracy: 0.5793
Epoch 49/500
289/289 - 39s - loss: 0.5149 - accuracy: 0.7487 - val_loss: 0.6874 - val_accuracy: 0.5819
Epoch 50/500
289/289 - 39s - loss: 0.5006 - accuracy: 0.7544 - val_loss: 0.6890 - val_accuracy: 0.5802
Epoch 51/500
289/289 - 39s - loss: 0.5022 - accuracy: 0.7540 - val_loss: 0.6913 - val_accuracy: 0.5776
Epoch 52/500
289/289 - 39s - loss: 0.4844 - accuracy: 0.7646 - val_loss: 0.6915 - val_accuracy: 0.5811
Epoch 53/500
289/289 - 39s - loss: 0.4812 - accuracy: 0.7685 - val_loss: 0.6936 - val_accuracy: 0.5802
Epoch 54/500
289/289 - 39s - loss: 0.4674 - accuracy: 0.7758 - val_loss: 0.6967 - val_accuracy: 0.5802
Epoch 55/500
289/289 - 39s - loss: 0.4544 - accuracy: 0.7891 - val_loss: 0.6974 - val_accuracy: 0.5846
Epoch 56/500
289/289 - 39s - loss: 0.4570 - accuracy: 0.7828 - val_loss: 0.6999 - val_accuracy: 0.5819
Epoch 57/500
289/289 - 39s - loss: 0.4313 - accuracy: 0.8013 - val_loss: 0.7023 - val_accuracy: 0.5837
Epoch 58/500
289/289 - 39s - loss: 0.4301 - accuracy: 0.8033 - val_loss: 0.7054 - val_accuracy: 0.5828
Epoch 59/500
289/289 - 39s - loss: 0.4152 - accuracy: 0.8146 - val_loss: 0.7078 - val_accuracy: 0.5855
Epoch 60/500
289/289 - 39s - loss: 0.4118 - accuracy: 0.8137 - val_loss: 0.7125 - val_accuracy: 0.5846
Epoch 61/500
289/289 - 39s - loss: 0.4034 - accuracy: 0.8139 - val_loss: 0.7146 - val_accuracy: 0.5872
Epoch 62/500
289/289 - 39s - loss: 0.3894 - accuracy: 0.8250 - val_loss: 0.7169 - val_accuracy: 0.5925
Epoch 63/500
289/289 - 39s - loss: 0.3833 - accuracy: 0.8281 - val_loss: 0.7211 - val_accuracy: 0.5907
Epoch 64/500
289/289 - 39s - loss: 0.3766 - accuracy: 0.8311 - val_loss: 0.7249 - val_accuracy: 0.5925
Epoch 65/500
289/289 - 39s - loss: 0.3743 - accuracy: 0.8354 - val_loss: 0.7298 - val_accuracy: 0.5881
Epoch 66/500
289/289 - 39s - loss: 0.3573 - accuracy: 0.8425 - val_loss: 0.7330 - val_accuracy: 0.5881
Epoch 67/500
289/289 - 39s - loss: 0.3586 - accuracy: 0.8382 - val_loss: 0.7352 - val_accuracy: 0.5942
Epoch 68/500
289/289 - 39s - loss: 0.3464 - accuracy: 0.8471 - val_loss: 0.7381 - val_accuracy: 0.5951
Epoch 69/500
289/289 - 39s - loss: 0.3320 - accuracy: 0.8555 - val_loss: 0.7428 - val_accuracy: 0.5916
Epoch 70/500
289/289 - 39s - loss: 0.3296 - accuracy: 0.8594 - val_loss: 0.7473 - val_accuracy: 0.5951
Epoch 71/500
289/289 - 39s - loss: 0.3254 - accuracy: 0.8566 - val_loss: 0.7524 - val_accuracy: 0.5925
Epoch 72/500
289/289 - 39s - loss: 0.3193 - accuracy: 0.8658 - val_loss: 0.7563 - val_accuracy: 0.5960
Epoch 73/500
289/289 - 39s - loss: 0.3140 - accuracy: 0.8663 - val_loss: 0.7611 - val_accuracy: 0.5960
Epoch 74/500
289/289 - 39s - loss: 0.2979 - accuracy: 0.8765 - val_loss: 0.7669 - val_accuracy: 0.5942
Epoch 75/500
289/289 - 39s - loss: 0.2954 - accuracy: 0.8762 - val_loss: 0.7684 - val_accuracy: 0.5951
Epoch 76/500
289/289 - 39s - loss: 0.2769 - accuracy: 0.8828 - val_loss: 0.7736 - val_accuracy: 0.5977
Epoch 77/500
289/289 - 39s - loss: 0.2804 - accuracy: 0.8825 - val_loss: 0.7811 - val_accuracy: 0.5977
Epoch 78/500
289/289 - 39s - loss: 0.2739 - accuracy: 0.8832 - val_loss: 0.7852 - val_accuracy: 0.5977
Epoch 79/500
289/289 - 39s - loss: 0.2634 - accuracy: 0.8895 - val_loss: 0.7912 - val_accuracy: 0.5977
Epoch 80/500
289/289 - 39s - loss: 0.2598 - accuracy: 0.8936 - val_loss: 0.7924 - val_accuracy: 0.6039
Epoch 81/500
289/289 - 39s - loss: 0.2513 - accuracy: 0.8967 - val_loss: 0.8000 - val_accuracy: 0.5986
Epoch 82/500
289/289 - 39s - loss: 0.2415 - accuracy: 0.9027 - val_loss: 0.8040 - val_accuracy: 0.5968
Epoch 83/500
289/289 - 39s - loss: 0.2359 - accuracy: 0.9068 - val_loss: 0.8127 - val_accuracy: 0.5986
Epoch 84/500
289/289 - 39s - loss: 0.2405 - accuracy: 0.9029 - val_loss: 0.8112 - val_accuracy: 0.6004
Epoch 85/500
289/289 - 39s - loss: 0.2167 - accuracy: 0.9145 - val_loss: 0.8195 - val_accuracy: 0.6004
Epoch 86/500
289/289 - 39s - loss: 0.2188 - accuracy: 0.9134 - val_loss: 0.8275 - val_accuracy: 0.6012
Epoch 87/500
289/289 - 39s - loss: 0.2089 - accuracy: 0.9142 - val_loss: 0.8364 - val_accuracy: 0.6021
Epoch 88/500
289/289 - 39s - loss: 0.2069 - accuracy: 0.9195 - val_loss: 0.8374 - val_accuracy: 0.6030
Epoch 89/500
289/289 - 39s - loss: 0.1972 - accuracy: 0.9239 - val_loss: 0.8448 - val_accuracy: 0.6047
Epoch 90/500
289/289 - 39s - loss: 0.1967 - accuracy: 0.9251 - val_loss: 0.8501 - val_accuracy: 0.6030
Epoch 91/500
289/289 - 39s - loss: 0.1907 - accuracy: 0.9269 - val_loss: 0.8545 - val_accuracy: 0.6074
Epoch 92/500
289/289 - 39s - loss: 0.1921 - accuracy: 0.9251 - val_loss: 0.8629 - val_accuracy: 0.6030
Epoch 93/500
289/289 - 39s - loss: 0.1834 - accuracy: 0.9310 - val_loss: 0.8678 - val_accuracy: 0.6082
Epoch 94/500
289/289 - 39s - loss: 0.1743 - accuracy: 0.9325 - val_loss: 0.8753 - val_accuracy: 0.6030
Epoch 95/500
289/289 - 39s - loss: 0.1748 - accuracy: 0.9332 - val_loss: 0.8774 - val_accuracy: 0.6091
Epoch 96/500
289/289 - 39s - loss: 0.1709 - accuracy: 0.9359 - val_loss: 0.8871 - val_accuracy: 0.6074
Epoch 97/500
289/289 - 39s - loss: 0.1556 - accuracy: 0.9429 - val_loss: 0.8916 - val_accuracy: 0.6065
Epoch 98/500
289/289 - 39s - loss: 0.1629 - accuracy: 0.9377 - val_loss: 0.9007 - val_accuracy: 0.6021
Epoch 99/500
289/289 - 39s - loss: 0.1509 - accuracy: 0.9449 - val_loss: 0.9012 - val_accuracy: 0.6109
Epoch 100/500
289/289 - 39s - loss: 0.1474 - accuracy: 0.9442 - val_loss: 0.9072 - val_accuracy: 0.6126
Epoch 101/500
289/289 - 39s - loss: 0.1404 - accuracy: 0.9491 - val_loss: 0.9171 - val_accuracy: 0.6109
Epoch 102/500
289/289 - 39s - loss: 0.1387 - accuracy: 0.9493 - val_loss: 0.9225 - val_accuracy: 0.6117
Epoch 103/500
289/289 - 39s - loss: 0.1344 - accuracy: 0.9506 - val_loss: 0.9251 - val_accuracy: 0.6144
Epoch 104/500
289/289 - 39s - loss: 0.1380 - accuracy: 0.9472 - val_loss: 0.9332 - val_accuracy: 0.6100
Epoch 105/500
289/289 - 39s - loss: 0.1291 - accuracy: 0.9491 - val_loss: 0.9403 - val_accuracy: 0.6056
Epoch 106/500
289/289 - 39s - loss: 0.1180 - accuracy: 0.9568 - val_loss: 0.9519 - val_accuracy: 0.6065
Epoch 107/500
289/289 - 39s - loss: 0.1236 - accuracy: 0.9548 - val_loss: 0.9535 - val_accuracy: 0.6065
Epoch 108/500
289/289 - 39s - loss: 0.1205 - accuracy: 0.9552 - val_loss: 0.9570 - val_accuracy: 0.6056
Epoch 109/500
289/289 - 39s - loss: 0.1134 - accuracy: 0.9579 - val_loss: 0.9591 - val_accuracy: 0.6144
Epoch 110/500
289/289 - 39s - loss: 0.1115 - accuracy: 0.9609 - val_loss: 0.9696 - val_accuracy: 0.6082
Epoch 111/500
289/289 - 39s - loss: 0.1104 - accuracy: 0.9611 - val_loss: 0.9797 - val_accuracy: 0.6030
Epoch 112/500
289/289 - 39s - loss: 0.1123 - accuracy: 0.9589 - val_loss: 0.9829 - val_accuracy: 0.6047
Epoch 113/500
289/289 - 39s - loss: 0.1022 - accuracy: 0.9650 - val_loss: 0.9960 - val_accuracy: 0.6056
Epoch 114/500
289/289 - 39s - loss: 0.1062 - accuracy: 0.9631 - val_loss: 0.9980 - val_accuracy: 0.6082
Epoch 115/500
289/289 - 39s - loss: 0.1026 - accuracy: 0.9632 - val_loss: 1.0049 - val_accuracy: 0.6074
Epoch 116/500
289/289 - 39s - loss: 0.1003 - accuracy: 0.9652 - val_loss: 1.0119 - val_accuracy: 0.6117
Epoch 117/500
289/289 - 39s - loss: 0.0960 - accuracy: 0.9674 - val_loss: 1.0118 - val_accuracy: 0.6179
Epoch 118/500
289/289 - 39s - loss: 0.0962 - accuracy: 0.9668 - val_loss: 1.0212 - val_accuracy: 0.6152
Epoch 119/500
289/289 - 39s - loss: 0.0970 - accuracy: 0.9654 - val_loss: 1.0245 - val_accuracy: 0.6126
Epoch 120/500
289/289 - 39s - loss: 0.0922 - accuracy: 0.9692 - val_loss: 1.0340 - val_accuracy: 0.6039
Epoch 121/500
289/289 - 39s - loss: 0.0878 - accuracy: 0.9701 - val_loss: 1.0355 - val_accuracy: 0.6126
Epoch 122/500
289/289 - 39s - loss: 0.0862 - accuracy: 0.9683 - val_loss: 1.0469 - val_accuracy: 0.6109
Epoch 123/500
289/289 - 39s - loss: 0.0811 - accuracy: 0.9700 - val_loss: 1.0510 - val_accuracy: 0.6144
Epoch 124/500
289/289 - 40s - loss: 0.0815 - accuracy: 0.9709 - val_loss: 1.0551 - val_accuracy: 0.6126
Epoch 125/500
289/289 - 40s - loss: 0.0832 - accuracy: 0.9705 - val_loss: 1.0611 - val_accuracy: 0.6135
Epoch 126/500
289/289 - 40s - loss: 0.0806 - accuracy: 0.9729 - val_loss: 1.0718 - val_accuracy: 0.6109
Epoch 127/500
289/289 - 40s - loss: 0.0793 - accuracy: 0.9722 - val_loss: 1.0732 - val_accuracy: 0.6126
Epoch 128/500
289/289 - 40s - loss: 0.0748 - accuracy: 0.9719 - val_loss: 1.0754 - val_accuracy: 0.6100
Epoch 129/500
289/289 - 40s - loss: 0.0755 - accuracy: 0.9735 - val_loss: 1.0829 - val_accuracy: 0.6126
Epoch 130/500
289/289 - 40s - loss: 0.0697 - accuracy: 0.9744 - val_loss: 1.0845 - val_accuracy: 0.6152
Epoch 131/500
289/289 - 40s - loss: 0.0731 - accuracy: 0.9737 - val_loss: 1.0988 - val_accuracy: 0.6091
Epoch 132/500
289/289 - 40s - loss: 0.0777 - accuracy: 0.9738 - val_loss: 1.1007 - val_accuracy: 0.6144
Epoch 133/500
289/289 - 40s - loss: 0.0712 - accuracy: 0.9770 - val_loss: 1.1077 - val_accuracy: 0.6152
Epoch 134/500
289/289 - 40s - loss: 0.0662 - accuracy: 0.9779 - val_loss: 1.1179 - val_accuracy: 0.6117
Epoch 135/500
289/289 - 40s - loss: 0.0657 - accuracy: 0.9780 - val_loss: 1.1199 - val_accuracy: 0.6117
Epoch 136/500
289/289 - 40s - loss: 0.0670 - accuracy: 0.9769 - val_loss: 1.1198 - val_accuracy: 0.6126
Epoch 137/500
289/289 - 40s - loss: 0.0563 - accuracy: 0.9815 - val_loss: 1.1288 - val_accuracy: 0.6135
========================================
save_weights
h5_weights/TH1.po/onehot_embedding_cnn_two_branch.h5
========================================

end time >>> Mon Oct  4 07:25:36 2021

end time >>> Mon Oct  4 07:25:36 2021

end time >>> Mon Oct  4 07:25:36 2021

end time >>> Mon Oct  4 07:25:36 2021

end time >>> Mon Oct  4 07:25:36 2021












args.model = onehot_embedding_cnn_two_branch
time used = 5417.804141998291


